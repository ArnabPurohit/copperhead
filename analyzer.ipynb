{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <img  src=\"https://swan.web.cern.ch/sites/swan.web.cern.ch/files/pictures/logo_swan_letters.png\" alt=\"SWAN\" style=\"float: left; width: 15%; margin-right: 5%; margin-left: 17%; margin-top: 1.0em; margin-bottom: 2.0em;\">\n",
    "<img src=\"https://spark.apache.org/images/spark-logo-trademark.png\" alt=\"EP-SFT\" style=\"float: left; width: 25%; margin-right: 0%; margin-left: 0%; margin-bottom: 2.0em;\">\n",
    "<img src=\"https://cms-docdb.cern.ch/cgi-bin/PublicDocDB/RetrieveFile?docid=3045&filename=CMSlogo_color_label_1024_May2014.png&version=3\" alt=\"CMS\" style=\"float: left; width: 12%; margin-left: 5%; margin-right: 5%; margin-bottom: 2.0em;\"> -->\n",
    "<p style=\"clear: both;\">\n",
    "<div style=\"text-align:center\"><h1>CMS H&#8594;µµ analysis with Apache Spark  \n",
    "     <br> using Coffea and Laurelin packages from Fermilab</h1></div>\n",
    "<div style=\"text-align:center\"><i>Author: Dmitry Kondratyev, based on example code by Lindsey Gray</i></div>\n",
    "<hr style=\"border-top-width: 4px; border-top-color: #34609b;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for Higgs boson decaying into two muons\n",
    "\n",
    "This code uses awkward array toolset, and utilizing Coffea [histograms](https://coffeateam.github.io/coffea/modules/coffea.hist.html).\n",
    "This also shows the analysis object syntax implemented by Coffea [JaggedCandidateArray](https://coffeateam.github.io/coffea/api/coffea.analysis_objects.JaggedCandidateMethods.html), and the usage of custom [accumulators](https://coffeateam.github.io/coffea/api/coffea.processor.AccumulatorABC.html) other than histograms.  Further, it introduces the [processor](https://coffeateam.github.io/coffea/api/coffea.processor.ProcessorABC.html) concept and the interface to apache spark.\n",
    "\n",
    "#### SWAN env: LCG96 Python3 stack and Cloud Containers cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you do not have coffea installed (e.g. on SWAN with LCG 96Python3 stack)\n",
    "!pip install --user --upgrade coffea\n",
    "\n",
    "# spark.jars.packages doesnt work with Spark 2.4 with kubernetes\n",
    "!wget -N https://repo1.maven.org/maven2/edu/vanderbilt/accre/laurelin/0.5.1/laurelin-0.5.1.jar && \\\n",
    "wget -N https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-api/2.11.2/log4j-api-2.11.2.jar && \\\n",
    "wget -N https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-core/2.11.2/log4j-core-2.11.2.jar && \\\n",
    "wget -N https://repo1.maven.org/maven2/org/lz4/lz4-java/1.5.1/lz4-java-1.5.1.jar && \\\n",
    "wget -N https://repo1.maven.org/maven2/org/tukaani/xz/1.2/xz-1.2.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell before establishing spark connection\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONPATH'] = os.environ['PYTHONPATH'] + ':' + '/usr/local/lib/python3.6/site-packages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import coffea\n",
    "print(\"Coffea version: \", coffea.__version__)\n",
    "\n",
    "from coffea import hist, util\n",
    "from coffea.analysis_objects import JaggedCandidateArray\n",
    "import coffea.processor as processor\n",
    "from coffea.lookup_tools import extractor, dense_lookup\n",
    "from coffea.lumi_tools import LumiMask\n",
    "\n",
    "import awkward\n",
    "import uproot\n",
    "import numpy as np\n",
    "import numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "paths = {\n",
    "    'data_B': '/eos/cms/store/data/Run2016B_ver1/SingleMuon/NANOAOD/Nano25Oct2019_ver1-v1/*/',\n",
    "    'data_C': '/eos/cms/store/data/Run2016C/SingleMuon/NANOAOD/Nano25Oct2019-v1/*/',\n",
    "    'data_D': '/eos/cms/store/data/Run2016D/SingleMuon/NANOAOD/Nano25Oct2019-v1/*/',\n",
    "    'data_E': '/eos/cms/store/data/Run2016E/SingleMuon/NANOAOD/Nano25Oct2019-v1/*/',\n",
    "    'data_F': '/eos/cms/store/data/Run2016F/SingleMuon/NANOAOD/Nano25Oct2019-v1/*/',\n",
    "    'data_G': '/eos/cms/store/data/Run2016G/SingleMuon/NANOAOD/Nano25Oct2019-v1/*/',\n",
    "    'data_H': '/eos/cms/store/data/Run2016H/SingleMuon/NANOAOD/Nano25Oct2019-v1/*/',\n",
    "#     'ggh':,\n",
    "#     'vbf':,\n",
    "#     '':,\n",
    "#     '':,\n",
    "    'dy': '/eos/cms/store/mc/RunIISummer16NanoAODv6/DYJetsToLL_M-50_TuneCUETP8M1_13TeV-amcatnloFXFX-pythia8/NANOAODSIM/*/*/',\n",
    "#     'dy_m105_160_amc': '/eos/cms/store/mc/RunIISummer16NanoAODv6/DYJetsToLL_M-105To160_TuneCP5_PSweights_13TeV-amcatnloFXFX-pythia8/NANOAODSIM/*/*/'\n",
    "    'ttjets_dl': '/eos/cms/store/mc/RunIISummer16NanoAODv6/TTJets_DiLept_TuneCUETP8M1_13TeV-madgraphMLM-pythia8/NANOAODSIM/*/*/',\n",
    "#     'st_t_top':,\n",
    "#     'st_t_antitop':,\n",
    "    'st_tw_top': '/eos/cms/store/mc/RunIISummer16NanoAODv6/ST_tW_top_5f_NoFullyHadronicDecays_13TeV-powheg_TuneCUETP8M1/NANOAODSIM/*/*/',\n",
    "    'st_tw_antitop': '/eos/cms/store/mc/RunIISummer16NanoAODv6/ST_tW_antitop_5f_NoFullyHadronicDecays_13TeV-powheg_TuneCUETP8M1/NANOAODSIM/*/*/',\n",
    "\n",
    "    'ww_2l2nu': '/eos/cms/store/mc/RunIISummer16NanoAODv6/WWTo2L2Nu_13TeV-powheg/NANOAODSIM/*/*/',\n",
    "    'wz_3lnu': '/eos/cms/store/mc/RunIISummer16NanoAODv6/WZTo3LNu_TuneCUETP8M1_13TeV-amcatnloFXFX-pythia8/NANOAODSIM/*/*/',\n",
    "\n",
    "    'www': '/eos/cms/store/mc/RunIISummer16NanoAODv6/WWW_4F_DiLeptonFilter_TuneCUETP8M1_13TeV-amcatnlo-pythia8/NANOAODSIM/*/*/',\n",
    "    'wwz': '/eos/cms/store/mc/RunIISummer16NanoAODv6/WWZ_TuneCUETP8M1_13TeV-amcatnlo-pythia8/NANOAODSIM/*/*/',\n",
    "    'wzz': '/eos/cms/store/mc/RunIISummer16NanoAODv6/WZZ_TuneCUETP8M1_13TeV-amcatnlo-pythia8/NANOAODSIM/*/*/',\n",
    "    'zzz': '/eos/cms/store/mc/RunIISummer16NanoAODv6/ZZZ_TuneCUETP8M1_13TeV-amcatnlo-pythia8/NANOAODSIM/*/*/',\n",
    "}\n",
    "\n",
    "for_debug = ['data_G', 'dy', 'ttjets_dl']\n",
    "\n",
    "# TODO: add all processes \n",
    "# TODO: add possibility to load of files through XrootD\n",
    "\n",
    "samples = paths.keys()\n",
    "\n",
    "debug = True\n",
    "\n",
    "fileset = {}\n",
    "fileset_debug = {}\n",
    "\n",
    "lumi = 1\n",
    "data_entries = 0\n",
    "\n",
    "# Find ROOT files in local directories\n",
    "for sample, path in paths.items():\n",
    "    all_files = []\n",
    "    all_files = glob.glob(path+'*root')\n",
    "    \n",
    "    if debug:\n",
    "        all_files = [all_files[0]]\n",
    "\n",
    "    if 'data' in sample:\n",
    "        for f in all_files:\n",
    "           fi = uproot.open('root://eoscms.cern.ch/'+f)['Events']\n",
    "           data_entries += fi.numentries\n",
    "        \n",
    "    fileset[sample] = {\n",
    "        'files': ['root://eoscms.cern.ch/'+f for f in all_files],\n",
    "        'treename': 'Events'\n",
    "    }\n",
    "\n",
    "    if sample in for_debug:\n",
    "        fileset_debug[sample] = {\n",
    "            'files': ['root://eoscms.cern.ch/'+f for f in all_files],\n",
    "            'treename': 'Events'\n",
    "        }\n",
    "\n",
    "lumi = 35860.*data_entries/645880988.\n",
    "print(f\"Loading {data_entries/645880988.*100}% of 2016 data.\")\n",
    "print(f\"Integrated luminosity {lumi}/pb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lumimasks = {\n",
    "    \"2016\": \"data/lumimasks/Cert_271036-284044_13TeV_23Sep2016ReReco_Collisions16_JSON.txt\",\n",
    "    \"2017\": \"data/lumimasks/Cert_294927-306462_13TeV_EOY2017ReReco_Collisions17_JSON_v1.txt\",\n",
    "    \"2018\": \"data/lumimasks/Cert_314472-325175_13TeV_17SeptEarlyReReco2018ABC_PromptEraD_Collisions18_JSON.txt\",\n",
    "}\n",
    "\n",
    "import python.RoccoR as roccor\n",
    "roccor_files = {\n",
    "    \"2016\": \"data/roch_corr/RoccoR2016.txt\",\n",
    "    \"2017\": \"data/roch_corr/RoccoR2017.txt\",\n",
    "    \"2018\": \"data/roch_corr/RoccoR2018.txt\",\n",
    "}\n",
    "# To generate RoccoR libraries:\n",
    "# cd plugin\n",
    "# make\n",
    "# cd ../\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "@numba.jit(parallel=True)\n",
    "def apply_roccor(roch_corrector, muons, nmuons=2):\n",
    "    corrections = muons.pt.ones_like()\n",
    "    for iev in range(muons.shape[0]):\n",
    "        for imu in range(nmuons):\n",
    "            mu = muons[iev][imu]\n",
    "            corrections[iev][imu] = roch_corrector.kScaleDT(mu.charge, mu.pt, mu.eta, mu.phi)\n",
    "    return corrections\n",
    "\n",
    "\n",
    "puLookup = util.load('data/pileup/puLookup.coffea')\n",
    "muSFFileList = [{'id'   : (\"data/muon_sf/EfficienciesStudies_2016_legacy_rereco_rootfiles_RunBCDEF_SF_ID.root\", \"NUM_TightID_DEN_genTracks_eta_pt\"),\n",
    "                 'iso'   : (\"data/muon_sf/EfficienciesStudies_2016_legacy_rereco_rootfiles_RunBCDEF_SF_ISO.root\", \"NUM_TightRelIso_DEN_TightIDandIPCut_eta_pt\"),\n",
    "                 'trig'  : (\"data/muon_sf/EfficienciesStudies_2016_trigger_EfficienciesAndSF_RunBtoF.root\", \"IsoMu24_OR_IsoTkMu24_PtEtaBins/abseta_pt_ratio\"),\n",
    "                 'scale' : 19.656062760/35.882515396},\n",
    "                {'id'     : (\"data/muon_sf/EfficienciesStudies_2016_legacy_rereco_rootfiles_RunGH_SF_ID.root\", \"NUM_TightID_DEN_genTracks_eta_pt\"),\n",
    "                 'iso'   : (\"data/muon_sf/EfficienciesStudies_2016_legacy_rereco_rootfiles_RunGH_SF_ISO.root\", \"NUM_TightRelIso_DEN_TightIDandIPCut_eta_pt\"),\n",
    "                 'trig'  : (\"data/muon_sf/EfficienciesStudies_2016_trigger_EfficienciesAndSF_RunGtoH.root\", \"IsoMu24_OR_IsoTkMu24_PtEtaBins/abseta_pt_ratio\"),\n",
    "                 'scale' : 16.226452636/35.882515396}]\n",
    "# TODO: check scale\n",
    "# TODO: generate SF for other years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at ProcessorABC documentation to see the expected methods and what they are supposed to do\n",
    "# https://coffeateam.github.io/coffea/api/coffea.processor.ProcessorABC.html\n",
    "class DimuonProcessor(processor.ProcessorABC):\n",
    "    def __init__(self, mass_window=[76,106], do_roccor=True):\n",
    "        self.mass_window = mass_window\n",
    "        self.do_roccor = do_roccor\n",
    "        event_branches = ['run', 'luminosityBlock', 'genWeight']\n",
    "        muon_branches = ['nMuon', 'Muon_pt', 'Muon_eta', 'Muon_phi', 'Muon_mass', 'Muon_charge', 'Muon_pfRelIso04_all']\n",
    "        jet_branches = ['nJet', 'Jet_pt', 'Jet_eta', 'Jet_phi', 'Jet_mass', 'Jet_qgl', 'Jet_jetId', 'Jet_puId']\n",
    "        vtx_branches = ['Pileup_nTrueInt', 'PV_npvsGood'] \n",
    "        other_branches = ['MET_pt']\n",
    "        event_flags = ['Flag_BadPFMuonFilter','Flag_EcalDeadCellTriggerPrimitiveFilter',\n",
    "                        'Flag_HBHENoiseFilter','Flag_HBHENoiseIsoFilter',\n",
    "                        'Flag_globalSuperTightHalo2016Filter','Flag_goodVertices','Flag_BadChargedCandidateFilter']\n",
    "        hlt_branches = ['HLT_IsoMu24', 'HLT_IsoTkMu24']\n",
    "        self._columns = event_branches + muon_branches + jet_branches +\\\n",
    "                        vtx_branches + other_branches + event_flags + hlt_branches\n",
    "\n",
    "        dataset_axis = hist.Cat(\"dataset\", \"\")\n",
    "\n",
    "        axes = {}\n",
    "        axes['dimuon_mass'] =  hist.Bin(\"dimuon_mass\", r\"$m_{\\mu\\mu}$ [GeV]\", 100, self.mass_window[0], self.mass_window[1])\n",
    "        axes['dimuon_pt'] = hist.Bin(\"dimuon_pt\", r\"$p_{T}(\\mu\\mu)$ [GeV]\", 100, 0, 400)\n",
    "        axes['dimuon_eta'] = hist.Bin(\"dimuon_eta\", r\"$\\eta (\\mu\\mu)$ [GeV]\", 100, -5, 5)\n",
    "        axes['dimuon_phi'] = hist.Bin(\"dimuon_phi\", r\"$\\phi (\\mu\\mu)$ [GeV]\", 100, -3.2, 3.2)\n",
    "        axes['dimuon_dEta'] = hist.Bin(\"dimuon_dEta\", r\"$\\Delta\\eta (\\mu\\mu)$ [GeV]\", 100, 0, 5)\n",
    "        axes['dimuon_dPhi'] = hist.Bin(\"dimuon_dPhi\", r\"$\\Delta\\phi (\\mu\\mu)$ [GeV]\", 100, 0, 6.4)\n",
    "\n",
    "        axes['mu1_pt'] = hist.Bin(\"mu1_pt\", r\"$p_{T}(\\mu_{1})$ [GeV]\", 100, 0, 250)\n",
    "        axes['mu1_pt_nosf'] = hist.Bin(\"mu1_pt_nosf\", r\"$p_{T}(\\mu_{1}) no SF$ [GeV]\", 100, 0, 250)\n",
    "        axes['mu1_eta'] = hist.Bin(\"mu1_eta\", r\"$\\eta (\\mu_{1})$ [GeV]\", 100, -2.4, 2.4)\n",
    "        axes['mu1_phi'] = hist.Bin(\"mu1_phi\", r\"$\\phi (\\mu_{1})$ [GeV]\", 100, -3.2, 3.2)\n",
    "        axes['mu2_pt'] = hist.Bin(\"mu2_pt\", r\"$p_{T}(\\mu_{2})$ [GeV]\", 100, 0, 150)\n",
    "        axes['mu2_eta'] = hist.Bin(\"mu2_eta\", r\"$\\eta (\\mu_{2})$ [GeV]\", 100, -2.4, 2.4)\n",
    "        axes['mu2_phi'] = hist.Bin(\"mu2_phi\", r\"$\\phi (\\mu_{2})$ [GeV]\", 100, -3.2, 3.2)\n",
    "\n",
    "        axes['jet1_pt'] = hist.Bin(\"jet1_pt\", r\"$p_{T}(jet1)$ [GeV]\", 100, 0, 400)\n",
    "        axes['jet1_eta'] = hist.Bin(\"jet1_eta\", r\"$\\eta (jet1)$ [GeV]\", 100, -4.7, 4.7)\n",
    "        axes['jet1_phi'] = hist.Bin(\"jet1_phi\", r\"$\\phi (jet1)$ [GeV]\", 100, -3.2, 3.2)\n",
    "        axes['jet1_qgl'] = hist.Bin(\"jet1_qgl\", r\"$QGL (jet1)$ [GeV]\", 10, 0, 1)\n",
    "        \n",
    "        axes['jet2_pt'] = hist.Bin(\"jet2_pt\", r\"$p_{T}(jet2)$ [GeV]\", 100, 0, 300)\n",
    "        axes['jet2_eta'] = hist.Bin(\"jet2_eta\", r\"$\\eta (jet2)$ [GeV]\", 100, -4.7, 4.7)\n",
    "        axes['jet2_phi'] = hist.Bin(\"jet2_phi\", r\"$\\phi (jet2)$ [GeV]\", 100, -3.2, 3.2)\n",
    "        axes['jet2_qgl'] = hist.Bin(\"jet2_qgl\", r\"$QGL (jet2)$ [GeV]\", 10, 0, 1)\n",
    "\n",
    "        \n",
    "        axes['njets'] = hist.Bin(\"njets\", \"njets\", 10, 0, 10)\n",
    "        axes['npv'] = hist.Bin(\"npv\", \"npv\", 50, 0, 50)\n",
    "        axes['npv_prePU'] = hist.Bin(\"npv_prePU\", \"npv_prePU\", 50, 0, 50)\n",
    "        axes['met'] = hist.Bin(\"met\", r\"$E_{T}^{miss.}$ [GeV]\", 100, 0, 200)\n",
    "\n",
    "        axes['genweight'] = hist.Bin(\"genweight\", \"genweight\", 50, 0, 50)\n",
    "        \n",
    "        variables = axes.keys()\n",
    "        \n",
    "        accumulators = {}\n",
    "        for v in variables:\n",
    "            accumulators[v] = hist.Hist(\"Counts\", dataset_axis, axes[v])\n",
    "            # TODO: add category axis and systematics axis\n",
    "            \n",
    "        accumulators['sumGenWeights'] = hist.Hist(\"sumGenWeights\", dataset_axis)\n",
    "        accumulators['entries'] = hist.Hist(\"Entries\", dataset_axis)\n",
    "        accumulators['cutflow'] = processor.defaultdict_accumulator(int)\n",
    "        \n",
    "        accumulators['dimuon_mass_unbinned'] = processor.column_accumulator(np.ndarray([]))\n",
    "        # request: possibility to add another axis to column_accumulator (e.g. 'dataset')\n",
    "        \n",
    "        self._accumulator = processor.dict_accumulator(accumulators)\n",
    "    \n",
    "        mu_id_vals = 0\n",
    "        mu_id_err = 0\n",
    "        mu_iso_vals = 0\n",
    "        mu_iso_err = 0\n",
    "        mu_trig_vals = 0\n",
    "        mu_trig_err = 0\n",
    "\n",
    "        for scaleFactors in muSFFileList:\n",
    "            id_file = uproot.open(scaleFactors['id'][0])\n",
    "            iso_file = uproot.open(scaleFactors['iso'][0])\n",
    "            trig_file = uproot.open(scaleFactors['trig'][0])\n",
    "            \n",
    "            mu_id_vals += id_file[scaleFactors['id'][1]].values * scaleFactors['scale']\n",
    "            mu_id_err += id_file[scaleFactors['id'][1]].variances**0.5 * scaleFactors['scale']\n",
    "            mu_id_edges = id_file[scaleFactors['id'][1]].edges\n",
    "\n",
    "            mu_iso_vals += iso_file[scaleFactors['iso'][1]].values * scaleFactors['scale']\n",
    "            mu_iso_err += iso_file[scaleFactors['iso'][1]].variances**0.5 * scaleFactors['scale']\n",
    "            mu_iso_edges = iso_file[scaleFactors['iso'][1]].edges\n",
    "\n",
    "            mu_trig_vals += trig_file[scaleFactors['trig'][1]].values * scaleFactors['scale']\n",
    "            mu_trig_err += trig_file[scaleFactors['trig'][1]].variances**0.5 * scaleFactors['scale']\n",
    "            mu_trig_edges = trig_file[scaleFactors['trig'][1]].edges\n",
    "\n",
    "        self.mu_id_sf = dense_lookup.dense_lookup(mu_id_vals, mu_id_edges)\n",
    "        self.mu_id_err = dense_lookup.dense_lookup(mu_id_err, mu_id_edges)\n",
    "        self.mu_iso_sf = dense_lookup.dense_lookup(mu_iso_vals, mu_iso_edges)\n",
    "        self.mu_iso_err = dense_lookup.dense_lookup(mu_iso_err, mu_iso_edges)\n",
    "        self.mu_trig_sf = dense_lookup.dense_lookup(mu_trig_vals, mu_trig_edges)\n",
    "        self.mu_trig_err = dense_lookup.dense_lookup(mu_trig_err, mu_trig_edges)    \n",
    "    \n",
    "#         self.extractor = extractor()\n",
    "#         self.extractor.add_weight_sets([\"\"])\n",
    "#         self.extractor.finalize()\n",
    "#         self.evaluator = self.extractor.make_evaluator()\n",
    "#         print(self.evaluator)\n",
    "    \n",
    "        self.roch_corrector = roccor.RoccoR(roccor_files[\"2016\"].encode('utf-8'))\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "    \n",
    "    @property\n",
    "    def columns(self):\n",
    "        return self._columns\n",
    "    \n",
    "    def process(self, df):\n",
    "        output = self.accumulator.identity()\n",
    "        dataset = df.metadata['dataset']\n",
    "        isData = 'data' in dataset\n",
    "       \n",
    "        nEvts = df.shape[0]\n",
    "        \n",
    "        if isData:\n",
    "            lumi_info = LumiMask(lumimasks[\"2016\"])\n",
    "            lumimask = lumi_info(df.run.flatten(), df.luminosityBlock.flatten())\n",
    "            genweight = np.ones(nEvts)\n",
    "            event_weight = np.ones(nEvts)\n",
    "            event_weight_nosf = np.ones(nEvts)\n",
    "            \n",
    "        else:\n",
    "            muID = self.mu_id_sf(df.Muon.eta, df.Muon.pt)\n",
    "            muIso = self.mu_iso_sf(df.Muon.eta, df.Muon.pt)\n",
    "            muTrig = self.mu_iso_sf(abs(df.Muon.eta), df.Muon.pt)\n",
    "            muSF = (muID*muIso*muTrig).prod()\n",
    "            \n",
    "#             muIDerr = self.mu_id_err(muons.eta, muons.pt)\n",
    "#             muIsoerr = self.mu_iso_err(muons.eta, muons.pt)\n",
    "#             muTrigerr = self.mu_iso_err(abs(muons.eta), muons.pt)\n",
    "#             muSF_up = ((muID + muIDerr) * (muIso + muIsoerr) * (muTrig + muTrigerr)).prod()\n",
    "#             muSF_down = ((muID - muIDerr) * (muIso - muIsoerr) * (muTrig - muTrigerr)).prod() \n",
    "    \n",
    "            lumimask = np.ones(nEvts, dtype=bool)\n",
    "            genweight = df.genWeight.array\n",
    "            pu_weight = puLookup(dataset, df.Pileup.nTrueInt)\n",
    "            event_weight = genweight*muSF*pu_weight\n",
    "            event_weight_nosf = genweight*pu_weight\n",
    "\n",
    "        output['sumGenWeights'].fill(dataset=dataset, weight=genweight.sum())\n",
    "\n",
    "        # TODO: move all parameters to external config\n",
    "        # TODO: Add FSR recovery\n",
    "        # TODO: trigger matching for muons\n",
    "        \n",
    "        muons = df.Muon\n",
    "        muons = muons[(muons.pt > 20) & (abs(muons.eta) < 2.4) & (muons.pfRelIso04_all<0.25) & muons.mediumId]\n",
    "    \n",
    "        \n",
    "        event_flags = df.Flag.BadPFMuonFilter & \\\n",
    "                        df.Flag.EcalDeadCellTriggerPrimitiveFilter &\\\n",
    "                        df.Flag.HBHENoiseFilter &\\\n",
    "                        df.Flag.HBHENoiseIsoFilter &\\\n",
    "                        df.Flag.globalSuperTightHalo2016Filter &\\\n",
    "                        df.Flag.goodVertices &\\\n",
    "                        df.Flag.BadChargedCandidateFilter\n",
    "        \n",
    "        hlt = df.HLT.IsoMu24 | df.HLT.IsoTkMu24 \n",
    "        electron_veto = (df.Electron[(df.Electron.pt>20) & (abs(df.Electron.eta)<2.5) & (df.Electron.mvaFall17V2Iso_WP90==1)].counts == 0)\n",
    "        event_filter = (lumimask & event_flags & hlt & (muons.counts == 2) & (muons['charge'].prod() == -1) &\\\n",
    "                        electron_veto & (df.PV.npvsGood > 0)).flatten()\n",
    "        \n",
    "        df = df[event_filter]\n",
    "        muons = muons[event_filter] # 'muons' only stores two muons per event\n",
    "        event_weight = event_weight[event_filter]\n",
    "        event_weight_nosf = event_weight_nosf[event_filter]        \n",
    "        \n",
    "        if self.do_roccor:\n",
    "            roccor_factors = apply_roccor(self.roch_corrector, muons)\n",
    "        else: \n",
    "            roccor_factors = muons.pt.ones_like()\n",
    "        \n",
    "        # for fast calculation of dimuon kinematics\n",
    "        muons_jca = JaggedCandidateArray.candidatesfromcounts(\n",
    "            muons.counts,\n",
    "            pt=muons.pt.content*roccor_factors.content,\n",
    "            eta=muons.eta.content,\n",
    "            phi=muons.phi.content,\n",
    "            mass=muons.mass.content,\n",
    "        )\n",
    "        \n",
    "        dimuons = muons_jca.distincts()\n",
    "        leading_mu = (dimuons.i0.pt.content > dimuons.i1.pt.content)\n",
    "\n",
    "        mu1_pt = np.where(leading_mu, dimuons.i0.pt.content, dimuons.i1.pt.content)\n",
    "        mu1_eta= np.where(leading_mu, dimuons.i0.eta.content, dimuons.i1.eta.content)\n",
    "        mu1_phi= np.where(leading_mu, dimuons.i0.phi.content, dimuons.i1.phi.content)\n",
    "\n",
    "        mu2_pt = np.where(~leading_mu, dimuons.i0.pt.content, dimuons.i1.pt.content)\n",
    "        mu2_eta= np.where(~leading_mu, dimuons.i0.eta.content, dimuons.i1.eta.content)\n",
    "        mu2_phi= np.where(~leading_mu, dimuons.i0.phi.content, dimuons.i1.phi.content)\n",
    "        \n",
    "        dimuon_filter = ((mu1_pt > 26) & (dimuons.mass > self.mass_window[0]) & (dimuons.mass < self.mass_window[1])).flatten()\n",
    "        \n",
    "        df = df[dimuon_filter]   \n",
    "\n",
    "        # this is messy\n",
    "        mu1_pt = mu1_pt[dimuon_filter]\n",
    "        mu1_eta = mu1_eta[dimuon_filter]\n",
    "        mu1_phi = mu1_phi[dimuon_filter]        \n",
    "        mu2_pt = mu2_pt[dimuon_filter]\n",
    "        mu2_eta = mu2_eta[dimuon_filter]\n",
    "        mu2_phi = mu2_phi[dimuon_filter]        \n",
    "        dimuons = dimuons[dimuon_filter]\n",
    "        \n",
    "        event_weight = event_weight[dimuon_filter]\n",
    "        event_weight_nosf = event_weight_nosf[dimuon_filter]\n",
    "    \n",
    "        # TODO: dimuon_costhetaCS, dimuon_phiCS\n",
    "        \n",
    "        # Select and fill jets and jet pairs\n",
    "        # TODO: nsoftjets, dijet mass, eta, phi, deta, dphi\n",
    "        \n",
    "#         print(df.Jet.columns)\n",
    "#         print(df.Jet.btagDeepB)\n",
    "        \n",
    "        jets = df.Jet\n",
    "        \n",
    "        # 2016: loose jetId, loose piId        \n",
    "        jet_id = (jets.jetId >= 1)\n",
    "        jet_puid = (((jets.puId >= 4) & (jets.pt < 50)) | (jets.pt > 50))\n",
    "        \n",
    "\n",
    "        jet_selection = ((jets.pt > 25) & (abs(jets.eta) < 4.7) & jet_id & jet_puid & (jets.qgl > -2))\n",
    "        \n",
    "        jets = jets[jet_selection]\n",
    "        event_weight_jet = event_weight[jet_selection.any()]        \n",
    "        # TODO: DeltaR(j, mu) > 0.4\n",
    "        # TODO: JEC, JER\n",
    "\n",
    "        one_jet = (jet_selection.any() & (jets.counts>0))\n",
    "        two_jets = (jet_selection.any() & (jets.counts>1))\n",
    "        \n",
    "        event_weight_jet1 = event_weight[one_jet]\n",
    "        event_weight_jet2 = event_weight[two_jets]\n",
    "        \n",
    "        # temporary - pT sorting will not stay preserved after jet corrections\n",
    "        jet1 = jets[one_jet,0]\n",
    "        jet2 = jets[two_jets,1]\n",
    "        \n",
    "        # this would be more accurate:\n",
    "#         jet1 = jets[jets.pt.argmax()]\n",
    "        # but I couldn't get jet2 the same way (argsort didn't work with jagged arrays)\n",
    "        \n",
    "        # TODO: kinematic variables of multimuon-multijet system\n",
    "        # >> many of them...\n",
    "        \n",
    "        # TODO: event-by-event mass resolution and calibration\n",
    "        \n",
    "        # Other variables\n",
    "        npv = df.PV.npvsGood      \n",
    "        met = df.MET.pt        \n",
    "\n",
    "        \n",
    "        # TODO: Add initial categorization (ggH vs. VBF)\n",
    "        \n",
    "        # Fill event weights\n",
    "        # TODO: NNLOPS reweighting (ggH), Zpt reweighting, LHE wgts\n",
    "        \n",
    "        # TODO: Add systematic uncertainties\n",
    "        \n",
    "        # TODO: Evaluate DNN\n",
    "        \n",
    "\n",
    "        output['cutflow']['all events'] += nEvts\n",
    "        output['cutflow']['event_filter'] += event_filter.sum()\n",
    "        output['cutflow']['dimuon_filter'] += dimuon_filter.sum()\n",
    "\n",
    "        ### Fill muons ###\n",
    "        output['dimuon_mass'].fill(dataset=dataset, dimuon_mass=dimuons.mass.flatten(), weight=event_weight) \n",
    "        if isData:\n",
    "            output['dimuon_mass_unbinned'] += processor.column_accumulator(dimuons.mass.flatten())\n",
    "        output['dimuon_pt'].fill(dataset=dataset, dimuon_pt=dimuons.pt.flatten(), weight=event_weight)\n",
    "        output['dimuon_eta'].fill(dataset=dataset, dimuon_eta=dimuons.eta.flatten(), weight=event_weight)\n",
    "        output['dimuon_phi'].fill(dataset=dataset, dimuon_phi=dimuons.phi.flatten(), weight=event_weight)\n",
    "    \n",
    "        output['dimuon_dEta'].fill(dataset=dataset, dimuon_dEta=abs(mu1_eta.flatten() - mu2_eta.flatten()), weight=event_weight)\n",
    "\n",
    "#         output['dimuon_dPhi'].fill(dataset=dataset, dimuon_dPhi=abs(mu1.delta_phi(mu2)).flatten(), weight=event_weight)        \n",
    "#         TODO: make sure delta_phi is accessible from uproot_methods/classes/TLorentzVector.py\n",
    "\n",
    "        output['mu1_pt'].fill(dataset=dataset, mu1_pt=mu1_pt.flatten(), weight=event_weight)\n",
    "        output['mu1_pt_nosf'].fill(dataset=dataset, mu1_pt_nosf=mu1_pt.flatten(), weight=event_weight_nosf)\n",
    "        output['mu1_eta'].fill(dataset=dataset, mu1_eta=mu1_eta.flatten(), weight=event_weight)\n",
    "        output['mu1_phi'].fill(dataset=dataset, mu1_phi=mu1_phi.flatten(), weight=event_weight)\n",
    "\n",
    "        output['mu2_pt'].fill(dataset=dataset, mu2_pt=mu2_pt.flatten(), weight=event_weight)\n",
    "        output['mu2_eta'].fill(dataset=dataset, mu2_eta=mu2_eta.flatten(), weight=event_weight)\n",
    "        output['mu2_phi'].fill(dataset=dataset, mu2_phi=mu2_phi.flatten(), weight=event_weight)    \n",
    "        \n",
    "        ### Fill jets ###\n",
    "        output['jet1_pt'].fill(dataset=dataset, jet1_pt=jet1.pt.flatten(), weight=event_weight_jet1)\n",
    "        output['jet1_eta'].fill(dataset=dataset, jet1_eta=jet1.eta.flatten(), weight=event_weight_jet1)\n",
    "        output['jet1_phi'].fill(dataset=dataset, jet1_phi=jet1.phi.flatten(), weight=event_weight_jet1)\n",
    "        output['jet1_qgl'].fill(dataset=dataset, jet1_qgl=jet1.qgl.flatten(), weight=event_weight_jet1)\n",
    "        \n",
    "        output['jet2_pt'].fill(dataset=dataset, jet2_pt=jet2.pt.flatten(), weight=event_weight_jet2)\n",
    "        output['jet2_eta'].fill(dataset=dataset, jet2_eta=jet2.eta.flatten(), weight=event_weight_jet2)\n",
    "        output['jet2_phi'].fill(dataset=dataset, jet2_phi=jet2.phi.flatten(), weight=event_weight_jet2)\n",
    "        output['jet2_qgl'].fill(dataset=dataset, jet2_qgl=jet2.qgl.flatten(), weight=event_weight_jet2)\n",
    "        \n",
    "        output['njets'].fill(dataset=dataset, njets=jets.counts, weight=event_weight)\n",
    "        \n",
    "        ### Fill other variables ###\n",
    "        output['npv'].fill(dataset=dataset, npv=npv, weight=event_weight)\n",
    "        output['met'].fill(dataset=dataset, met=met, weight=event_weight)\n",
    "            \n",
    "        output['entries'].fill(dataset=dataset, weight=nEvts) # temporary\n",
    "        \n",
    "        \n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: Iterative executor\n",
    "===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: iterative processing \n",
    "\n",
    "from coffea.processor.executor import iterative_executor\n",
    "\n",
    "tstart = time.time() \n",
    "output = processor.run_uproot_job(fileset_debug, 'Events', DimuonProcessor(), iterative_executor, executor_args={'nano': True})\n",
    "elapsed = time.time() - tstart\n",
    "\n",
    "print(f\"Processed {output['cutflow']['all events']} events\")\n",
    "print(f\"Total time: {elapsed} s\")\n",
    "print(f\"Rate: {output['cutflow']['all events']/elapsed} events/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: Dask executor\n",
    "===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from coffea.processor.executor import dask_executor\n",
    "\n",
    "distributed = pytest.importorskip(\"distributed\", minversion=\"1.28.1\")\n",
    "client = distributed.Client(processes=False, dashboard_address=None)\n",
    "\n",
    "tstart = time.time() \n",
    "output = processor.run_uproot_job(fileset, 'Events', DimuonProcessor(), dask_executor, executor_args={'nano': True, 'client': client})\n",
    "elapsed = time.time() - tstart\n",
    "\n",
    "print(f\"Processed {output['cutflow']['all events']} events\")\n",
    "print(f\"Total time: {elapsed} s\")\n",
    "print(f\"Rate: {output['cutflow']['all events']/elapsed} events/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 3: Apache Spark\n",
    "===\n",
    "\n",
    "\n",
    "NOW IT IS TIME TO START SPARK CLUSTER CONNECTION\n",
    "---\n",
    "\n",
    "When using SWAN, click on the 5-point start icon in Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql\n",
    "from pyarrow.compat import guid\n",
    "from coffea.processor.spark.detail import _spark_initialize, _spark_stop\n",
    "from coffea.processor.spark.spark_executor import spark_executor\n",
    "\"\"\"\n",
    "# NOT needed on SWAN, spark config is offloaded to spark connector\n",
    "\n",
    "spark_config = pyspark.sql.SparkSession.builder \\\n",
    "    .appName('spark-executor-test-%s' % guid()) \\\n",
    "    .master('local[*]') \\\n",
    "    .config('spark.driver.memory', '4g') \\\n",
    "    .config('spark.executor.memory', '4g') \\\n",
    "    .config('spark.sql.execution.arrow.enabled','true') \\\n",
    "    .config('spark.sql.execution.arrow.maxRecordsPerBatch', 200000)\n",
    "\n",
    "spark = _spark_initialize(config=spark, log_level='WARN', \n",
    "                          spark_progress=False, laurelin_version='0.5.1')\n",
    "\"\"\"\n",
    "partitionsize = 200000\n",
    "thread_workers = 2\n",
    "\n",
    "tstart = time.time() \n",
    "# if jobs fail, it might be because some columns are missing from processor._columns\n",
    "output = processor.run_spark_job(fileset, DimuonProcessor(), spark_executor, \n",
    "                                 spark=spark, partitionsize=partitionsize, thread_workers=thread_workers,\n",
    "                                 executor_args={'file_type': 'edu.vanderbilt.accre.laurelin.Root', 'cache': False}\n",
    "                                )\n",
    "\n",
    "elapsed = time.time() - tstart\n",
    "\n",
    "print(f\"Processed {output['cutflow']['all events']} events\")\n",
    "print(f\"Total time: {elapsed} s\")\n",
    "print(f\"Rate: {output['cutflow']['all events']/elapsed} events/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_variable(fig, var, gs, weights):\n",
    "    \n",
    "    output_copy = output[var].copy() # copy to prevent scaling multiple times\n",
    "    output_copy.scale(weights, axis='dataset')\n",
    "    \n",
    "    # Group the samples for plotting\n",
    "    \n",
    "#     data = output_copy['data']\n",
    "    data_sources = {\n",
    "        'data': ['data_B','data_C','data_D','data_E','data_F','data_G','data_H']    \n",
    "    }\n",
    "    data = output_copy.group('dataset', hist.Cat(\"dataset\", \"Dataset\"), data_sources)\n",
    "    \n",
    "    bkg_sources = {\n",
    "        'dy': ['dy'],\n",
    "#         'dy': ['dy_m105_160_amc'],\n",
    "        'ttbar':['ttjets_dl'],\n",
    "        'single top': ['st_tw_top', 'st_tw_antitop'],\n",
    "        'vv': ['ww_2l2nu', 'wz_3lnu'],\n",
    "        'vvv': ['www','wwz','wzz','zzz']\n",
    "    }\n",
    "\n",
    "    bkg = output_copy.group('dataset', hist.Cat(\"dataset\", \"Dataset\"), bkg_sources)\n",
    "    bkg.axis('dataset').sorting = 'integral' # sort backgrounds by event yields\n",
    "    \n",
    "    data_opts = {'color': 'k', 'marker': '.', 'markersize':15}\n",
    "    stack_fill_opts = {'alpha': 0.8, 'edgecolor':(0,0,0)}\n",
    "    stack_error_opts = {'label':'Stat. unc.','facecolor':(0,0,0,.4), 'hatch':'', 'linewidth': 0}\n",
    "    \n",
    "    # Top panel: Data vs. MC plot\n",
    "    plt1 = fig.add_subplot(gs[0])\n",
    "    ax_bkg = hist.plot1d(bkg, ax=plt1, overlay='dataset', overflow='all', stack=True, fill_opts=stack_fill_opts, error_opts=stack_error_opts)\n",
    "    ax_data = hist.plot1d(data, overlay='dataset', overflow='all', line_opts=None, error_opts=data_opts)\n",
    "    plt1.set_yscale('log')\n",
    "    plt1.set_ylim(0.1, 1e7)\n",
    "    lbl = hep.cms.cmslabel(plt1, data=True, paper=False, year='2016')\n",
    "    plt1.set_xlabel('')\n",
    "    plt1.tick_params(axis='x', labelbottom=False)\n",
    "    plt1.legend(prop={'size': 'xx-small'})\n",
    "    \n",
    "    # Bottom panel: Data/MC ratio plot\n",
    "    plt2 = fig.add_subplot(gs[1], sharex=plt1)\n",
    "    num = data.sum('dataset')\n",
    "    denom = bkg.sum('dataset')\n",
    "    hist.plotratio(num=num, ax=plt2,\n",
    "                    denom=denom,\n",
    "                    error_opts=data_opts, denom_fill_opts={}, guide_opts={},\n",
    "                    unc='num')\n",
    "    \n",
    "    \n",
    "    plt2.axhline(1, ls='--')\n",
    "    plt2.set_ylim([0,2])    \n",
    "    plt2.set_ylabel('Data/MC')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare things to plot\n",
    "from parameters import cross_sections\n",
    "\n",
    "mc_datasets = [s for s in samples if 'data' not in s]\n",
    "\n",
    "lumi_weights = {'data':1}\n",
    "for mc in mc_datasets:\n",
    "    N = output['sumGenWeights'].values()[(mc,)]\n",
    "    lumi_weights[mc] = cross_sections[mc]*lumi / N\n",
    "\n",
    "# print(output['dimuon_mass_unbinned'].value)    \n",
    "    \n",
    "vars_to_plot = []\n",
    "vars_to_plot += ['dimuon_mass', 'dimuon_pt', 'dimuon_eta', 'dimuon_phi']\n",
    "vars_to_plot += ['dimuon_dEta']#, 'dimuon_dPhi']\n",
    "vars_to_plot += ['mu1_pt','mu2_pt', 'mu1_eta','mu2_eta', 'mu1_phi','mu2_phi']\n",
    "vars_to_plot += ['jet1_pt', 'jet1_eta', 'jet1_phi', 'jet1_qgl']\n",
    "vars_to_plot += ['jet2_pt', 'jet2_eta', 'jet2_phi', 'jet2_qgl']\n",
    "# vars_to_plot += ['mu1_pt_nosf']\n",
    "vars_to_plot += ['njets', 'npv', 'met']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make plots iteratively and put into a grid\n",
    "---\n",
    "\n",
    "Slower, but output looks nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "plt.style.use(hep.cms.style.ROOT)\n",
    "from matplotlib import gridspec\n",
    "import math\n",
    "    \n",
    "fig = plt.figure()\n",
    "    \n",
    "nplots_x = 4 # number of plots in one row\n",
    "nplots_y = math.ceil(len(vars_to_plot) / nplots_x) # number of rows\n",
    "\n",
    "plotsize=10\n",
    "ratio_plot_size = 0.25\n",
    "fig.set_size_inches(nplots_x*plotsize,nplots_y*plotsize*(1+ratio_plot_size))\n",
    "outer_grid = gridspec.GridSpec(nplots_y, nplots_x, hspace = .3) \n",
    "for i, var in enumerate(vars_to_plot):\n",
    "    gs = gridspec.GridSpecFromSubplotSpec(2, 1, subplot_spec = outer_grid[i], height_ratios=[(1-ratio_plot_size),ratio_plot_size], hspace = .05)\n",
    "    plot_variable(fig, var, gs, lumi_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make plots in parallel\n",
    "---\n",
    "\n",
    "Faster, but can't display the plots in a grid. Maybe will use to save png/pdf plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import mplhep as hep\n",
    "plt.style.use(hep.cms.style.ROOT)\n",
    "import multiprocessing as mp\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 50\n",
    "    \n",
    "figs = {}\n",
    "def plot_var(i):\n",
    "    fig = plt.figure()\n",
    "    plotsize=8\n",
    "    ratio_plot_size = 0.24\n",
    "    fig.set_size_inches(plotsize,plotsize*(1+ratio_plot_size))\n",
    "    gs = gridspec.GridSpec(2, 1, height_ratios=[(1-ratio_plot_size),ratio_plot_size], hspace = .05)\n",
    "    plot_variable(fig, vars_to_plot[i], gs, lumi_weights)\n",
    "    return fig\n",
    "\n",
    "pool = mp.Pool(mp.cpu_count() - 1)\n",
    "results = [pool.apply(plot_var, args=(x,)) for x in range(len(vars_to_plot))]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "sparkconnect": {
   "bundled_options": [
    "MemoryIntensive",
    "ComputeIntensive"
   ],
   "list_of_options": [
    {
     "name": "spark.kubernetes.container.image",
     "value": "gitlab-registry.cern.ch/db/spark-service/docker-registry/swan:laurelin"
    },
    {
     "name": "spark.sql.execution.arrow.enabled",
     "value": "true"
    },
    {
     "name": "spark.sql.execution.arrow.maxRecordsPerBatch",
     "value": "200000"
    },
    {
     "name": "spark.kubernetes.container.image.pullPolicy",
     "value": "Always"
    },
    {
     "name": "spark.driver.extraClassPath",
     "value": "./laurelin-0.5.1.jar:./lz4-java-1.5.1.jar:./log4j-core-2.11.2.jar:./log4j-api-2.11.2.jar:./xz-1.2.jar"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
