{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <img  src=\"https://swan.web.cern.ch/sites/swan.web.cern.ch/files/pictures/logo_swan_letters.png\" alt=\"SWAN\" style=\"float: left; width: 15%; margin-right: 5%; margin-left: 17%; margin-top: 1.0em; margin-bottom: 2.0em;\">\n",
    "<img src=\"https://spark.apache.org/images/spark-logo-trademark.png\" alt=\"EP-SFT\" style=\"float: left; width: 25%; margin-right: 0%; margin-left: 0%; margin-bottom: 2.0em;\">\n",
    "<img src=\"https://cms-docdb.cern.ch/cgi-bin/PublicDocDB/RetrieveFile?docid=3045&filename=CMSlogo_color_label_1024_May2014.png&version=3\" alt=\"CMS\" style=\"float: left; width: 12%; margin-left: 5%; margin-right: 5%; margin-bottom: 2.0em;\"> -->\n",
    "<p style=\"clear: both;\">\n",
    "<div style=\"text-align:center\"><h1>CMS H&#8594;µµ analysis  \n",
    "     <br> with Coffea package from Fermilab</h1></div>\n",
    "<div style=\"text-align:center\"><i>Author: Dmitry Kondratyev, based on example code by Lindsey Gray</i></div>\n",
    "<hr style=\"border-top-width: 4px; border-top-color: #34609b;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for Higgs boson decaying into two muons\n",
    "\n",
    "This code uses awkward array toolset, and utilizing Coffea [histograms](https://coffeateam.github.io/coffea/modules/coffea.hist.html).\n",
    "This also shows the analysis object syntax implemented by Coffea [JaggedCandidateArray](https://coffeateam.github.io/coffea/api/coffea.analysis_objects.JaggedCandidateMethods.html), and the usage of custom [accumulators](https://coffeateam.github.io/coffea/api/coffea.processor.AccumulatorABC.html) other than histograms.  Further, it introduces the [processor](https://coffeateam.github.io/coffea/api/coffea.processor.ProcessorABC.html) concept and the interface to apache spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions to run at SWAN:\n",
    "===\n",
    "\n",
    "#### Load SWAN environment: LCG96 Python3 stack and Cloud Containers cluster\n",
    "\n",
    "Then run next two cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this cell if you do not have coffea installed (e.g. on SWAN with LCG 96Python3 stack)\n",
    "!pip install --user --upgrade coffea\n",
    "\n",
    "# spark.jars.packages doesnt work with Spark 2.4 with kubernetes\n",
    "!wget -N https://repo1.maven.org/maven2/edu/vanderbilt/accre/laurelin/0.5.1/laurelin-0.5.1.jar && \\\n",
    "wget -N https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-api/2.11.2/log4j-api-2.11.2.jar && \\\n",
    "wget -N https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-core/2.11.2/log4j-core-2.11.2.jar && \\\n",
    "wget -N https://repo1.maven.org/maven2/org/lz4/lz4-java/1.5.1/lz4-java-1.5.1.jar && \\\n",
    "wget -N https://repo1.maven.org/maven2/org/tukaani/xz/1.2/xz-1.2.jar\n",
    "                    \n",
    "!mkdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this cell before establishing spark connection\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONPATH'] = os.environ['PYTHONPATH'] + ':' + '/usr/local/lib/python3.6/site-packages'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions to run at Purdue Jupyter hub:\n",
    "===\n",
    "\n",
    "- Login to interactive node on a cluster (e.g. `hammer.rcac.purdue.edu`)\n",
    "- Activate local conda environment (create a new directory for the environment, if needed):\n",
    "```\n",
    "   module load anaconda/5.3.1-py37 \n",
    "   source activate /home/dkondra/conda_tests/\n",
    "```\n",
    "- Once the environment is activated, `conda install` will automatically install packages to that environment\n",
    "- Install missing packages like this: `conda install [-c conda-forge] <package name>`\n",
    "- `coffea` can be installed like this (it will use conda's `pip`): \n",
    "```\n",
    "    pip install --upgrade coffea\n",
    "```\n",
    "- In order for conda to work with notebooks, install `nb_conda`: \n",
    "```\n",
    "    conda install nb_conda\n",
    "```\n",
    "- After that, in Jupyter notebook there will be an option in **Kernel -> Change Kernel** to run the notebook using desired conda environment\n",
    "- Set up VOMS proxy:\n",
    "```\n",
    "    . setup_proxy.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env X509_USER_PROXY=/home/dkondra/x509up_u616617"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import coffea\n",
    "print(\"Coffea version: \", coffea.__version__)\n",
    "import socket\n",
    "at_purdue = ('hammer' in socket.gethostname())\n",
    "\n",
    "from coffea import util\n",
    "import coffea.processor as processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python.samples_info import SamplesInfo\n",
    "samples = [\n",
    "#   'data_G',\n",
    "\n",
    "#      'dy_0j',\n",
    "#     'dy_1j', \n",
    "#     'dy_2j',\n",
    "#    'dy_m105_160_amc',\n",
    "#     'dy_m105_160_vbf_amc',\n",
    "#     'ewk_lljj_mll50_mjj120', #done\n",
    "#    'ewk_lljj_mll105_160', #done\n",
    "    \"ewk_lljj_mll105_160_ptj0\", #done\n",
    "#    'ttjets_dl', \n",
    "#    'ggh_amcPS', # done\n",
    "#     'vbf_amcPS', # done\n",
    "#     'ggh_powheg', 'ggh_powhegPS',\n",
    "#     'vbf_powheg', 'vbf_powhegPS', 'vbf_powheg_herwig',\n",
    "\n",
    "    \n",
    "#   'ttjets_sl', # fails - too big? #done\n",
    "#     'ttz', # done\n",
    "#     'ttw', # done\n",
    "#    'st_tw_top','st_tw_antitop', # done\n",
    "#    'ww_2l2nu', # done\n",
    "#     'wz_2l2q', # done\n",
    "#     'wz_3lnu', # done\n",
    "#     'wz_1l1nu2q','zz',\n",
    "#    'www','wwz', # done\n",
    "#     'wzz','zzz', # done\n",
    "#       'data_B','data_C','data_D','data_E','data_F','data_G','data_H',\n",
    "\n",
    "]\n",
    "\n",
    "purdue = 'root://xrootd.rcac.purdue.edu/'\n",
    "legrano = 'root://t2-xrdcms.lnl.infn.it:7070//'\n",
    "\n",
    "# samp_info = SamplesInfo(year=\"2016\", out_path='/depot/cms/hmm/coffea/test_large/', server=purdue, datasets_from='purdue', debug=False)\n",
    "samp_info = SamplesInfo(year=\"2016\", out_path='/depot/cms/hmm/coffea/test_large/', server=legrano, datasets_from='pisa', debug=False)\n",
    "\n",
    "# samp_info = SamplesInfo(year=\"2016\", out_path='/depot/cms/hmm/coffea/test_long/', server=purdue, debug=False)\n",
    "samp_info.load(samples)\n",
    "samp_info.compute_lumi_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterative executor\n",
    "===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea.processor.executor import iterative_executor\n",
    "from python.dimuon_processor import DimuonProcessor\n",
    "\n",
    "# include this to make it work at Purdue:\n",
    "from ipywidgets import IntProgress, HBox, HTML\n",
    "# however, still doesn't show a nice progress bar widget\n",
    "\n",
    "tstart = time.time() \n",
    "output = processor.run_uproot_job(samp_info.full_fileset, 'Events', DimuonProcessor(\n",
    "                                                                     samp_info=samp_info,\\\n",
    "                                                                     do_roccor=False,\\\n",
    "                                                                     do_fsr=True,\\\n",
    "                                                                     evaluate_dnn=False,\\\n",
    "                                                                     do_timer=True),\\\n",
    "                                        iterative_executor, executor_args={'nano': True})\n",
    "elapsed = time.time() - tstart\n",
    "\n",
    "print(f\"Processed {output['cutflow']['all events']} events\")\n",
    "print(f\"Total time: {elapsed} s\")\n",
    "print(f\"Rate: {output['cutflow']['all events']/elapsed} events/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask executor\n",
    "===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "from coffea.processor.executor import dask_executor\n",
    "import dask\n",
    "from python.dimuon_processor import DimuonProcessor\n",
    "\n",
    "\n",
    "if at_purdue:\n",
    "    n_workers = 18\n",
    "else:\n",
    "    n_workers = 4\n",
    "\n",
    "distributed = pytest.importorskip(\"distributed\", minversion=\"1.28.1\")\n",
    "distributed.config['distributed']['worker']['memory']['terminate'] = False\n",
    "client = distributed.Client(processes=True, dashboard_address=None, n_workers=n_workers, threads_per_worker=1) \n",
    "\n",
    "tstart = time.time()\n",
    " \n",
    "for label, fileset_ in samp_info.filesets.items():\n",
    "    print(f\"Processing {label}...\")\n",
    "    output = processor.run_uproot_job(fileset_, 'Events',\\\n",
    "                                  DimuonProcessor(samp_info=samp_info,\\\n",
    "                                                  do_fsr=True,\\\n",
    "                                                  do_roccor=True,\\\n",
    "                                                  evaluate_dnn=False,),\\\n",
    "                                  dask_executor,\\\n",
    "                                  executor_args={'nano': True, 'client': client})\n",
    "\n",
    "    prefix = \"\"\n",
    "    out_path = f\"{samp_info.out_path}/{prefix}{label}.coffea\"\n",
    "    util.save(output, out_path)\n",
    "    print(f\"Saved output to {out_path}\")   \n",
    "    \n",
    "elapsed = time.time() - tstart\n",
    "\n",
    "print(f\"Total time: {elapsed} s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting\n",
    "===\n",
    "\n",
    "Make plots and put them into a grid\n",
    "\n",
    "Plot Data/MC comparison\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from python.plotting import Plotter\n",
    "from python.dimuon_processor import DimuonProcessor\n",
    "    \n",
    "vars_to_plot = []\n",
    "\n",
    "# vars_to_plot += ['dimuon_mass', 'dimuon_pt']\n",
    "# vars_to_plot += ['dimuon_mass_res', 'dimuon_mass_res_rel']\n",
    "# vars_to_plot += ['dimuon_eta', 'dimuon_phi']\n",
    "# vars_to_plot += ['dimuon_dEta', 'dimuon_dPhi', 'dimuon_dR']\n",
    "# vars_to_plot += ['mu1_pt', 'mu1_pt_over_mass', 'mu1_eta', 'mu1_phi']\n",
    "# vars_to_plot += ['mu2_pt', 'mu2_pt_over_mass', 'mu2_eta', 'mu2_phi']\n",
    "# vars_to_plot += ['jet1_pt', 'jet1_eta', 'jet1_phi', 'jet1_qgl']\n",
    "vars_to_plot += ['jet2_pt']#, 'jet2_eta', 'jet2_phi', 'jet2_qgl']\n",
    "# vars_to_plot += ['deta_mumuj1', 'dphi_mumuj1']\n",
    "# vars_to_plot += ['deta_mumuj2', 'dphi_mumuj2']\n",
    "# vars_to_plot += ['jj_mass']\n",
    "# vars_to_plot += ['jj_pt', 'jj_eta', 'jj_phi']\n",
    "# vars_to_plot += ['jj_dEta', 'jj_dPhi']\n",
    "# vars_to_plot += ['njets', 'npv', 'met']\n",
    "# vars_to_plot += ['dnn_score']\n",
    "\n",
    "all_plots_2016_pars = {\n",
    "    'processor': DimuonProcessor(),\n",
    "    'path': '/depot/cms/hmm/coffea/test_large/',\n",
    "    'prefix': '',\n",
    "    'samples': [\n",
    "        'data_B','data_C','data_D','data_E','data_F','data_G','data_H',\n",
    "        'dy_0j', 'dy_1j', 'dy_2j',\n",
    "        'dy_m105_160_amc',\n",
    "        'dy_m105_160_vbf_amc',\n",
    "        'ewk_lljj_mll50_mjj120', \n",
    "        'ewk_lljj_mll105_160_ptj0',\n",
    "        'ttjets_dl', \n",
    "        'ggh_amcPS', 'vbf_amcPS',\n",
    "        'ttjets_sl', 'ttz', 'ttw',\n",
    "        'st_tw_top','st_tw_antitop',\n",
    "        'ww_2l2nu','wz_2l2q','wz_3lnu','wz_1l1nu2q','zz',\n",
    "        'www','wwz','wzz','zzz',\n",
    "        ],\n",
    "    'vars': vars_to_plot,\n",
    "    'year': '2016',\n",
    "    'regions' : [\"z-peak\", \"h-sidebands\", \"h-peak\"],\n",
    "    'channels': [\"ggh_01j\", \"ggh_2j\", \"vbf\"], \n",
    "#     'regions' : [\"h-peak\"],\n",
    "#     'channels': [\"vbf\"], \n",
    "}\n",
    "\n",
    "all_plots = Plotter(**all_plots_2016_pars)\n",
    "all_plots.make_datamc_comparison(do_inclusive=True, do_exclusive=True, normalize=False, logy=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from python.plotting import Plotter\n",
    "from python.dimuon_processor import DimuonProcessor\n",
    "    \n",
    "vars_to_plot = []\n",
    "\n",
    "# vars_to_plot += ['dimuon_mass', 'dimuon_pt']\n",
    "# vars_to_plot += ['dimuon_mass_res', 'dimuon_mass_res_rel']\n",
    "# vars_to_plot += ['dimuon_eta', 'dimuon_phi']\n",
    "# vars_to_plot += ['dimuon_dEta', 'dimuon_dPhi', 'dimuon_dR']\n",
    "# vars_to_plot += ['mu1_pt', 'mu1_pt_over_mass', 'mu1_eta', 'mu1_phi']\n",
    "# vars_to_plot += ['mu2_pt', 'mu2_pt_over_mass', 'mu2_eta', 'mu2_phi']\n",
    "# vars_to_plot += ['jet1_pt', 'jet1_eta', 'jet1_phi', 'jet1_qgl']\n",
    "vars_to_plot += ['jet2_pt']#, 'jet2_eta', 'jet2_phi', 'jet2_qgl']\n",
    "# vars_to_plot += ['deta_mumuj1', 'dphi_mumuj1']\n",
    "# vars_to_plot += ['deta_mumuj2', 'dphi_mumuj2']\n",
    "# vars_to_plot += ['jj_mass']\n",
    "# vars_to_plot += ['jj_pt', 'jj_eta', 'jj_phi']\n",
    "# vars_to_plot += ['jj_dEta', 'jj_dPhi']\n",
    "# vars_to_plot += ['njets', 'npv', 'met']\n",
    "# vars_to_plot += ['dnn_score']\n",
    "\n",
    "all_plots_2016_pars = {\n",
    "    'processor': DimuonProcessor(),\n",
    "    'path': '/depot/cms/hmm/coffea/test_large/',\n",
    "    'prefix': '',\n",
    "    'samples': [\n",
    "        'data_B','data_C','data_D','data_E','data_F','data_G','data_H',\n",
    "        'dy_0j', 'dy_1j', 'dy_2j',\n",
    "        'dy_m105_160_amc',\n",
    "        'dy_m105_160_vbf_amc',\n",
    "        'ewk_lljj_mll50_mjj120', \n",
    "        'ewk_lljj_mll105_160',\n",
    "        'ttjets_dl', \n",
    "        'ggh_amcPS', 'vbf_amcPS',\n",
    "        'ttjets_sl', 'ttz', 'ttw',\n",
    "        'st_tw_top','st_tw_antitop',\n",
    "        'ww_2l2nu','wz_2l2q','wz_3lnu','wz_1l1nu2q','zz',\n",
    "        'www','wwz','wzz','zzz',\n",
    "        ],\n",
    "    'vars': vars_to_plot,\n",
    "    'year': '2016',\n",
    "    'regions' : [\"z-peak\", \"h-sidebands\", \"h-peak\"],\n",
    "    'channels': [\"ggh_01j\", \"ggh_2j\", \"vbf\"], \n",
    "#     'regions' : [\"h-peak\"],\n",
    "#     'channels': [\"vbf\"], \n",
    "}\n",
    "\n",
    "all_plots = Plotter(**all_plots_2016_pars)\n",
    "all_plots.make_datamc_comparison(do_inclusive=True, do_exclusive=True, normalize=False, logy=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot jet distributions for new EWK samples\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from python.plotting import Plotter\n",
    "from python.dimuon_processor import DimuonProcessor\n",
    "\n",
    "ewz_study_pars = {\n",
    "    'processor': DimuonProcessor(),\n",
    "    'path': '/depot/cms/hmm/coffea/pisa_test/',\n",
    "    'prefix': '',\n",
    "#     'samples': ['ewk_lljj_mll105_160_ptj0b'],\n",
    "    'samples': [\n",
    "        'data_B','data_C','data_D','data_E','data_F','data_G','data_H',\n",
    "        'dy_0j', 'dy_1j', 'dy_2j',\n",
    "        'dy_m105_160_amc',\n",
    "        'dy_m105_160_vbf_amc',\n",
    "        'ewk_lljj_mll50_mjj120', 'ewk_lljj_mll105_160',\n",
    "        'ttjets_dl', \n",
    "        'ggh_amcPS', 'vbf_amcPS',\n",
    "        'ttjets_sl', 'ttz', 'ttw',\n",
    "        'st_tw_top','st_tw_antitop',\n",
    "        'ww_2l2nu','wz_2l2q','wz_3lnu','wz_1l1nu2q','zz',\n",
    "        'www','wwz','wzz','zzz',\n",
    "    ],\n",
    "#     'vars': ['jet1_pt', 'jet1_eta', 'jet2_pt', 'jet2_eta'],\n",
    "    'vars': ['htsoft2', 'htsoft5', 'nsoftjets2', 'nsoftjets5'],\n",
    "#     'vars': ['dimuon_mass'],\n",
    "    'year': '2016',\n",
    "    'regions' : [\"z-peak\", \"h-sidebands\", \"h-peak\"],\n",
    "    'channels': [\"ggh_01j\", \"ggh_2j\", \"vbf\"], \n",
    "}\n",
    "\n",
    "ewz_study_plots = Plotter(**ewz_study_pars)\n",
    "# ewz_study_plots.make_shape_comparison(do_inclusive=True, do_exclusive=False)\n",
    "ewz_study_plots.make_datamc_comparison(do_inclusive=True, do_exclusive=False, logy=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roccor test\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import coffea\n",
    "print(\"Coffea version: \", coffea.__version__)\n",
    "import socket\n",
    "at_purdue = ('hammer' in socket.gethostname())\n",
    "\n",
    "from coffea import util\n",
    "import coffea.processor as processor\n",
    "\n",
    "import pytest\n",
    "from coffea.processor.executor import dask_executor\n",
    "import dask\n",
    "from python.dimuon_processor import DimuonProcessor\n",
    "\n",
    "if at_purdue:\n",
    "    n_workers = 18\n",
    "else:\n",
    "    n_workers = 4\n",
    "\n",
    "distributed = pytest.importorskip(\"distributed\", minversion=\"1.28.1\")\n",
    "distributed.config['distributed']['worker']['memory']['terminate'] = False\n",
    "client = distributed.Client(processes=True, dashboard_address=None, n_workers=n_workers, threads_per_worker=1) \n",
    "\n",
    "from python.samples_info import SamplesInfo\n",
    "\n",
    "purdue = 'root://xrootd.rcac.purdue.edu/'\n",
    "samp_info_fsr_test = SamplesInfo(year=\"2016\", out_path='/depot/cms/hmm/coffea/roccor_test/', server=purdue, datasets_from='purdue', debug=True)\n",
    "samp_info_fsr_test.load(['dy_m105_160_amc'])\n",
    "samp_info_fsr_test.compute_lumi_weights()\n",
    "\n",
    "for do_roccor in [True, False]:\n",
    "    for label, fileset_ in samp_info_fsr_test.filesets.items():\n",
    "        print(f\"Processing {label}...\")\n",
    "        output = processor.run_uproot_job(fileset_, 'Events',\\\n",
    "                                      DimuonProcessor(samp_info=samp_info_fsr_test,\\\n",
    "                                                      do_fsr=True,\\\n",
    "                                                      do_roccor=do_roccor,\\\n",
    "                                                      evaluate_dnn=False,),\\\n",
    "                                      dask_executor,\\\n",
    "                                      executor_args={'nano': True, 'client': client})\n",
    "\n",
    "        prefix = \"roccor_\" if do_roccor else \"noroccor_\"\n",
    "        out_path = f\"{samp_info_fsr_test.out_path}/{prefix}{label}.coffea\"\n",
    "        util.save(output, out_path)\n",
    "        print(f\"Saved output to {out_path}\") \n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "from python.plotting import Plotter\n",
    "from python.dimuon_processor import DimuonProcessor\n",
    "\n",
    "fsr_study_pars = {\n",
    "    'processor': DimuonProcessor(),\n",
    "    'path': '/depot/cms/hmm/coffea/roccor_test/',\n",
    "    'prefix': '',\n",
    "    'files': ['roccor_dy_m105_160_amc', 'noroccor_dy_m105_160_amc'],\n",
    "    'samples': ['dy_m105_160_amc'],\n",
    "    'vars': ['dimuon_mass'],\n",
    "    'year': '2016',\n",
    "    'regions' : [\"h-sidebands\", \"h-peak\", \"z-peak\"],\n",
    "    'channels': [\"vbf\"], \n",
    "    'merge_files': False,\n",
    "}\n",
    "\n",
    "fsr_study_plots = Plotter(**fsr_study_pars)\n",
    "fsr_study_plots.make_shape_comparison(do_inclusive=True, do_exclusive=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env: conda_tests]",
   "language": "python",
   "name": "conda-env-conda_tests-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "sparkconnect": {
   "bundled_options": [
    "MemoryIntensive",
    "ComputeIntensive",
    "LongRunningAnalysis"
   ],
   "list_of_options": [
    {
     "name": "spark.kubernetes.container.image",
     "value": "gitlab-registry.cern.ch/db/spark-service/docker-registry/swan:laurelin"
    },
    {
     "name": "spark.sql.execution.arrow.enabled",
     "value": "true"
    },
    {
     "name": "spark.sql.execution.arrow.maxRecordsPerBatch",
     "value": "200000"
    },
    {
     "name": "spark.kubernetes.container.image.pullPolicy",
     "value": "Always"
    },
    {
     "name": "spark.driver.extraClassPath",
     "value": "./laurelin-0.5.1.jar:./lz4-java-1.5.1.jar:./log4j-core-2.11.2.jar:./log4j-api-2.11.2.jar:./xz-1.2.jar"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
