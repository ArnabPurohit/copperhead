{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <img  src=\"https://swan.web.cern.ch/sites/swan.web.cern.ch/files/pictures/logo_swan_letters.png\" alt=\"SWAN\" style=\"float: left; width: 15%; margin-right: 5%; margin-left: 17%; margin-top: 1.0em; margin-bottom: 2.0em;\">\n",
    "<img src=\"https://spark.apache.org/images/spark-logo-trademark.png\" alt=\"EP-SFT\" style=\"float: left; width: 25%; margin-right: 0%; margin-left: 0%; margin-bottom: 2.0em;\">\n",
    "<img src=\"https://cms-docdb.cern.ch/cgi-bin/PublicDocDB/RetrieveFile?docid=3045&filename=CMSlogo_color_label_1024_May2014.png&version=3\" alt=\"CMS\" style=\"float: left; width: 12%; margin-left: 5%; margin-right: 5%; margin-bottom: 2.0em;\"> -->\n",
    "<p style=\"clear: both;\">\n",
    "<div style=\"text-align:center\"><h1>CMS H&#8594;µµ analysis with Apache Spark  \n",
    "     <br> using Coffea and Laurelin packages from Fermilab</h1></div>\n",
    "<div style=\"text-align:center\"><i>Author: Dmitry Kondratyev, based on example code by Lindsey Gray</i></div>\n",
    "<hr style=\"border-top-width: 4px; border-top-color: #34609b;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for Higgs boson decaying into two muons\n",
    "\n",
    "This code uses awkward array toolset, and utilizing Coffea [histograms](https://coffeateam.github.io/coffea/modules/coffea.hist.html).\n",
    "This also shows the analysis object syntax implemented by Coffea [JaggedCandidateArray](https://coffeateam.github.io/coffea/api/coffea.analysis_objects.JaggedCandidateMethods.html), and the usage of custom [accumulators](https://coffeateam.github.io/coffea/api/coffea.processor.AccumulatorABC.html) other than histograms.  Further, it introduces the [processor](https://coffeateam.github.io/coffea/api/coffea.processor.ProcessorABC.html) concept and the interface to apache spark.\n",
    "\n",
    "#### SWAN env: LCG96 Python3 stack and Cloud Containers cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you do not have coffea installed (e.g. on SWAN with LCG 96Python3 stack)\n",
    "!pip install --user --upgrade coffea\n",
    "\n",
    "# spark.jars.packages doesnt work with Spark 2.4 with kubernetes\n",
    "!wget -N https://repo1.maven.org/maven2/edu/vanderbilt/accre/laurelin/0.5.1/laurelin-0.5.1.jar && \\\n",
    "wget -N https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-api/2.11.2/log4j-api-2.11.2.jar && \\\n",
    "wget -N https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-core/2.11.2/log4j-core-2.11.2.jar && \\\n",
    "wget -N https://repo1.maven.org/maven2/org/lz4/lz4-java/1.5.1/lz4-java-1.5.1.jar && \\\n",
    "wget -N https://repo1.maven.org/maven2/org/tukaani/xz/1.2/xz-1.2.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell before establishing spark connection\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONPATH'] = os.environ['PYTHONPATH'] + ':' + '/usr/local/lib/python3.6/site-packages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import coffea\n",
    "print(coffea.__version__)\n",
    "\n",
    "from coffea import hist\n",
    "from coffea.analysis_objects import JaggedCandidateArray\n",
    "import coffea.processor as processor\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "paths = {\n",
    "'data': ['/eos/cms/store/data/Run2016C/SingleMuon/NANOAOD/Nano25Oct2019-v1/40000/',], # can add more\n",
    "'dy': '/eos/cms/store/mc/RunIISummer16NanoAODv6/DYJetsToLL_M-50_TuneCUETP8M1_13TeV-amcatnloFXFX-pythia8/NANOAODSIM/PUMoriond17_Nano25Oct2019_102X_mcRun2_asymptotic_v7_ext2-v1/100000/',\n",
    "'ttjets_dl': '/eos/cms/store/mc/RunIISummer16NanoAODv6/TTJets_DiLept_TuneCUETP8M1_13TeV-madgraphMLM-pythia8/NANOAODSIM/PUMoriond17_Nano25Oct2019_102X_mcRun2_asymptotic_v7-v1/270000/'\n",
    "}\n",
    "\n",
    "# TODO: add remote loading of files\n",
    "# TODO: add Golden JSON lumi-section filter\n",
    "\n",
    "samples = paths.keys()\n",
    "\n",
    "fileset = {}\n",
    "fileset_lite = {}\n",
    "for sample, path in paths.items():\n",
    "    all_files = []\n",
    "    if isinstance(path, list): # possibility to load files from multiple local paths\n",
    "        for p in path:\n",
    "            all_files += glob.glob(p+'*root')\n",
    "    else:                      # ... or just one local path\n",
    "        all_files = glob.glob(path+'*root')\n",
    "    \n",
    "    print(f\"{sample}: {len(all_files)} files\")\n",
    "    \n",
    "    fileset[sample] = {\n",
    "        'files': ['root://eoscms.cern.ch/'+f for f in all_files],\n",
    "        'treename': 'Events'\n",
    "    }\n",
    "\n",
    "    fileset_lite[sample] = {\n",
    "        'files': ['root://eoscms.cern.ch/'+all_files[0]], # for debugging, use only one file from each sample\n",
    "        'treename': 'Events'\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at ProcessorABC documentation to see the expected methods and what they are supposed to do\n",
    "# https://coffeateam.github.io/coffea/api/coffea.processor.ProcessorABC.html\n",
    "class DimuonProcessor(processor.ProcessorABC):\n",
    "    def __init__(self, nano=True):\n",
    "        self.nano = nano\n",
    "        self._columns = ['nMuon', 'Muon_pt', 'Muon_eta', 'Muon_phi', 'Muon_mass', 'Muon_charge',\n",
    "                         'nJet', 'Jet_pt', 'Jet_eta', 'Jet_phi', 'Jet_mass',\n",
    "                        'PV_npvsGood', 'genWeight']\n",
    "        dataset_axis = hist.Cat(\"dataset\", \"\")\n",
    "\n",
    "        axes = {}\n",
    "        axes['dimuon_mass'] =  hist.Bin(\"dimuon_mass\", r\"$m_{\\mu\\mu}$ [GeV]\", 100, 76, 106)\n",
    "        axes['dimuon_pt'] = hist.Bin(\"dimuon_pt\", r\"$p_{T}(\\mu\\mu)$ [GeV]\", 100, 0, 400)\n",
    "        axes['mu1_pt'] = hist.Bin(\"mu1_pt\", r\"$p_{T}(\\mu_{1})$ [GeV]\", 100, 0, 400)\n",
    "        axes['mu2_pt'] = hist.Bin(\"mu2_pt\", r\"$p_{T}(\\mu_{2})$ [GeV]\", 100, 0, 300)\n",
    "        axes['jet_pt'] = hist.Bin(\"jet_pt\", r\"$p_{T}(jet)$ [GeV]\", 100, 0, 400)\n",
    "        axes['njets'] = hist.Bin(\"njets\", \"njets\", 10, 0, 10)\n",
    "        axes['npv'] = hist.Bin(\"npv\", \"npv\", 50, 0, 50)\n",
    "        axes['genweight'] = hist.Bin(\"genweight\", \"genweight\", 50, 0, 50)\n",
    "        # TODO: reimplement genweight (weighting should be done before filling histograms)\n",
    "        \n",
    "        variables = axes.keys()\n",
    "        \n",
    "        accumulators = {}\n",
    "        for v in variables:\n",
    "            accumulators[v] = hist.Hist(\"Counts\", dataset_axis, axes[v])\n",
    "            \n",
    "        accumulators['sumGenWeights'] = hist.Hist(\"sumGenWeights\", dataset_axis)\n",
    "        accumulators['entries'] = hist.Hist(\"Entries\", dataset_axis)\n",
    "        accumulators['cutflow'] = processor.defaultdict_accumulator(int)\n",
    "        \n",
    "        self._accumulator = processor.dict_accumulator(accumulators)\n",
    "    \n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "    \n",
    "    @property\n",
    "    def columns(self):\n",
    "        return self._columns\n",
    "    \n",
    "    def process(self, df):\n",
    "        output = self.accumulator.identity()\n",
    "        \n",
    "        if self.nano:\n",
    "            dataset = df.metadata['dataset']\n",
    "            \n",
    "            hlt = np.logical_or(df.HLT.IsoMu24, df.HLT.IsoTkMu24)\n",
    "            \n",
    "            # muons = df.Muon\n",
    "            # jets = df.Jet\n",
    "            \n",
    "            # Use JaggedCandidateArrays to automatically add 4-vectors\n",
    "            muons = JaggedCandidateArray.candidatesfromcounts(\n",
    "                df.Muon.counts,\n",
    "                pt=df.Muon.pt.content,\n",
    "                eta=df.Muon.eta.content,\n",
    "                phi=df.Muon.phi.content,\n",
    "                mass=df.Muon.mass.content,\n",
    "                charge=df.Muon.charge.content,\n",
    "            )\n",
    "            jets = JaggedCandidateArray.candidatesfromcounts(\n",
    "                df.Jet.counts,\n",
    "                pt=df.Jet.pt.content,\n",
    "                eta=df.Jet.eta.content,\n",
    "                phi=df.Jet.phi.content,\n",
    "                mass=df.Jet.mass.content\n",
    "            )\n",
    "        \n",
    "            genweight = df.genWeight.array\n",
    "            npv = df.PV.npvs\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            dataset = df['dataset']\n",
    "\n",
    "            hlt = np.logical_or(df[\"HLT_IsoMu24\"], df[\"HLT_IsoTkMu24\"])\n",
    "            \n",
    "            muons = JaggedCandidateArray.candidatesfromcounts(\n",
    "                df['nMuon'],\n",
    "                pt=df['Muon_pt'].content,\n",
    "                eta=df['Muon_eta'].content,\n",
    "                phi=df['Muon_phi'].content,\n",
    "                mass=df['Muon_mass'].content,\n",
    "                charge=df['Muon_charge'].content,\n",
    "            )\n",
    "        \n",
    "            jets = JaggedCandidateArray.candidatesfromcounts(\n",
    "                df['nJet'],\n",
    "                pt=df['Jet_pt'].content,\n",
    "                eta=df['Jet_eta'].content,\n",
    "                phi=df['Jet_phi'].content,\n",
    "                mass=df['Jet_mass'].content,\n",
    "            )\n",
    "        \n",
    "            genweight = df['genWeight']\n",
    "            npv = df['PV_npvsGood']\n",
    "            \n",
    "        \n",
    "        # Select and fill general event info\n",
    "        # TODO: electron veto\n",
    "        \n",
    "        if 'data' not in dataset:\n",
    "            output['sumGenWeights'].fill(dataset=dataset, weight=genweight)\n",
    "        \n",
    "        # Select and fill muons and dimuon pair\n",
    "        \n",
    "        # TODO: Add FSR recovery here\n",
    "\n",
    "        # TODO: Add muon scale factors\n",
    "        \n",
    "        output['cutflow']['all events'] += muons.size\n",
    "           \n",
    "        #print(muons['__fast_pt'].shape) # yields number of events\n",
    "        #print(muons.pt.shape) # yields number of events\n",
    "        #print(muons.pt.content.shape) # yields number of muons\n",
    "        \n",
    "        #print(f\"nEvts: {muons.shape}\") # yields number of events\n",
    "        #print(muons.content.shape) # yields number of muons\n",
    "        \n",
    "        muons = muons[muons.pt > 20]\n",
    "        event_filter = (hlt & (muons.counts == 2) & (muons['charge'].prod() == -1)).flatten()\n",
    "        # event_filter has dimensionality = nEvts\n",
    "        \n",
    "        muons = muons[event_filter]\n",
    "        \n",
    "        # TODO: Add Rochester correction\n",
    "            \n",
    "        dimuons = muons.distincts()\n",
    "\n",
    "        leading_mu_pt = dimuons.i0.pt\n",
    "        subleading_mu_pt = dimuons.i1.pt\n",
    "            \n",
    "        #TODO: check if this implementation from Coffea example is better\n",
    "#         leading_mu = (dimuons.i0.pt.content > dimuons.i1.pt.content)\n",
    "#         pt_lead = JaggedArray.fromoffsets(dimuons.offsets, np.where(leading_mu,\n",
    "#                                                                     dimuons.i0.pt.content, dimuons.i1.pt.content))\n",
    "#         pt_trail = JaggedArray.fromoffsets(dimuons.offsets, np.where(~leading_mu,\n",
    "#                                                                      dimuons.i0.pt.content, dimuons.i1.pt.content))\n",
    "\n",
    "        dimuon_filter = ((leading_mu_pt > 26) & (dimuons.mass > 76) & (dimuons.mass < 106)).flatten()\n",
    "        # dimuon_filter has dimensionality = events passing event_filter\n",
    "        \n",
    "        dimuons = dimuons[dimuon_filter]\n",
    "        muons = muons[dimuon_filter]\n",
    "        \n",
    "        output['cutflow']['event_filter'] += event_filter.sum()\n",
    "        output['cutflow']['dimuon_filter'] += dimuon_filter.sum()\n",
    "\n",
    "        \n",
    "        output['dimuon_mass'].fill(dataset=dataset, dimuon_mass=dimuons.mass.flatten())        \n",
    "        output['dimuon_pt'].fill(dataset=dataset, dimuon_pt=dimuons.pt.flatten())\n",
    "        output['mu1_pt'].fill(dataset=dataset, mu1_pt=leading_mu_pt.flatten())\n",
    "        output['mu2_pt'].fill(dataset=dataset, mu2_pt=subleading_mu_pt.flatten())\n",
    "        output['entries'].fill(dataset=dataset, weight=muons.size) # temporary\n",
    "        # TODO: dimuon_eta, dimuon_phi, dimuon_deta, dimuon_dphi, dimuon_costhetaCS, dimuon_phiCS,\n",
    "        #       eta, phi of mu1 and mu2\n",
    "        \n",
    "        # Select and fill jets and jet pairs\n",
    "        # TODO: njets, nsoftjets, jet pT, eta, phi, QGL; dijet mass, eta, phi, deta, dphi\n",
    "        \n",
    "        jets = jets[jets.pt > 20]\n",
    "        jets = jets[event_filter]\n",
    "        jets = jets[dimuon_filter]\n",
    "\n",
    "        output['jet_pt'].fill(dataset=dataset, jet_pt=jets.pt.flatten())\n",
    "        output['njets'].fill(dataset=dataset, njets=jets.counts)\n",
    "        \n",
    "        # Fill kinematic variables of multimuon-multijet system\n",
    "        # >> many of them...\n",
    "        \n",
    "        # Fill other variables\n",
    "        # TODO: number of vertices, MET\n",
    "        npv = npv[event_filter]\n",
    "        output['npv'].fill(dataset=dataset, npv=npv)\n",
    "        \n",
    "        # TODO: Add initial categorization (ggH vs. VBF)\n",
    "        \n",
    "        # Fill event weights\n",
    "        # TODO: PU weight, NNLOPS reweighting, Zpt reweighting\n",
    "        \n",
    "        # TODO: Add systematic uncertainties\n",
    "        \n",
    "        # TODO: Evaluate DNN\n",
    "        \n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: Uproot\n",
    "===\n",
    "Files will be read with NanoEvents (`nano = True`) or LazyDataframe (`nano = False`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: iterative processing \n",
    "# skip this cell if running Spark\n",
    "# skip next cell if debugging\n",
    "\n",
    "nano = False\n",
    "\n",
    "from coffea.processor.executor import iterative_executor\n",
    "output = processor.run_uproot_job(fileset_lite, 'Events', DimuonProcessor(nano=nano), iterative_executor, executor_args={'nano': nano})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: Apache Spark\n",
    "===\n",
    "Files will be read with LazyDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW IT IS TIME TO START SPARK CLUSTER CONNECTION\n",
    "\n",
    "When using SWAN, click on the 5-point start icon in Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql\n",
    "from pyarrow.compat import guid\n",
    "from coffea.processor.spark.detail import _spark_initialize, _spark_stop\n",
    "from coffea.processor.spark.spark_executor import spark_executor\n",
    "\"\"\"\n",
    "# NOT needed on SWAN, spark config is offloaded to spark connector\n",
    "\n",
    "spark_config = pyspark.sql.SparkSession.builder \\\n",
    "    .appName('spark-executor-test-%s' % guid()) \\\n",
    "    .master('local[*]') \\\n",
    "    .config('spark.driver.memory', '4g') \\\n",
    "    .config('spark.executor.memory', '4g') \\\n",
    "    .config('spark.sql.execution.arrow.enabled','true') \\\n",
    "    .config('spark.sql.execution.arrow.maxRecordsPerBatch', 200000)\n",
    "\n",
    "spark = _spark_initialize(config=spark, log_level='WARN', \n",
    "                          spark_progress=False, laurelin_version='0.5.1')\n",
    "\"\"\"\n",
    "partitionsize = 200000\n",
    "thread_workers = 2\n",
    "\n",
    "tstart = time.time() \n",
    "\n",
    "output = processor.run_spark_job(fileset_lite, DimuonProcessor(nano=False), spark_executor, \n",
    "                                 spark=spark, partitionsize=partitionsize, thread_workers=thread_workers,\n",
    "                                 executor_args={'file_type': 'edu.vanderbilt.accre.laurelin.Root', 'cache': False, 'nano': False}\n",
    "                                )\n",
    "\n",
    "elapsed = time.time() - tstart\n",
    "print(output)\n",
    "print(\"Total time: \", elapsed)\n",
    "print(\"Events/s:\", output['cutflow']['all events']/elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_variable(fig, var, gs, weights):\n",
    "    \n",
    "    new_output = output[var].copy() # copy to prevent scaling multiple times\n",
    "    new_output.scale(weights, axis='dataset')\n",
    "    \n",
    "    \n",
    "    # Group the samples for plotting (will be important when we unite low-yield samples like V)\n",
    "    \n",
    "    data = new_output['data']\n",
    "    \n",
    "    bkg_sources = {\n",
    "        'dy': ['dy'],\n",
    "        'ttbar':['ttjets_dl'],  \n",
    "    }\n",
    "    bkg = new_output.group('dataset', hist.Cat(\"dataset\", \"Dataset\"), bkg_sources)\n",
    "    bkg.axis('dataset').sorting = 'integral' # sort backgrounds by event yields\n",
    "    \n",
    "    data_opts = {'color': 'k', 'marker': '.', 'markersize':15}\n",
    "    stack_fill_opts = {'alpha': 0.8, 'edgecolor':(0,0,0)}\n",
    "    stack_error_opts = {'label':'Stat. unc.','facecolor':(0,0,0,.4), 'hatch':'', 'linewidth': 0}\n",
    "    \n",
    "    # Top panel: Data vs. MC plot\n",
    "    plt1 = fig.add_subplot(gs[0])\n",
    "    ax_bkg = hist.plot1d(bkg, ax=plt1, overlay='dataset', overflow='all', stack=True, fill_opts=stack_fill_opts, error_opts=stack_error_opts)\n",
    "    ax_data = hist.plot1d(data, overlay='dataset', overflow='all', line_opts=None, error_opts=data_opts)\n",
    "    plt1.set_yscale('log')\n",
    "    plt1.set_ylim(0.1, 1e7)\n",
    "    lbl = hep.cms.cmslabel(plt1, data=True, paper=False, year='2016')\n",
    "    plt1.set_xlabel('')\n",
    "    plt1.tick_params(axis='x', labelbottom=False)\n",
    "    \n",
    "\n",
    "    # Bottom panel: Data/MC ratio plot\n",
    "    # TODO: add MC uncertainty (shaded)\n",
    "    plt2 = fig.add_subplot(gs[1], sharex=plt1)\n",
    "    num = data.sum('dataset')\n",
    "    denom = bkg.sum('dataset')\n",
    "    hist.plotratio(num=num, ax=plt2,\n",
    "                    denom=denom,\n",
    "                    error_opts=data_opts, denom_fill_opts={}, guide_opts={},\n",
    "                    unc='num')\n",
    "    \n",
    "    \n",
    "    plt2.axhline(1, ls='--')\n",
    "    plt2.set_ylim([0,2])    \n",
    "    plt2.set_ylabel('Data/MC')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare things to plot\n",
    "from parameters import cross_sections\n",
    "\n",
    "mc_datasets = [s for s in samples if 'data' not in s]\n",
    "\n",
    "# Luminosity weight (maybe should be done in postprocess()?)\n",
    "lumi_weights = {'data':1}\n",
    "lumi = 15\n",
    "# TODO: add proper luminosity evaluation\n",
    "\n",
    "# print(output['sumGenWeights'].values())\n",
    "# print(output['entries'].values())\n",
    "\n",
    "for mc in mc_datasets:\n",
    "    N = output['entries'].values().get((mc,)).sum() # temporary - will replace with sum of weights\n",
    "    lumi_weights[mc] = cross_sections[mc]*lumi / N\n",
    "\n",
    "vars_to_plot = ['dimuon_mass', 'dimuon_pt','mu1_pt','mu2_pt', 'jet_pt', 'njets', 'npv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make plots iteratively and put into a grid\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "plt.style.use(hep.cms.style.ROOT)\n",
    "from matplotlib import gridspec\n",
    "import math\n",
    "    \n",
    "fig = plt.figure()\n",
    "    \n",
    "nplots_x = 4 # number of plots in one row\n",
    "nplots_y = math.ceil(len(vars_to_plot) / nplots_x) # number of rows\n",
    "\n",
    "plotsize=10\n",
    "ratio_plot_size = 0.24\n",
    "fig.set_size_inches(nplots_x*plotsize,nplots_y*plotsize*(1+ratio_plot_size))\n",
    "outer_grid = gridspec.GridSpec(nplots_y, nplots_x, hspace = .3) \n",
    "for i, var in enumerate(vars_to_plot):\n",
    "    gs = gridspec.GridSpecFromSubplotSpec(2, 1, subplot_spec = outer_grid[i], height_ratios=[(1-ratio_plot_size),ratio_plot_size], hspace = .05)\n",
    "    plot_variable(fig, var, gs, lumi_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make plots in parallel\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import mplhep as hep\n",
    "plt.style.use(hep.cms.style.ROOT)\n",
    "import multiprocessing as mp\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 50\n",
    "    \n",
    "figs = {}\n",
    "def plot_var(i):\n",
    "    fig = plt.figure()\n",
    "    plotsize=8\n",
    "    ratio_plot_size = 0.24\n",
    "    fig.set_size_inches(plotsize,plotsize*(1+ratio_plot_size))\n",
    "    gs = gridspec.GridSpec(2, 1, height_ratios=[(1-ratio_plot_size),ratio_plot_size], hspace = .05)\n",
    "    plot_variable(fig, vars_to_plot[i], gs, lumi_weights)\n",
    "    return fig\n",
    " \n",
    "\n",
    "pool = mp.Pool(mp.cpu_count() - 1)\n",
    "results = [pool.apply(plot_var, args=(x,)) for x in range(len(vars_to_plot))]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "sparkconnect": {
   "bundled_options": [
    "MemoryIntensive"
   ],
   "list_of_options": [
    {
     "name": "spark.kubernetes.container.image",
     "value": "gitlab-registry.cern.ch/db/spark-service/docker-registry/swan:laurelin"
    },
    {
     "name": "spark.sql.execution.arrow.enabled",
     "value": "true"
    },
    {
     "name": "spark.sql.execution.arrow.maxRecordsPerBatch",
     "value": "200000"
    },
    {
     "name": "spark.kubernetes.container.image.pullPolicy",
     "value": "Always"
    },
    {
     "name": "spark.driver.extraClassPath",
     "value": "./laurelin-0.5.1.jar:./lz4-java-1.5.1.jar:./log4j-core-2.11.2.jar:./log4j-api-2.11.2.jar:./xz-1.2.jar"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
