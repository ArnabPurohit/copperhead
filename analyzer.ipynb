{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <img  src=\"https://swan.web.cern.ch/sites/swan.web.cern.ch/files/pictures/logo_swan_letters.png\" alt=\"SWAN\" style=\"float: left; width: 15%; margin-right: 5%; margin-left: 17%; margin-top: 1.0em; margin-bottom: 2.0em;\">\n",
    "<img src=\"https://spark.apache.org/images/spark-logo-trademark.png\" alt=\"EP-SFT\" style=\"float: left; width: 25%; margin-right: 0%; margin-left: 0%; margin-bottom: 2.0em;\">\n",
    "<img src=\"https://cms-docdb.cern.ch/cgi-bin/PublicDocDB/RetrieveFile?docid=3045&filename=CMSlogo_color_label_1024_May2014.png&version=3\" alt=\"CMS\" style=\"float: left; width: 12%; margin-left: 5%; margin-right: 5%; margin-bottom: 2.0em;\"> -->\n",
    "<p style=\"clear: both;\">\n",
    "<div style=\"text-align:center\"><h1>CMS H&#8594;µµ analysis  \n",
    "     <br> with Coffea package from Fermilab</h1></div>\n",
    "<div style=\"text-align:center\"><i>Author: Dmitry Kondratyev, based on example code by Lindsey Gray</i></div>\n",
    "<hr style=\"border-top-width: 4px; border-top-color: #34609b;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for Higgs boson decaying into two muons\n",
    "\n",
    "This code uses awkward array toolset, and utilizing Coffea [histograms](https://coffeateam.github.io/coffea/modules/coffea.hist.html).\n",
    "This also shows the analysis object syntax implemented by Coffea [JaggedCandidateArray](https://coffeateam.github.io/coffea/api/coffea.analysis_objects.JaggedCandidateMethods.html), and the usage of custom [accumulators](https://coffeateam.github.io/coffea/api/coffea.processor.AccumulatorABC.html) other than histograms.  Further, it introduces the [processor](https://coffeateam.github.io/coffea/api/coffea.processor.ProcessorABC.html) concept and the interface to apache spark.\n",
    "\n",
    "#### SWAN env: LCG96 Python3 stack and Cloud Containers cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you do not have coffea installed (e.g. on SWAN with LCG 96Python3 stack)\n",
    "!pip install --user --upgrade coffea\n",
    "\n",
    "# spark.jars.packages doesnt work with Spark 2.4 with kubernetes\n",
    "!wget -N https://repo1.maven.org/maven2/edu/vanderbilt/accre/laurelin/0.5.1/laurelin-0.5.1.jar && \\\n",
    "wget -N https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-api/2.11.2/log4j-api-2.11.2.jar && \\\n",
    "wget -N https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-core/2.11.2/log4j-core-2.11.2.jar && \\\n",
    "wget -N https://repo1.maven.org/maven2/org/lz4/lz4-java/1.5.1/lz4-java-1.5.1.jar && \\\n",
    "wget -N https://repo1.maven.org/maven2/org/tukaani/xz/1.2/xz-1.2.jar\n",
    "                    \n",
    "!mkdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell before establishing spark connection\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONPATH'] = os.environ['PYTHONPATH'] + ':' + '/usr/local/lib/python3.6/site-packages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import coffea\n",
    "print(\"Coffea version: \", coffea.__version__)\n",
    "\n",
    "from coffea import hist, util\n",
    "from coffea.analysis_objects import JaggedCandidateArray\n",
    "import coffea.processor as processor\n",
    "from coffea.lookup_tools import extractor, dense_lookup, txt_converters, rochester_lookup\n",
    "from coffea.lumi_tools import LumiMask\n",
    "\n",
    "import awkward\n",
    "import uproot\n",
    "import numpy as np\n",
    "import numba\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "from datasets import datasets, lumi_data\n",
    "\n",
    "\n",
    "year = \"2016\"\n",
    "paths = datasets[year]\n",
    "# TODO: add possibility to load of files through XrootD\n",
    "\n",
    "debug = True\n",
    "\n",
    "if debug:\n",
    "    samples = [\n",
    "        'data_G', \n",
    "        'dy', \n",
    "        'dy_m105_160_amc',\n",
    "        'ttjets_dl',      \n",
    "        'ewk_lljj_mll50_mjj120',\n",
    "        'ewk_lljj_mll105_160',\n",
    "        'ggh_amcPS',\n",
    "        'vbf_amcPS',\n",
    "        \n",
    "#         'st_tw_top',\n",
    "#         'st_tw_antitop',\n",
    "#         'ww_2l2nu',\n",
    "#         'wz_3lnu',\n",
    "#         'www',\n",
    "#         'wwz',\n",
    "#         'wzz',\n",
    "#         'zzz',\n",
    "        \n",
    "    ]\n",
    "else:\n",
    "    samples = paths.keys()\n",
    "\n",
    "fileset = {}\n",
    "metadata = {}\n",
    "\n",
    "data_samples = [s for s in samples if 'data' in s]\n",
    "signal_samples = ['ggh_amcPS', 'vbf_amcPS']\n",
    "main_bkg_samples = ['dy_m105_160_amc', 'ttjets_dl', 'ewk_lljj_mll105_160']\n",
    "datasets_to_save_unbin = data_samples + signal_samples + main_bkg_samples\n",
    "\n",
    "lumi = 1\n",
    "data_entries = 0\n",
    "\n",
    "# Find ROOT files in local directories\n",
    "for sample in samples:\n",
    "    metadata[sample] = {}\n",
    "    all_files = []\n",
    "    all_files = glob.glob(paths[sample]+'*root')\n",
    "    \n",
    "    if debug:\n",
    "        all_files = [all_files[0]]\n",
    "#         if 'data' in sample:\n",
    "#             all_files = all_files[0:10]\n",
    "#         else:\n",
    "#             all_files = [all_files[0]]\n",
    "     \n",
    "#     server = 'root://eoscms.cern.ch/'\n",
    "    server = ''\n",
    "            \n",
    "    if 'data' in sample:\n",
    "        for f in all_files:\n",
    "            tree = uproot.open(server+f)['Events']\n",
    "            data_entries += tree.numentries\n",
    "    else:\n",
    "        sumGenWgts = 0\n",
    "        nGenEvts = 0\n",
    "        for f in all_files:\n",
    "            tree = uproot.open(server+f)['Runs']\n",
    "            if 'NanoAODv6' in paths[sample]:\n",
    "                sumGenWgts += tree.array('genEventSumw_')[0]\n",
    "                nGenEvts += tree.array('genEventCount_')[0]\n",
    "            else:\n",
    "                sumGenWgts += tree.array('genEventSumw')[0]\n",
    "                nGenEvts += tree.array('genEventCount')[0]\n",
    "        metadata[sample]['sumGenWgts'] = sumGenWgts\n",
    "        metadata[sample]['nGenEvts'] = nGenEvts\n",
    "        \n",
    "    fileset[sample] = {\n",
    "        'files': [server+f for f in all_files],\n",
    "        'treename': 'Events'\n",
    "    }\n",
    "\n",
    "print(f\"Loading {data_entries} of {year} data events\")\n",
    "lumi = lumi_data[year]['lumi']*data_entries/lumi_data[year]['events']\n",
    "print(f\"This is ~ {data_entries/lumi_data[year]['events']*100}% of {year} data.\")\n",
    "print(f\"Integrated luminosity {lumi}/pb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "puLookup = util.load('data/pileup/puLookup.coffea')\n",
    "muSFFileList = [{'id'   : (\"data/muon_sf/EfficienciesStudies_2016_legacy_rereco_rootfiles_RunBCDEF_SF_ID.root\", \"NUM_TightID_DEN_genTracks_eta_pt\"),\n",
    "                 'iso'   : (\"data/muon_sf/EfficienciesStudies_2016_legacy_rereco_rootfiles_RunBCDEF_SF_ISO.root\", \"NUM_TightRelIso_DEN_TightIDandIPCut_eta_pt\"),\n",
    "                 'trig'  : (\"data/muon_sf/EfficienciesStudies_2016_trigger_EfficienciesAndSF_RunBtoF.root\", \"IsoMu24_OR_IsoTkMu24_PtEtaBins/abseta_pt_ratio\"),\n",
    "                 'scale' : 19.656062760/35.882515396},\n",
    "                {'id'     : (\"data/muon_sf/EfficienciesStudies_2016_legacy_rereco_rootfiles_RunGH_SF_ID.root\", \"NUM_TightID_DEN_genTracks_eta_pt\"),\n",
    "                 'iso'   : (\"data/muon_sf/EfficienciesStudies_2016_legacy_rereco_rootfiles_RunGH_SF_ISO.root\", \"NUM_TightRelIso_DEN_TightIDandIPCut_eta_pt\"),\n",
    "                 'trig'  : (\"data/muon_sf/EfficienciesStudies_2016_trigger_EfficienciesAndSF_RunGtoH.root\", \"IsoMu24_OR_IsoTkMu24_PtEtaBins/abseta_pt_ratio\"),\n",
    "                 'scale' : 16.226452636/35.882515396}]\n",
    "# TODO: check scale\n",
    "\n",
    "zpt_weights_file = \"data/zpt/zpt_weights.histo.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def apply_roccor(rochester, isData, muons):\n",
    "    muons = muons.compact()\n",
    "    corrections = muons.pt.ones_like()  \n",
    "    if isData:\n",
    "        corrections = rochester.kScaleDT(muons.charge, muons.pt, muons.eta, muons.phi)      \n",
    "    else:\n",
    "        mc_rand = np.random.rand(*muons.pt.flatten().shape)\n",
    "        mc_rand = awkward.JaggedArray.fromoffsets(muons.pt.offsets, mc_rand)\n",
    "        hasgen = ~np.isnan(muons.matched_gen.pt.fillna(np.nan))\n",
    "        mc_rand = awkward.JaggedArray.fromoffsets(hasgen.offsets, mc_rand)._content\n",
    "\n",
    "        mc_kspread = rochester.kSpreadMC(muons.charge[hasgen], muons.pt[hasgen], muons.eta[hasgen], muons.phi[hasgen],\n",
    "                                         muons.matched_gen.pt[hasgen])\n",
    "        mc_ksmear = rochester.kSmearMC(muons.charge[~hasgen], muons.pt[~hasgen],muons.eta[~hasgen],muons.phi[~hasgen],\n",
    "                                       muons.nTrackerLayers[~hasgen], mc_rand[~hasgen])\n",
    "        corrections = np.ones_like(muons.pt.flatten())\n",
    "        corrections[hasgen.flatten()] = mc_kspread.flatten()\n",
    "        corrections[~hasgen.flatten()] = mc_ksmear.flatten() \n",
    "    return corrections\n",
    "\n",
    "def p4_sum(obj1, obj2):\n",
    "    assert(obj1.shape==obj2.shape)\n",
    "    px = np.zeros(obj1.shape[0])\n",
    "    py = np.zeros(obj1.shape[0])\n",
    "    pz = np.zeros(obj1.shape[0])\n",
    "    e = np.zeros(obj1.shape[0])\n",
    "    \n",
    "    for obj in [obj1, obj2]:\n",
    "        px_ = obj.pt*np.cos(obj.phi)\n",
    "        py_ = obj.pt*np.sin(obj.phi)\n",
    "        pz_ = obj.pt*np.sinh(obj.eta)\n",
    "        e_  = np.sqrt(px_**2 + py_**2 + pz_**2 + obj.mass**2)\n",
    "        px = px + px_\n",
    "        py = py + py_\n",
    "        pz = pz + pz_\n",
    "        e = e + e_\n",
    "        \n",
    "    pt = np.sqrt(px**2 + py**2)\n",
    "    eta = np.arcsinh(pz / pt)\n",
    "    phi = np.arctan2(py, px)\n",
    "    mass = np.sqrt(e**2 - px**2 - py**2 - pz**2)    \n",
    "    return pt, eta, phi, mass\n",
    "\n",
    "\n",
    "        \n",
    "def get_regions(mass):\n",
    "    regions = {\n",
    "        \"z-peak\": ((mass>70) & (mass<110)),\n",
    "        \"h-sidebands\": ((mass>110) & (mass<115)) | ((mass>135) & (mass<150)),\n",
    "        \"h-peak\": ((mass>115) & (mass<135)),\n",
    "    }\n",
    "    return regions\n",
    "\n",
    "\n",
    "class Timer(object):\n",
    "    def __init__(self, name=\"t\"):\n",
    "        self.name = name\n",
    "        self.time_dict = {}\n",
    "        self.last_checkpoint = time.time()    \n",
    "        \n",
    "    def update(self):\n",
    "        self.last_checkpoint = time.time()\n",
    "        \n",
    "    def add_checkpoint(self, comment):\n",
    "        now = time.time()\n",
    "        dt = now - self.last_checkpoint\n",
    "        if comment in self.time_dict.keys():\n",
    "            self.time_dict[comment] += dt\n",
    "        else:\n",
    "            self.time_dict[comment] = dt\n",
    "        self.last_checkpoint = now\n",
    "        \n",
    "    def summary(self):\n",
    "        columns = [\"Action\", \"Time (s)\", \"% CPU\"]\n",
    "        summary = pd.DataFrame(columns=columns)        \n",
    "        total_time = sum(list(self.time_dict.values()))\n",
    "        summary[columns[0]] = np.array(list(self.time_dict.keys()))\n",
    "        summary[columns[1]] = np.round( np.array(list(self.time_dict.values())), 5)\n",
    "        summary[columns[2]] = np.round( 100*np.array(list(self.time_dict.values()))/total_time, 3)\n",
    "\n",
    "        print('-'*50)\n",
    "        print(f'Summary of {self.name} timer:')\n",
    "        print('-'*50)\n",
    "        print(summary)\n",
    "        print('-'*50)\n",
    "        print(f'Total time: {total_time}')\n",
    "        print('='*50)\n",
    "        print()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at ProcessorABC documentation to see the expected methods and what they are supposed to do\n",
    "# https://coffeateam.github.io/coffea/api/coffea.processor.ProcessorABC.html\n",
    "class DimuonProcessor(processor.ProcessorABC):\n",
    "    def __init__(self, mass_window=[70,150], do_roccor=True, evaluate_dnn=True, do_timer=False): \n",
    "        from configuration import parameters\n",
    "        from variables import variables\n",
    "        year = '2016'\n",
    "        self.mass_window = mass_window\n",
    "        self.do_roccor = do_roccor\n",
    "        self.evaluate_dnn = evaluate_dnn\n",
    "        self.parameters = {k:v[year] for k,v in parameters.items()}\n",
    "        self.timer = Timer('global') if do_timer else None\n",
    "        \n",
    "        self._columns = self.parameters[\"proc_columns\"]\n",
    "\n",
    "        dataset_axis = hist.Cat(\"dataset\", \"\")\n",
    "        region_axis = hist.Cat(\"region\", \" \") # Z-peak, Higgs SB, Higgs peak\n",
    "        channel_axis = hist.Cat(\"channel\", \" \") # ggh or VBF       \n",
    "        accumulators = {}\n",
    "        \n",
    "        ### Prepare accumulators for binned output ###\n",
    "        \n",
    "        for v in variables:\n",
    "            if 'dimuon_mass' in v.name:\n",
    "                axis = hist.Bin(v.name, v.caption, v.nbins, self.mass_window[0], self.mass_window[1])\n",
    "            else:\n",
    "                axis = hist.Bin(v.name, v.caption, v.nbins, v.xmin, v.xmax)\n",
    "            accumulators[v.name] = hist.Hist(\"Counts\", dataset_axis, region_axis, channel_axis, axis)\n",
    "            \n",
    "        accumulators['cutflow'] = processor.defaultdict_accumulator(int)\n",
    "\n",
    "        ### Prepare accumulators for unbinned output ###\n",
    "        \n",
    "        self.vars_unbin = ['event', 'event_weight', 'dimuon_mass', 'dimuon_pt', 'dimuon_eta',\\\n",
    "                           'dimuon_dEta', 'mu1_pt', 'mu2_pt']\n",
    "        self.regions = ['z-peak', 'h-sidebands', 'h-peak']\n",
    "        self.channels = ['ggh_01j', 'ggh_2j', 'vbf']\n",
    "        \n",
    "        self.dont_fill = {\n",
    "            'z-peak': ['dy_m105_160_amc', 'ewk_lljj_mll105_160'],\n",
    "            'h-sidebands': ['dy', 'ewk_lljj_mll50_mjj120'],\n",
    "            'h-peak': ['dy', 'ewk_lljj_mll50_mjj120'],\n",
    "        }\n",
    "        \n",
    "        for p in datasets_to_save_unbin:\n",
    "            for v in self.vars_unbin:\n",
    "                for c in self.channels:\n",
    "                    for r in self.regions:\n",
    "                        if 'z-peak' in r: continue # don't need unbinned data for Z-peak\n",
    "                        accumulators[f'{v}_unbin_{p}_c_{c}_r_{r}'] = processor.column_accumulator(np.ndarray([]))\n",
    "                        # have to encode everything into the name because having multiple axes isn't possible\n",
    "        \n",
    "        self._accumulator = processor.dict_accumulator(accumulators)\n",
    "    \n",
    "        ### --------------------------------------- ###\n",
    "        \n",
    "        mu_id_vals = 0\n",
    "        mu_id_err = 0\n",
    "        mu_iso_vals = 0\n",
    "        mu_iso_err = 0\n",
    "        mu_trig_vals = 0\n",
    "        mu_trig_err = 0\n",
    "\n",
    "        for scaleFactors in muSFFileList:\n",
    "            id_file = uproot.open(scaleFactors['id'][0])\n",
    "            iso_file = uproot.open(scaleFactors['iso'][0])\n",
    "            trig_file = uproot.open(scaleFactors['trig'][0])\n",
    "            \n",
    "            mu_id_vals += id_file[scaleFactors['id'][1]].values * scaleFactors['scale']\n",
    "            mu_id_err += id_file[scaleFactors['id'][1]].variances**0.5 * scaleFactors['scale']\n",
    "            mu_id_edges = id_file[scaleFactors['id'][1]].edges\n",
    "\n",
    "            mu_iso_vals += iso_file[scaleFactors['iso'][1]].values * scaleFactors['scale']\n",
    "            mu_iso_err += iso_file[scaleFactors['iso'][1]].variances**0.5 * scaleFactors['scale']\n",
    "            mu_iso_edges = iso_file[scaleFactors['iso'][1]].edges\n",
    "\n",
    "            mu_trig_vals += trig_file[scaleFactors['trig'][1]].values * scaleFactors['scale']\n",
    "            mu_trig_err += trig_file[scaleFactors['trig'][1]].variances**0.5 * scaleFactors['scale']\n",
    "            mu_trig_edges = trig_file[scaleFactors['trig'][1]].edges\n",
    "\n",
    "        self.mu_id_sf = dense_lookup.dense_lookup(mu_id_vals, mu_id_edges)\n",
    "        self.mu_id_err = dense_lookup.dense_lookup(mu_id_err, mu_id_edges)\n",
    "        self.mu_iso_sf = dense_lookup.dense_lookup(mu_iso_vals, mu_iso_edges)\n",
    "        self.mu_iso_err = dense_lookup.dense_lookup(mu_iso_err, mu_iso_edges)\n",
    "        self.mu_trig_sf = dense_lookup.dense_lookup(mu_trig_vals, mu_trig_edges)\n",
    "        self.mu_trig_err = dense_lookup.dense_lookup(mu_trig_err, mu_trig_edges)    \n",
    "\n",
    "        self.extractor = extractor()\n",
    "        self.extractor.add_weight_sets([f\"* * {zpt_weights_file}\"])\n",
    "        self.extractor.finalize()\n",
    "        self.evaluator = self.extractor.make_evaluator()\n",
    "        if '2016' in year:\n",
    "            self.zpt_path = 'zpt_weights/2016_value'\n",
    "        else:\n",
    "            self.zpt_path = 'zpt_weights/2017_value'\n",
    "        self.evaluator[self.zpt_path]._axes = self.evaluator[self.zpt_path]._axes[0]            \n",
    "#         Have to do the last line because of a bug in lookup_tools\n",
    "#         For 1-dimensional histograms, _axes is a tuple, and np.searchsorted doesn't understand it\n",
    "#         https://github.com/CoffeaTeam/coffea/blob/2650ad7657094f6e50ebf962a1fc1763cd2c6601/coffea/lookup_tools/dense_lookup.py#L37\n",
    "#         TODO: tell developers?\n",
    "\n",
    "        rochester_data = txt_converters.convert_rochester_file(self.parameters[\"roccor_file\"], loaduncs=True)\n",
    "        self.rochester = rochester_lookup.rochester_lookup(rochester_data)\n",
    "    \n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "    \n",
    "    @property\n",
    "    def columns(self):\n",
    "        return self._columns\n",
    "    \n",
    "    def process(self, df):\n",
    "        # TODO: Properly calculate integrated luminosity\n",
    "        # TODO: Add FSR recovery\n",
    "        # TODO: verify PU weigths (ask at https://github.com/dnoonan08/TTGamma_LongExercise)\n",
    "        # TODO: generate lepton SF for 2017/2018\n",
    "        # TODO: NNLOPS reweighting (ggH)\n",
    "        # TODO: L1 prefiring weights\n",
    "        # TODO: btag sf\n",
    "        # TODO: jet PU ID sf (copy-paste my code from hepaccelerate)\n",
    "        # TODO: compute dimuon_costhetaCS, dimuon_phiCS\n",
    "        # TODO: compute nsoftjets\n",
    "        # TODO: JEC, JER\n",
    "        # TODO: event-by-event mass resolution and calibration\n",
    "        # TODO: kinematic variables of multimuon-multijet system\n",
    "        # TODO: filter by event number to make sure DNN is evaluated only on test events\n",
    "        # TODO: Add systematic uncertainties\n",
    "        \n",
    "        if self.timer:\n",
    "            self.timer.update()\n",
    "            \n",
    "        output = self.accumulator.identity()\n",
    "        dataset = df.metadata['dataset']\n",
    "        isData = 'data' in dataset\n",
    "            \n",
    "        nEvts = df.shape[0]\n",
    "\n",
    "        if isData:\n",
    "            lumi_info = LumiMask(self.parameters['lumimask'])\n",
    "            lumimask = lumi_info(df.run.flatten(), df.luminosityBlock.flatten())\n",
    "            event_weight = np.ones(nEvts)\n",
    "        else:    \n",
    "            lumimask = np.ones(nEvts, dtype=bool)\n",
    "            genweight = df.genWeight.flatten()\n",
    "            pu_weight = puLookup(dataset, df.Pileup.nTrueInt)\n",
    "            event_weight = genweight*pu_weight        \n",
    "        \n",
    "        hlt = np.zeros(nEvts, dtype=bool)\n",
    "        for hlt_path in self.parameters['hlt']:\n",
    "            hlt = hlt | df.HLT[hlt_path]\n",
    "\n",
    "        mask = hlt & lumimask\n",
    "    \n",
    "        # Filter 0: HLT & lumimask\n",
    "        #--------------------------------#    \n",
    "        df = df[mask]\n",
    "        event_weight = event_weight[mask]\n",
    "        if self.timer:\n",
    "            self.timer.add_checkpoint(\"Applied HLT and lumimask\")\n",
    "        #--------------------------------# \n",
    "        \n",
    "        pass_event_flags = np.ones(df.shape[0], dtype=bool)\n",
    "        for flag in self.parameters[\"event_flags\"]:\n",
    "            pass_event_flags = pass_event_flags & df.Flag[flag]\n",
    "        \n",
    "        pass_muon_flags = np.ones(df.shape[0], dtype=bool)\n",
    "        for flag in self.parameters[\"muon_flags\"]:\n",
    "            pass_muon_flags = pass_muon_flags & df.Muon[flag]\n",
    "        \n",
    "        muons = df.Muon[(df.Muon.pt > self.parameters[\"muon_pt_cut\"]) &\\\n",
    "                        (abs(df.Muon.eta) < self.parameters[\"muon_eta_cut\"]) &\\\n",
    "                        (df.Muon.pfRelIso04_all < self.parameters[\"muon_iso_cut\"]) &\\\n",
    "                        df.Muon[self.parameters[\"muon_id\"]] & pass_muon_flags]\n",
    "              \n",
    "        two_os_muons = ((muons.counts == 2) & (muons['charge'].prod() == -1))\n",
    "        \n",
    "        electrons = df.Electron[(df.Electron.pt > self.parameters[\"electron_pt_cut\"]) &\\\n",
    "                                     (abs(df.Electron.eta) < self.parameters[\"electron_eta_cut\"]) &\\\n",
    "                                     (df.Electron[self.parameters[\"electron_id\"]] == 1)]\n",
    "        \n",
    "        electron_veto = (electrons.counts==0)\n",
    "        good_pv = (df.PV.npvsGood > 0)\n",
    "            \n",
    "        event_filter = (pass_event_flags & two_os_muons & electron_veto & good_pv).flatten()\n",
    "        \n",
    "        \n",
    "        # Filter 1: Event selection\n",
    "        #--------------------------------#    \n",
    "        df = df[event_filter]\n",
    "        muons = muons[event_filter]\n",
    "        event_weight = event_weight[event_filter]\n",
    "        if self.timer:\n",
    "            self.timer.add_checkpoint(\"Applied preselection\")\n",
    "        #--------------------------------# \n",
    "\n",
    "        mu1 = muons[muons.pt.argmax()]\n",
    "        mu2 = muons[muons.pt.argmin()]\n",
    "        if self.do_roccor:\n",
    "            mu1 = JaggedCandidateArray.candidatesfromcounts(\n",
    "                np.ones(mu1.pt.shape),\n",
    "                pt=mu1.pt.content*apply_roccor(self.rochester, isData, mu1).flatten(),\n",
    "                eta=mu1.eta.content,\n",
    "                phi=mu1.phi.content,\n",
    "                mass=mu1.mass.content,\n",
    "                pfRelIso04_all=mu1.pfRelIso04_all.content\n",
    "            )\n",
    "            mu2 = JaggedCandidateArray.candidatesfromcounts(\n",
    "                np.ones(mu2.pt.shape),\n",
    "                pt=mu2.pt.content*apply_roccor(self.rochester, isData, mu2).flatten(),\n",
    "                eta=mu2.eta.content,\n",
    "                phi=mu2.phi.content,\n",
    "                mass=mu2.mass.content,\n",
    "                pfRelIso04_all=mu2.pfRelIso04_all.content\n",
    "            )\n",
    "            if self.timer:\n",
    "                self.timer.add_checkpoint(\"Applied Rochester\")\n",
    "\n",
    "        # correct dimuon kinematics..\n",
    "        dimuon_pt, dimuon_eta, dimuon_phi, dimuon_mass = p4_sum(mu1, mu2)\n",
    "        \n",
    "        # gives wrong dimuon mass!\n",
    "#         dimuons = JaggedCandidateArray.candidatesfromcounts(\n",
    "#             np.ones(dimuon_pt.shape),\n",
    "#             pt=dimuon_pt.content,\n",
    "#             eta=dimuon_eta.content,\n",
    "#             phi=dimuon_phi.content,\n",
    "#             mass=dimuon_mass.content,\n",
    "#         )\n",
    "\n",
    "        if 'dy' in dataset:\n",
    "            zpt_weights = self.evaluator[self.zpt_path](dimuon_pt).flatten()\n",
    "            event_weight = event_weight*zpt_weights\n",
    "\n",
    "        mu_pass_leading_pt = muons[(muons.pt > self.parameters[\"muon_leading_pt\"]) &\\\n",
    "                                   (muons.pfRelIso04_all < self.parameters[\"muon_trigmatch_iso\"]) &\\\n",
    "                                   muons[self.parameters[\"muon_trigmatch_id\"]]]\n",
    "        trig_muons = df.TrigObj[df.TrigObj.id == 13]\n",
    "        muTrig = mu_pass_leading_pt.cross(trig_muons, nested = True)\n",
    "        matched = (muTrig.i0.delta_r(muTrig.i1) < self.parameters[\"muon_trigmatch_dr\"])\n",
    "\n",
    "        # at least one muon matched with L3 object, and that muon passes pt, iso and id cuts\n",
    "        trig_matched = (mu_pass_leading_pt[matched.any()].counts>0)\n",
    "\n",
    "        \n",
    "        dimuon_filter = ((mu1.pt>self.parameters[\"muon_leading_pt\"]) & trig_matched &\\\n",
    "                         (dimuon_mass > self.mass_window[0]) & (dimuon_mass < self.mass_window[1])).flatten()\n",
    "\n",
    "        if not isData:\n",
    "            muID = self.mu_id_sf(muons.eta.compact(), muons.pt.compact())\n",
    "            muIso = self.mu_iso_sf(muons.eta.compact(), muons.pt.compact())\n",
    "            muTrig = self.mu_iso_sf(abs(muons.eta.compact()), muons.pt.compact())\n",
    "            muSF = (muID*muIso*muTrig).prod()\n",
    "            event_weight = event_weight*muSF\n",
    "#             muIDerr = self.mu_id_err(muons.eta, muons.pt)\n",
    "#             muIsoerr = self.mu_iso_err(muons.eta, muons.pt)\n",
    "#             muTrigerr = self.mu_iso_err(abs(muons.eta), muons.pt)\n",
    "#             muSF_up = ((muID + muIDerr) * (muIso + muIsoerr) * (muTrig + muTrigerr)).prod()\n",
    "#             muSF_down = ((muID - muIDerr) * (muIso - muIsoerr) * (muTrig - muTrigerr)).prod() \n",
    "\n",
    "    \n",
    "        # Filter 2: Dimuon pair selection\n",
    "        #--------------------------------#\n",
    "        df = df[dimuon_filter]   \n",
    "        mu1 = mu1[dimuon_filter] \n",
    "        mu2 = mu2[dimuon_filter] \n",
    "        muons = muons[dimuon_filter]\n",
    "        dimuon_pt = dimuon_pt[dimuon_filter]\n",
    "        dimuon_eta = dimuon_eta[dimuon_filter]\n",
    "        dimuon_phi = dimuon_phi[dimuon_filter]\n",
    "        dimuon_mass = dimuon_mass[dimuon_filter]\n",
    "        event_weight = event_weight[dimuon_filter]\n",
    "        if self.timer:\n",
    "            self.timer.add_checkpoint(\"Applied dimuon cuts\")\n",
    "        #--------------------------------#   \n",
    "\n",
    "                            \n",
    "        mujet = df.Jet.cross(muons, nested=True)\n",
    "        deltar_mujet = mujet.i0.delta_r(mujet.i1)\n",
    "        deltar_mujet_ok =  (deltar_mujet > self.parameters[\"min_dr_mu_jet\"]).all()\n",
    "        \n",
    "        # 2016: loose jetId, loose piId \n",
    "        if \"loose\" in self.parameters[\"jet_id\"]:\n",
    "            jet_id = (df.Jet.jetId >= 1)\n",
    "        elif \"tight\" in self.parameters[\"jet_id\"]:\n",
    "            jet_id = (df.Jet.jetId >= 1)\n",
    "        else:\n",
    "            jet_id = df.Jet.ones_like()\n",
    "            \n",
    "        if \"loose\" in self.parameters[\"jet_puid\"]:\n",
    "            jet_puid = (((df.Jet.puId >= 4) & (df.Jet.pt < 50)) | (df.Jet.pt > 50))\n",
    "        else:\n",
    "            jet_puid = df.Jet.ones_like()\n",
    "            \n",
    "        jet_selection = ((df.Jet.pt > self.parameters[\"jet_pt_cut\"]) &\\\n",
    "                         (abs(df.Jet.eta) < self.parameters[\"jet_eta_cut\"]) &\\\n",
    "                         jet_id & jet_puid & (df.Jet.qgl > -2) & deltar_mujet_ok )\n",
    "        \n",
    "        # TODO: check jet selections\n",
    "        \n",
    "        jets = df.Jet[jet_selection]\n",
    "        # check if there should be this eta<2.5 cut \n",
    "        nBtagLoose = jets[(jets.btagDeepB>self.parameters[\"btag_loose_wp\"]) & (abs(jets.eta)<2.5)].counts\n",
    "        nBtagMedium = jets[(jets.btagDeepB>self.parameters[\"btag_medium_wp\"])  & (abs(jets.eta)<2.5)].counts\n",
    "        jet_filter = ((nBtagLoose<2)&(nBtagMedium<1))\n",
    "\n",
    "        # Filter 3: Jet filter\n",
    "        #--------------------------------#\n",
    "        df = df[jet_filter]   \n",
    "        mu1 = mu1[jet_filter] \n",
    "        mu2 = mu2[jet_filter] \n",
    "        dimuon_pt = dimuon_pt[jet_filter]\n",
    "        dimuon_eta = dimuon_eta[jet_filter]\n",
    "        dimuon_phi = dimuon_phi[jet_filter]\n",
    "        dimuon_mass = dimuon_mass[jet_filter]\n",
    "        jets = jets[jet_filter]\n",
    "        jet_selection = jet_selection[jet_filter]\n",
    "        event_weight = event_weight[jet_filter]\n",
    "        if self.timer:\n",
    "            self.timer.add_checkpoint(\"Applied jet cuts\")\n",
    "\n",
    "        #--------------------------------#\n",
    "\n",
    "        \n",
    "        # In the computations below I'm trying to keep the size of objects in first dimension the same\n",
    "        # as it is in the previous step, in order to be able to apply event_weight similarly for all variables\n",
    "        \n",
    "        one_jet = (jet_selection.any() & (jets.counts>0))\n",
    "        two_jets = (jet_selection.any() & (jets.counts>1))\n",
    "        \n",
    "        category = np.empty(df.shape[0], dtype=object)\n",
    "        category[~two_jets] = 'ggh_01j'\n",
    "        \n",
    "        jet1_mask = np.zeros(len(event_weight))\n",
    "        jet1_mask[one_jet] = 1\n",
    "        jet1 = JaggedCandidateArray.candidatesfromcounts(\n",
    "            jet1_mask,\n",
    "            pt=jets[one_jet,0].pt.flatten(),\n",
    "            eta=jets[one_jet,0].eta.flatten(),\n",
    "            phi=jets[one_jet,0].phi.flatten(),\n",
    "            mass=jets[one_jet,0].mass.flatten(),\n",
    "            qgl=jets[one_jet,0].qgl.flatten()\n",
    "        )\n",
    "        \n",
    "        jet2_mask = np.zeros(len(event_weight))\n",
    "        jet2_mask[two_jets] = 1\n",
    "        jet2 = JaggedCandidateArray.candidatesfromcounts(\n",
    "            jet2_mask,\n",
    "            pt=jets[two_jets,1].pt.flatten(),\n",
    "            eta=jets[two_jets,1].eta.flatten(),\n",
    "            phi=jets[two_jets,1].phi.flatten(),\n",
    "            mass=jets[two_jets,1].mass.flatten(),\n",
    "            qgl=jets[two_jets,1].qgl.flatten()\n",
    "        )\n",
    "        \n",
    "        dijet_pairs = jets[two_jets, 0:2]\n",
    "        \n",
    "        dijet_mask = np.zeros(len(event_weight))\n",
    "        dijet_mask[two_jets] = 2\n",
    "        dijet_jca = JaggedCandidateArray.candidatesfromcounts(\n",
    "            dijet_mask,\n",
    "            pt=dijet_pairs.pt.content,\n",
    "            eta=dijet_pairs.eta.content,\n",
    "            phi=dijet_pairs.phi.content,\n",
    "            mass=dijet_pairs.mass.content,\n",
    "        )\n",
    "        \n",
    "        dijet = dijet_jca.distincts()\n",
    "        dijet = dijet.p4.sum()\n",
    "\n",
    "        dijet_deta = np.full(len(event_weight), -999.)\n",
    "        dijet_deta[two_jets] = abs(jet1[two_jets].eta - jet2[two_jets].eta)\n",
    "        \n",
    "        dijet_dphi = np.full(len(event_weight), -999.)\n",
    "        dijet_dphi[two_jets] = abs(jet1[two_jets].p4.delta_phi(jet2[two_jets].p4))\n",
    "\n",
    "        vbf_cut = (dijet.mass>400)&(dijet_deta>2.5)\n",
    "        category[two_jets&(~vbf_cut)] = 'ggh_2j'\n",
    "        category[two_jets&vbf_cut] = 'vbf'\n",
    "        \n",
    "        if self.timer:\n",
    "            self.timer.add_checkpoint(\"Computed jet variables\")\n",
    "        \n",
    "        assert(np.count_nonzero(category)==category.shape[0]), \"Not all events have been categorized!\"\n",
    "        channels = list(set(category))\n",
    "        \n",
    "#----------------------------------------------------------------------------#\n",
    "        # flatten variables where exactly one value per event expected\n",
    "        variable_map = {\n",
    "            'dimuon_mass': dimuon_mass.flatten(),\n",
    "            'dimuon_pt': dimuon_pt.flatten(),\n",
    "            'dimuon_eta': dimuon_eta.flatten(),\n",
    "            'dimuon_phi': dimuon_phi.flatten(),\n",
    "            'dimuon_dEta': abs(mu1.eta - mu2.eta).flatten(),\n",
    "            'dimuon_dPhi': abs(mu1.p4.delta_phi(mu2.p4)).flatten(),\n",
    "            \n",
    "            'mu1_pt': mu1.pt.flatten(),\n",
    "            'mu1_eta': mu1.eta.flatten(),\n",
    "            'mu1_phi': mu1.phi.flatten(),\n",
    "            'mu1_iso': mu1.pfRelIso04_all.flatten(),\n",
    "\n",
    "            'mu2_pt': mu2.pt.flatten().flatten(),\n",
    "            'mu2_eta': mu2.eta.flatten().flatten(),\n",
    "            'mu2_phi': mu2.phi.flatten().flatten(),\n",
    "            'mu2_iso': mu2.pfRelIso04_all.flatten(),\n",
    "            \n",
    "            'jet1_pt': jet1.pt,\n",
    "            'jet1_eta': jet1.eta,\n",
    "            'jet1_phi': jet1.phi,\n",
    "            'jet1_qgl': jet1.qgl,\n",
    "\n",
    "            'jet2_pt': jet2.pt,\n",
    "            'jet2_eta': jet2.eta,\n",
    "            'jet2_phi': jet2.phi,\n",
    "            'jet2_qgl': jet2.qgl,\n",
    "     \n",
    "            'jj_mass': dijet.mass,\n",
    "            'jj_pt': dijet.pt,\n",
    "            'jj_eta': dijet.eta,\n",
    "            'jj_phi': dijet.phi,\n",
    "            'jj_dEta': dijet_deta,\n",
    "            'jj_dPhi': dijet_dphi,      \n",
    " \n",
    "            'njets': jets.counts.flatten(),\n",
    "        \n",
    "            'npv': df.PV.npvsGood.flatten(),\n",
    "            'met': df.MET.pt.flatten(),\n",
    "            'event': df.event.flatten(),\n",
    "            'event_weight': event_weight,\n",
    "        }\n",
    "\n",
    "        # Evaluate DNN \n",
    "\n",
    "        if self.evaluate_dnn:\n",
    "            do_dnn_timer = True\n",
    "            if do_dnn_timer:\n",
    "                timer_dnn = Timer('dnn evaluation')\n",
    "                timer_dnn.update()\n",
    "                \n",
    "            training_features = ['dimuon_mass', 'dimuon_pt', 'dimuon_eta', 'dimuon_dEta', 'mu1_pt', 'mu2_pt']\n",
    "            n_rows = len(dimuon_mass.flatten())\n",
    "            df_for_dnn = pd.DataFrame(columns=training_features)\n",
    "                \n",
    "            for tf in training_features:\n",
    "                feature_column = variable_map[tf]\n",
    "                assert(n_rows==len(feature_column))\n",
    "                df_for_dnn[tf] = feature_column\n",
    "\n",
    "            if do_dnn_timer:\n",
    "                timer_dnn.add_checkpoint(\"Filled input dataframe\")                \n",
    "                \n",
    "            from keras.models import load_model\n",
    "            import keras.backend as K\n",
    "            import tensorflow as tf\n",
    "            config = tf.ConfigProto()\n",
    "            config.intra_op_parallelism_threads=1\n",
    "            config.inter_op_parallelism_threads=1\n",
    "            K.set_session(tf.Session(config=config))\n",
    "            \n",
    "            if do_dnn_timer:\n",
    "                timer_dnn.add_checkpoint(\"Created TF session\")   \n",
    "\n",
    "            # BOTTLENECK: can't load model outside of a worker\n",
    "            # https://github.com/keras-team/keras/issues/9964\n",
    "            dnn_model = load_model('output/trained_models/test.h5')\n",
    "            \n",
    "            if do_dnn_timer:\n",
    "                timer_dnn.add_checkpoint(\"Loaded model\")   \n",
    "            \n",
    "            dnn_score = dnn_model.predict(df_for_dnn[training_features]).flatten()\n",
    "            variable_map['dnn_score'] = dnn_score\n",
    "            \n",
    "            if do_dnn_timer:\n",
    "                timer_dnn.add_checkpoint(\"Made predictions\")\n",
    "                timer_dnn.summary()\n",
    "            \n",
    "            if self.timer:\n",
    "                self.timer.add_checkpoint(\"Evaluated DNN\")\n",
    "\n",
    "\n",
    "\n",
    "        #################### Fill outputs ####################\n",
    "        #------------------ Binned outputs ------------------#  \n",
    "        for vname, expression in variable_map.items():\n",
    "            regions = get_regions(variable_map['dimuon_mass'])            \n",
    "            for cname in channels:\n",
    "                ccut = (category==cname)\n",
    "                for rname, rcut in regions.items():\n",
    "                    if dataset in self.dont_fill[rname]: continue # e.g. dy_m105_160 is only for higgs region\n",
    "                    if isData and ('h-peak' in rname) and ('dimuon_mass' in vname): continue # blinding\n",
    "                    value = expression[rcut & ccut]\n",
    "                    if not value.size: continue # skip empty arrays\n",
    "                    if isinstance(value, awkward.JaggedArray):\n",
    "                        # correctly fill arrays with empty elements (for example events with 0 jets)\n",
    "                        weight = event_weight[rcut & ccut][value.any()]\n",
    "                    else:\n",
    "                        weight = event_weight[rcut & ccut]\n",
    "                    output[vname].fill(**{'dataset': dataset, 'region': rname, 'channel': cname,\\\n",
    "                                         vname: value.flatten(), 'weight': weight})\n",
    "\n",
    "        output['cutflow']['all events'] += nEvts\n",
    "    \n",
    "\n",
    "        \n",
    "        #----------------- Unbinned outputs -----------------#\n",
    "        \n",
    "        if dataset in datasets_to_save_unbin: # don't need unbinned data for all samples\n",
    "            for v in self.vars_unbin:\n",
    "                if v not in variable_map.keys(): continue\n",
    "                for cname in channels:\n",
    "                    ccut = (category==cname)\n",
    "                    for rname, rcut in regions.items():\n",
    "                        if dataset in self.dont_fill[rname]: continue # e.g. dy_m105_160 is only for higgs region\n",
    "                        if 'z-peak' in rname: continue # don't need unbinned data under Z-peak\n",
    "                        output[f'{v}_unbin_{dataset}_c_{cname}_r_{rname}'] += processor.column_accumulator(variable_map[v][rcut & ccut])\n",
    "    \n",
    "        if self.timer:\n",
    "            self.timer.add_checkpoint(\"Filled outputs\")\n",
    "            \n",
    "        if self.timer:\n",
    "            self.timer.summary()\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: Iterative executor\n",
    "===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea.processor.executor import iterative_executor\n",
    "# from python.processor_lite import DimuonProcessorLite\n",
    "# For debugging purposes\n",
    "\n",
    "tstart = time.time() \n",
    "# output = processor.run_uproot_job(fileset, 'Events', DimuonProcessorLite(), iterative_executor, executor_args={'nano': True})\n",
    "output = processor.run_uproot_job(fileset, 'Events', DimuonProcessor(do_roccor=True,\\\n",
    "                                                                     evaluate_dnn=True,\\\n",
    "                                                                     do_timer=True),\\\n",
    "                                        iterative_executor, executor_args={'nano': True})\n",
    "elapsed = time.time() - tstart\n",
    "\n",
    "print(f\"Processed {output['cutflow']['all events']} events\")\n",
    "print(f\"Total time: {elapsed} s\")\n",
    "print(f\"Rate: {output['cutflow']['all events']/elapsed} events/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: Futures executor\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea.processor.executor import futures_executor\n",
    "\n",
    "tstart = time.time() \n",
    "output = processor.run_uproot_job(fileset, 'Events',\\\n",
    "                                  DimuonProcessor(),\\\n",
    "                                  futures_executor,\\\n",
    "                                  executor_args={'nano': True, 'workers':8})\n",
    "elapsed = time.time() - tstart\n",
    "\n",
    "print(f\"Processed {output['cutflow']['all events']} events\")\n",
    "print(f\"Total time: {elapsed} s\")\n",
    "print(f\"Rate: {output['cutflow']['all events']/elapsed} events/s\")\n",
    "\n",
    "out_path = \"output/test_futures.coffea\"\n",
    "util.save(output, out_path)\n",
    "print(f\"Saved output to {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 3: Dask executor\n",
    "===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "from coffea.processor.executor import dask_executor\n",
    "import dask\n",
    "\n",
    "distributed = pytest.importorskip(\"distributed\", minversion=\"1.28.1\")\n",
    "distributed.config['distributed']['scheduler']['allowed-failures'] = 20\n",
    "client = distributed.Client(processes=True, dashboard_address=None, n_workers=4, threads_per_worker=1) \n",
    "\n",
    "# Works well for small jobs\n",
    "# Unstable for large jobs (kernel dies)\n",
    "\n",
    "tstart = time.time() \n",
    "output = processor.run_uproot_job(fileset, 'Events',\\\n",
    "                                  DimuonProcessor(evaluate_dnn=False),\\\n",
    "                                  dask_executor,\\\n",
    "                                  executor_args={'nano': True, 'client': client, 'retries':20})\n",
    "elapsed = time.time() - tstart\n",
    "\n",
    "print(f\"Processed {output['cutflow']['all events']} events\")\n",
    "print(f\"Total time: {elapsed} s\")\n",
    "print(f\"Rate: {output['cutflow']['all events']/elapsed} events/s\")\n",
    "      \n",
    "out_path = \"output/test_dask.coffea\"\n",
    "util.save(output, out_path)\n",
    "print(f\"Saved output to {out_path}\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 4: Parsl executor\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coffea.processor.executor import parsl_executor\n",
    "import parsl\n",
    "from coffea.processor.parsl.detail import (_parsl_initialize, _parsl_stop, _default_cfg)\n",
    "_parsl_initialize(config=_default_cfg)\n",
    "\n",
    "# Doesn't work\n",
    "\n",
    "tstart = time.time() \n",
    "output = processor.run_uproot_job(fileset, 'Events', DimuonProcessor(), parsl_executor, executor_args={'nano': True})\n",
    "elapsed = time.time() - tstart\n",
    "\n",
    "print(f\"Processed {output['cutflow']['all events']} events\")\n",
    "print(f\"Total time: {elapsed} s\")\n",
    "print(f\"Rate: {output['cutflow']['all events']/elapsed} events/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 5: Apache Spark\n",
    "===\n",
    "\n",
    "\n",
    "NOW IT IS TIME TO START SPARK CLUSTER CONNECTION\n",
    "---\n",
    "\n",
    "When using SWAN, click on the 5-point start icon in Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql\n",
    "from pyarrow.compat import guid\n",
    "from coffea.processor.spark.detail import _spark_initialize, _spark_stop\n",
    "from coffea.processor.spark.spark_executor import spark_executor\n",
    "\"\"\"\n",
    "# NOT needed on SWAN, spark config is offloaded to spark connector\n",
    "\n",
    "spark_config = pyspark.sql.SparkSession.builder \\\n",
    "    .appName('spark-executor-test-%s' % guid()) \\\n",
    "    .master('local[*]') \\\n",
    "    .config('spark.driver.memory', '4g') \\\n",
    "    .config('spark.executor.memory', '4g') \\\n",
    "    .config('spark.sql.execution.arrow.enabled','true') \\\n",
    "    .config('spark.sql.execution.arrow.maxRecordsPerBatch', 200000)\n",
    "\n",
    "spark = _spark_initialize(config=spark, log_level='WARN', \n",
    "                          spark_progress=False, laurelin_version='0.5.1')\n",
    "\"\"\"\n",
    "partitionsize = 200000\n",
    "thread_workers = 2\n",
    "\n",
    "# Doesn't work (no full NanoEvents support)\n",
    "\n",
    "tstart = time.time() \n",
    "# if jobs fail, it might be because some columns are missing from processor._columns\n",
    "output = processor.run_spark_job(fileset, DimuonProcessor(), spark_executor, \n",
    "                                 spark=spark, partitionsize=partitionsize, thread_workers=thread_workers,\n",
    "                                 executor_args={'file_type': 'edu.vanderbilt.accre.laurelin.Root', 'cache': False, 'nano': True, 'retries': 5}\n",
    "                                )\n",
    "\n",
    "elapsed = time.time() - tstart\n",
    "\n",
    "print(f\"Processed {output['cutflow']['all events']} events\")\n",
    "print(f\"Total time: {elapsed} s\")\n",
    "print(f\"Rate: {output['cutflow']['all events']/elapsed} events/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_variable(output, inclusive, channel, region, fig, var, gs, weights):\n",
    "    data_sources = {\n",
    "        'data': ['data_B','data_C','data_D','data_E','data_F','data_G','data_H']  \n",
    "    }\n",
    "    bkg_sources = {\n",
    "        'DY': ['dy', 'dy_m105_160_amc'],\n",
    "        'EWK': ['ewk_lljj_mll50_mjj120','ewk_lljj_mll105_160'],\n",
    "        'TTbar + Single Top':['ttjets_dl', 'st_tw_top', 'st_tw_antitop'],\n",
    "        'VV + VVV': ['ww_2l2nu', 'wz_3lnu', 'www','wwz','wzz','zzz'],\n",
    "    }\n",
    "\n",
    "    if inclusive:\n",
    "        output_copy = output[var].sum('region')[:,channel].copy()\n",
    "        output_copy.scale(weights, axis='dataset')\n",
    "        data = output_copy.sum('channel').group('dataset', hist.Cat(\"dataset\", \"Dataset\"), data_sources)\n",
    "        bkg = output_copy.sum('channel').group('dataset', hist.Cat(\"dataset\", \"Dataset\"), bkg_sources)\n",
    "        ggh = output_copy['ggh_amcPS'].sum('channel')\n",
    "        vbf = output_copy['vbf_amcPS'].sum('channel')\n",
    "    else:\n",
    "        output_copy = output[var][:,region, channel].copy()\n",
    "        output_copy.scale(weights, axis='dataset')\n",
    "        data = output_copy.sum('region').sum('channel').group('dataset', hist.Cat(\"dataset\", \"Dataset\"), data_sources)\n",
    "        bkg = output_copy.sum('region').sum('channel').group('dataset', hist.Cat(\"dataset\", \"Dataset\"), bkg_sources)\n",
    "        ggh = output_copy['ggh_amcPS'].sum('region').sum('channel')\n",
    "        vbf = output_copy['vbf_amcPS'].sum('region').sum('channel')\n",
    "        \n",
    "    data_is_valid = data.sum(var).sum('dataset').values()\n",
    "\n",
    "    bkg.axis('dataset').sorting = 'integral' # sort backgrounds by event yields\n",
    "        \n",
    "        \n",
    "    scale_mc_to_data = True\n",
    "    if scale_mc_to_data and data_is_valid:\n",
    "        data_int = data.sum(var).sum('dataset').values()[()]\n",
    "        bkg_int = bkg.sum(var).sum('dataset').values()[()]    \n",
    "        bkg_sf = data_int/bkg_int\n",
    "        bkg.scale(bkg_sf)\n",
    "        \n",
    "    data_opts = {'color': 'k', 'marker': '.', 'markersize':15}\n",
    "    stack_fill_opts = {'alpha': 0.8, 'edgecolor':(0,0,0)}\n",
    "    stack_error_opts = {'label':'Stat. unc.','facecolor':(0,0,0,.4), 'hatch':'', 'linewidth': 0}\n",
    "    \n",
    "    # Top panel: Data vs. MC plot\n",
    "    plt1 = fig.add_subplot(gs[0])\n",
    "    ax_bkg = hist.plot1d(bkg, ax=plt1, overlay='dataset', overflow='all', stack=True, fill_opts=stack_fill_opts, error_opts=stack_error_opts)\n",
    "    # draw signal histograms one by one manually because set_prop_cycle didn't work for changing color map\n",
    "    ax_ggh = hist.plot1d(ggh, overlay='dataset', overflow='all', line_opts={'linewidth':2, 'color':'r'}, error_opts=None)    \n",
    "    ax_vbf = hist.plot1d(vbf, overlay='dataset', overflow='all', line_opts={'linewidth':2, 'color':'b'}, error_opts=None)    \n",
    "    \n",
    "    if data_is_valid:\n",
    "        ax_data = hist.plot1d(data, overlay='dataset', overflow='all', line_opts=None, error_opts=data_opts)\n",
    "    plt1.set_yscale('log')\n",
    "    plt1.set_ylim(0.001, 1e9)\n",
    "    lbl = hep.cms.cmslabel(plt1, data=True, paper=False, year='2016')\n",
    "    plt1.set_xlabel('')\n",
    "    plt1.tick_params(axis='x', labelbottom=False)\n",
    "    plt1.legend(prop={'size': 'xx-small'})\n",
    "    \n",
    "    # Bottom panel: Data/MC ratio plot\n",
    "    plt2 = fig.add_subplot(gs[1], sharex=plt1)\n",
    "    if data_is_valid:\n",
    "        num = data.sum('dataset')\n",
    "        denom = bkg.sum('dataset')\n",
    "        hist.plotratio(num=num, ax=plt2,\n",
    "                    denom=denom,\n",
    "                    error_opts=data_opts, denom_fill_opts={}, guide_opts={},\n",
    "                    unc='num')\n",
    "    \n",
    "    \n",
    "    plt2.axhline(1, ls='--')\n",
    "    plt2.set_ylim([0.5,1.5])    \n",
    "    plt2.set_ylabel('Data/MC')\n",
    "    lbl = plt2.get_xlabel()\n",
    "    lbl = lbl if lbl else var\n",
    "    if inclusive:\n",
    "        plt2.set_xlabel(f'{lbl}, inclusive, {channel} channel')\n",
    "    else:\n",
    "        plt2.set_xlabel(f'{lbl}, {region}, {channel} channel')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare things to plot\n",
    "from cross_sections import cross_sections\n",
    "import json\n",
    "\n",
    "mc_datasets = [s for s in fileset.keys() if 'data' not in s]\n",
    "\n",
    "lumi_weights = {'data':1}\n",
    "for mc in mc_datasets:\n",
    "    N = metadata[mc]['sumGenWgts']\n",
    "    if 'ewk_lljj_mll50_mjj120' in mc:\n",
    "        xsec = cross_sections[mc]['2016']\n",
    "    else:\n",
    "        xsec = cross_sections[mc]\n",
    "#     print(f\"{mc}: xsec:{xsec}, N:{N}\")\n",
    "    lumi_weights[mc] = xsec*lumi / N\n",
    "\n",
    "json = json.dumps(lumi_weights)\n",
    "f = open(\"output/lumi_weights.json\",\"w\")\n",
    "f.write(json)\n",
    "f.close()\n",
    "\n",
    "vars_to_plot = []\n",
    "\n",
    "vars_to_plot += ['dimuon_mass', 'dimuon_pt', 'dimuon_eta', 'dimuon_phi']\n",
    "# vars_to_plot += ['dimuon_dEta', 'dimuon_dPhi']\n",
    "# vars_to_plot += ['mu1_pt', 'mu1_eta', 'mu1_phi',  'mu2_pt', 'mu2_eta', 'mu2_phi']\n",
    "# vars_to_plot += ['jet1_pt', 'jet1_eta', 'jet1_phi', 'jet1_qgl']\n",
    "# vars_to_plot += ['jet2_pt', 'jet2_eta', 'jet2_phi', 'jet2_qgl']\n",
    "# vars_to_plot += ['jj_mass', 'jj_pt', 'jj_eta', 'jj_phi']\n",
    "# vars_to_plot += ['jj_dEta', 'jj_dPhi']\n",
    "# vars_to_plot += ['njets', 'npv', 'met']\n",
    "# vars_to_plot += ['dnn_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make plots and put them into a grid\n",
    "---\n",
    "\n",
    "Slower, but output looks nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "plt.style.use(hep.cms.style.ROOT)\n",
    "from matplotlib import gridspec\n",
    "import math\n",
    "   \n",
    "out_path = \"output/test_dask.coffea\"\n",
    "output = util.load(out_path)\n",
    "print(f\"Loading output from {out_path}\")\n",
    "fig = plt.figure()\n",
    "\n",
    "regions = [\"z-peak\", \"h-sidebands\", \"h-peak\"]\n",
    "channels = [\"ggh_01j\", \"ggh_2j\", \"vbf\"]\n",
    "nplots_x = 4 # number of plots in one row\n",
    "nplots_y = math.ceil(len(vars_to_plot)*(len(regions)+1)*len(channels) / nplots_x) # number of rows\n",
    "\n",
    "plotsize=10\n",
    "ratio_plot_size = 0.25\n",
    "fig.set_size_inches(nplots_x*plotsize,nplots_y*plotsize*(1+ratio_plot_size))\n",
    "outer_grid = gridspec.GridSpec(nplots_y, nplots_x, hspace = .3) \n",
    "idx = 0\n",
    "for var in vars_to_plot:\n",
    "    for c in channels:\n",
    "        gs = gridspec.GridSpecFromSubplotSpec(2, 1, subplot_spec = outer_grid[idx], height_ratios=[(1-ratio_plot_size),ratio_plot_size], hspace = .05)\n",
    "        plot_variable(output, True, c, \"\", fig, var, gs, lumi_weights)\n",
    "        idx += 1\n",
    "        for r in regions:\n",
    "    #         print(f\"Plotting {var} in {r}\")\n",
    "            gs = gridspec.GridSpecFromSubplotSpec(2, 1, subplot_spec = outer_grid[idx], height_ratios=[(1-ratio_plot_size),ratio_plot_size], hspace = .05)\n",
    "            plot_variable(output, False, c, r, fig, var, gs, lumi_weights)\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "sparkconnect": {
   "bundled_options": [
    "MemoryIntensive",
    "ComputeIntensive",
    "LongRunningAnalysis"
   ],
   "list_of_options": [
    {
     "name": "spark.kubernetes.container.image",
     "value": "gitlab-registry.cern.ch/db/spark-service/docker-registry/swan:laurelin"
    },
    {
     "name": "spark.sql.execution.arrow.enabled",
     "value": "true"
    },
    {
     "name": "spark.sql.execution.arrow.maxRecordsPerBatch",
     "value": "200000"
    },
    {
     "name": "spark.kubernetes.container.image.pullPolicy",
     "value": "Always"
    },
    {
     "name": "spark.driver.extraClassPath",
     "value": "./laurelin-0.5.1.jar:./lz4-java-1.5.1.jar:./log4j-core-2.11.2.jar:./log4j-api-2.11.2.jar:./xz-1.2.jar"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
