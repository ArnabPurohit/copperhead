{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <img  src=\"https://swan.web.cern.ch/sites/swan.web.cern.ch/files/pictures/logo_swan_letters.png\" alt=\"SWAN\" style=\"float: left; width: 15%; margin-right: 5%; margin-left: 17%; margin-top: 1.0em; margin-bottom: 2.0em;\">\n",
    "<img src=\"https://spark.apache.org/images/spark-logo-trademark.png\" alt=\"EP-SFT\" style=\"float: left; width: 25%; margin-right: 0%; margin-left: 0%; margin-bottom: 2.0em;\">\n",
    "<img src=\"https://cms-docdb.cern.ch/cgi-bin/PublicDocDB/RetrieveFile?docid=3045&filename=CMSlogo_color_label_1024_May2014.png&version=3\" alt=\"CMS\" style=\"float: left; width: 12%; margin-left: 5%; margin-right: 5%; margin-bottom: 2.0em;\"> -->\n",
    "<p style=\"clear: both;\">\n",
    "<div style=\"text-align:center\"><h1>CMS H&#8594;µµ analysis  \n",
    "     <br> with Coffea package from Fermilab</h1></div>\n",
    "<div style=\"text-align:center\"><i>Author: Dmitry Kondratyev, based on example code by Lindsey Gray</i></div>\n",
    "<hr style=\"border-top-width: 4px; border-top-color: #34609b;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for Higgs boson decaying into two muons\n",
    "\n",
    "This code uses awkward array toolset, and utilizing Coffea [histograms](https://coffeateam.github.io/coffea/modules/coffea.hist.html).\n",
    "This also shows the analysis object syntax implemented by Coffea [JaggedCandidateArray](https://coffeateam.github.io/coffea/api/coffea.analysis_objects.JaggedCandidateMethods.html), and the usage of custom [accumulators](https://coffeateam.github.io/coffea/api/coffea.processor.AccumulatorABC.html) other than histograms.  Further, it introduces the [processor](https://coffeateam.github.io/coffea/api/coffea.processor.ProcessorABC.html) concept and the interface to apache spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions to run at SWAN:\n",
    "===\n",
    "\n",
    "#### Load SWAN environment: LCG96 Python3 stack and Cloud Containers cluster\n",
    "\n",
    "Then run next two cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this cell if you do not have coffea installed (e.g. on SWAN with LCG 96Python3 stack)\n",
    "!pip install --user --upgrade coffea\n",
    "\n",
    "# spark.jars.packages doesnt work with Spark 2.4 with kubernetes\n",
    "!wget -N https://repo1.maven.org/maven2/edu/vanderbilt/accre/laurelin/0.5.1/laurelin-0.5.1.jar && \\\n",
    "wget -N https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-api/2.11.2/log4j-api-2.11.2.jar && \\\n",
    "wget -N https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-core/2.11.2/log4j-core-2.11.2.jar && \\\n",
    "wget -N https://repo1.maven.org/maven2/org/lz4/lz4-java/1.5.1/lz4-java-1.5.1.jar && \\\n",
    "wget -N https://repo1.maven.org/maven2/org/tukaani/xz/1.2/xz-1.2.jar\n",
    "                    \n",
    "!mkdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this cell before establishing spark connection\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONPATH'] = os.environ['PYTHONPATH'] + ':' + '/usr/local/lib/python3.6/site-packages'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions to run at Purdue Jupyter hub:\n",
    "===\n",
    "\n",
    "- Login to interactive node on a cluster (e.g. `hammer.rcac.purdue.edu`)\n",
    "- Activate local conda environment (create a new directory for the environment, if needed):\n",
    "```\n",
    "   module load anaconda/5.3.1-py37 \n",
    "   source activate /home/dkondra/conda_tests/\n",
    "```\n",
    "- Once the environment is activated, `conda install` will automatically install packages to that environment\n",
    "- Install missing packages like this: `conda install [-c conda-forge] <package name>`\n",
    "- `coffea` can be installed like this (it will use conda's `pip`): \n",
    "```\n",
    "    pip install --upgrade coffea\n",
    "```\n",
    "- In order for conda to work with notebooks, install `nb_conda`: \n",
    "```\n",
    "    conda install nb_conda\n",
    "```\n",
    "- After that, in Jupyter notebook there will be an option in **Kernel -> Change Kernel** to run the notebook using desired conda environment\n",
    "- Set up VOMS proxy:\n",
    "```\n",
    "    . setup_proxy.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env X509_USER_PROXY=/home/dkondra/x509up_u616617"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import coffea\n",
    "print(\"Coffea version: \", coffea.__version__)\n",
    "import socket\n",
    "at_purdue = ('hammer' in socket.gethostname())\n",
    "\n",
    "from coffea import util\n",
    "import coffea.processor as processor\n",
    "import uproot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SamplesInfo(object):\n",
    "    def __init__(self, year, at_purdue=True, debug=False):\n",
    "        \n",
    "        self.year = year\n",
    "        self.at_purdue = at_purdue\n",
    "        self.debug = debug\n",
    "        \n",
    "        if self.at_purdue:\n",
    "            from config.datasets import datasets, lumi_data\n",
    "            self.server = 'root://xrootd.rcac.purdue.edu/'\n",
    "        else:\n",
    "            from config.datasets_eos import datasets, lumi_data\n",
    "            self.server = ''\n",
    "            \n",
    "        self.paths = datasets[self.year]\n",
    "        self.datasets = datasets\n",
    "        self.lumi_data = lumi_data\n",
    "                \n",
    "        self.lumi = 40000 # default value\n",
    "        self.data_entries = 0\n",
    "\n",
    "        self.samples = []\n",
    "        self.missing_samples = []\n",
    "\n",
    "        self.filesets = {}\n",
    "        self.full_fileset = {}\n",
    "        self.metadata = {}\n",
    "        \n",
    "        #--- Define regions and channels used in the analysis ---#\n",
    "        self.regions = ['z-peak', 'h-sidebands', 'h-peak']\n",
    "        self.channels = ['ggh_01j', 'ggh_2j', 'vbf']\n",
    "\n",
    "        #--- Select samples for which unbinned data will be saved ---#\n",
    "        self.data_samples = [s for s in self.samples if 'data' in s]\n",
    "        self.mc_samples = [s for s in self.samples if 'data' not in s]\n",
    "        self.signal_samples = ['ggh_amcPS', 'vbf_amcPS']\n",
    "        self.main_bkg_samples = ['dy_m105_160_amc', 'dy_m105_160_vbf_amc', 'ttjets_dl', 'ewk_lljj_mll105_160', \"ewk_lljj_mll105_160_ptj0\"]\n",
    "        self.datasets_to_save_unbin = self.data_samples + self.signal_samples + self.main_bkg_samples\n",
    "        \n",
    "        #--- Take overlapping samples and assign them to different regions ---#\n",
    "        all_dy = [\"dy\", \"dy_0j\", \"dy_1j\", \"dy_2j\", \"dy_m105_160_amc\", \"dy_m105_160_vbf_amc\",\"dy_m105_160_mg\", \"dy_m105_160_vbf_mg\"]\n",
    "        all_ewk = [\"ewk_lljj_mll50_mjj120\", \"ewk_lljj_mll105_160\", \"ewk_lljj_mll105_160_ptj0\"]\n",
    "\n",
    "        self.overlapping_samples = all_dy + all_ewk\n",
    "        self.specific_samples = {\n",
    "                'z-peak': {\n",
    "                    'ggh_01j' : [\"dy_0j\", \"dy_1j\", \"dy_2j\", 'ewk_lljj_mll50_mjj120', 'ggh_amcPS', 'vbf_amcPS'],\n",
    "                    'ggh_2j' : [\"dy_0j\", \"dy_1j\", \"dy_2j\", 'ewk_lljj_mll50_mjj120', 'ggh_amcPS', 'vbf_amcPS'],\n",
    "                    'vbf' : [\"dy_0j\", \"dy_1j\", \"dy_2j\", 'ewk_lljj_mll50_mjj120', 'ggh_amcPS', 'vbf_amcPS']\n",
    "                },\n",
    "                'h-sidebands': {\n",
    "                    'ggh_01j' : ['dy_m105_160_amc', 'ewk_lljj_mll105_160', 'ggh_amcPS', 'vbf_amcPS'],\n",
    "                    'ggh_2j' : ['dy_m105_160_amc', 'ewk_lljj_mll105_160', 'ggh_amcPS', 'vbf_amcPS'],\n",
    "                    'vbf' : ['dy_m105_160_vbf_amc', 'ewk_lljj_mll105_160', 'ggh_amcPS', 'vbf_amcPS'],\n",
    "                },\n",
    "                'h-peak': {\n",
    "                    'ggh_01j' : ['dy_m105_160_amc', 'ewk_lljj_mll105_160'],\n",
    "                    'ggh_2j' : ['dy_m105_160_amc', 'ewk_lljj_mll105_160'],\n",
    "                    'vbf' : ['dy_m105_160_vbf_amc', 'ewk_lljj_mll105_160'],\n",
    "                }\n",
    "            }\n",
    "        all_ggh = [\"ggh_amcPS\", \"ggh_amcPS_TuneCP5down\", \"ggh_amcPS_TuneCP5up\", \"ggh_powheg\", \"ggh_powhegPS\"]\n",
    "        all_vbf = [\"vbf_amcPS\", \"vbf_amcPS_TuneCP5down\", \"vbf_amcPS_TuneCP5up\", \"vbf_powheg\", \"vbf_powhegPS\", \"vbf_amc_herwig\", \"vbf_powheg_herwig\"]\n",
    "\n",
    "        self.lumi_weights = {}\n",
    "\n",
    "        \n",
    "    def load(self, samples):\n",
    "        import multiprocessing as mp\n",
    "        t0 = time.time()\n",
    "\n",
    "        pool = mp.Pool(mp.cpu_count())\n",
    "        a = [pool.apply_async(self.load_sample, args=(s,)) for s in samples]\n",
    "        results = []\n",
    "        for process in a:\n",
    "            process.wait()\n",
    "            results.append(process.get())\n",
    "        pool.close()\n",
    "\n",
    "        for res in results: \n",
    "            sample = res['sample']\n",
    "            if res['is_missing']:\n",
    "                self.missing_samples.append(sample)\n",
    "            else:\n",
    "                self.samples.append(sample)\n",
    "                self.filesets[sample] = {}\n",
    "                self.filesets[sample][sample] = res['files']\n",
    "                self.full_fileset[sample] = res['files']\n",
    "    \n",
    "                self.metadata[sample] = {}\n",
    "                self.metadata[sample] = res['metadata']\n",
    "\n",
    "                self.data_entries = self.data_entries + res['data_entries']\n",
    "                \n",
    "\n",
    "        if self.data_entries:\n",
    "            print()   \n",
    "            print(f\"Loaded {self.data_entries} of {self.year} data events\")\n",
    "            self.lumi = lumi_data[self.year]['lumi']*self.data_entries/self.lumi_data[self.year]['events']\n",
    "            prc = round(self.data_entries/self.lumi_data[self.year]['events']*100, 2)\n",
    "            print(f\"This is ~ {prc}% of {self.year} data.\")\n",
    "            print(f\"Integrated luminosity {lumi}/pb\")\n",
    "            print()\n",
    "        if self.missing_samples:\n",
    "            print(f\"Missing samples: {self.missing_samples}\")\n",
    "\n",
    "        t1 = time.time()        \n",
    "        dt=round(t1-t0, 2)\n",
    "        print(f\"Loading took {dt} s\")\n",
    "\n",
    "    def load_sample(self, sample):\n",
    "        import glob, tqdm\n",
    "        from python.utils import read_via_xrootd\n",
    "        print(\"Loading\", sample)\n",
    "\n",
    "        if sample not in self.paths:\n",
    "            print(f\"Couldn't load {sample}! Skipping.\")\n",
    "            return {'sample': sample, 'metadata': {}, 'files': {}, 'data_entries': 0, 'is_missing': True}\n",
    "        \n",
    "        all_files = []\n",
    "        metadata = {}\n",
    "        data_entries = 0\n",
    "        \n",
    "        if self.at_purdue:\n",
    "            all_files = read_via_xrootd(self.paths[sample], self.server)\n",
    "        else:\n",
    "            all_files = [self.server+ f for f in glob.glob(self.paths[sample]+'*root')]      \n",
    "\n",
    "        if self.debug:\n",
    "            all_files = [all_files[0]]\n",
    "\n",
    "        if 'data' in sample:\n",
    "            for f in all_files:\n",
    "                tree = uproot.open(f)['Events']\n",
    "                data_entries += tree.numentries\n",
    "        else:\n",
    "            sumGenWgts = 0\n",
    "            nGenEvts = 0\n",
    "            for f in all_files:\n",
    "                tree = uproot.open(f)['Runs']\n",
    "                if 'NanoAODv6' in self.paths[sample]:\n",
    "                    sumGenWgts += tree.array('genEventSumw_')[0]\n",
    "                    nGenEvts += tree.array('genEventCount_')[0]\n",
    "                else:\n",
    "                    sumGenWgts += tree.array('genEventSumw')[0]\n",
    "                    nGenEvts += tree.array('genEventCount')[0]\n",
    "            metadata['sumGenWgts'] = sumGenWgts\n",
    "            metadata['nGenEvts'] = nGenEvts\n",
    "\n",
    "        files = {\n",
    "            'files': all_files,\n",
    "            'treename': 'Events'\n",
    "        }\n",
    "        return {'sample': sample, 'metadata': metadata, 'files': files, 'data_entries':data_entries, 'is_missing':False}\n",
    "\n",
    "    def compute_lumi_weights(self):\n",
    "        from config.cross_sections import cross_sections\n",
    "        import json\n",
    "        self.lumi_weights = {'data':1}\n",
    "        for sample in self.mc_samples:\n",
    "            N = self.metadata[sample]['sumGenWgts']\n",
    "            if 'ewk_lljj_mll50_mjj120' in sample:\n",
    "                xsec = cross_sections[sample]['2016']\n",
    "            else:\n",
    "                if 'ewk_lljj_mll105_160_ptj0' in sample:\n",
    "                    xsec = cross_sections['ewk_lljj_mll105_160']            \n",
    "                else:\n",
    "                    xsec = cross_sections[sample]\n",
    "            self.lumi_weights[sample] = xsec*self.lumi / N\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "samples = [\n",
    "#     'data_G',\n",
    "#     'data_B','data_C','data_D','data_E','data_F','data_G','data_H',\n",
    "#     'dy_0j', 'dy_1j', 'dy_2j',\n",
    "#     'dy_m105_160_amc',\n",
    "#     'dy_m105_160_vbf_amc',\n",
    "    'ewk_lljj_mll50_mjj120', 'ewk_lljj_mll105_160', \"ewk_lljj_mll105_160_ptj0\"\n",
    "#     'ttjets_dl', \n",
    "#     'ggh_amcPS', 'vbf_amcPS',\n",
    "\n",
    "    \n",
    "#    'ttjets_sl', 'ttz', 'ttw',\n",
    "#     'st_tw_top','st_tw_antitop',\n",
    "#     'ww_2l2nu','wz_2l2q','wz_3lnu','wz_1l1nu2q','zz',\n",
    "#     'www','wwz','wzz','zzz',\n",
    "\n",
    "]\n",
    "\n",
    "samp_info = SamplesInfo(\"2016\")\n",
    "samp_info.load(samples)\n",
    "samp_info.compute_lumi_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: Iterative executor\n",
    "===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Summary of global timer:\n",
      "--------------------------------------------------\n",
      "                     Action  Time (s)   % CPU\n",
      "0  Applied HLT and lumimask  10.53784  34.880\n",
      "1      Applied preselection   8.63608  28.585\n",
      "2         Applied Rochester   4.09531  13.555\n",
      "3       Applied dimuon cuts   2.55628   8.461\n",
      "4          Applied jet cuts   2.69480   8.920\n",
      "5    Computed jet variables   0.38160   1.263\n",
      "6            Filled outputs   1.30978   4.335\n",
      "--------------------------------------------------\n",
      "Total time: 30.2117 s\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "Summary of preselection timer:\n",
      "--------------------------------------------------\n",
      "                  Action  Time (s)   % CPU\n",
      "0         Flags computed   2.46162  30.073\n",
      "1         Muons selected   1.73152  21.153\n",
      "2     Electrons selected   3.62032  44.228\n",
      "3            df filtered   0.36982   4.518\n",
      "4         muons filtered   0.00190   0.023\n",
      "5  event_weight filtered   0.00036   0.004\n",
      "--------------------------------------------------\n",
      "Total time: 8.18554 s\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "Summary of global timer:\n",
      "--------------------------------------------------\n",
      "                     Action  Time (s)   % CPU\n",
      "0  Applied HLT and lumimask   5.93283  23.295\n",
      "1      Applied preselection   8.19621  32.182\n",
      "2         Applied Rochester   4.00432  15.723\n",
      "3       Applied dimuon cuts   2.39682   9.411\n",
      "4          Applied jet cuts   3.05430  11.993\n",
      "5    Computed jet variables   0.54816   2.152\n",
      "6            Filled outputs   1.33557   5.244\n",
      "--------------------------------------------------\n",
      "Total time: 25.46821 s\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "Summary of preselection timer:\n",
      "--------------------------------------------------\n",
      "                  Action  Time (s)   % CPU\n",
      "0         Flags computed   1.79534  26.446\n",
      "1         Muons selected   1.56886  23.110\n",
      "2     Electrons selected   3.02250  44.522\n",
      "3            df filtered   0.39723   5.851\n",
      "4         muons filtered   0.00383   0.056\n",
      "5  event_weight filtered   0.00097   0.014\n",
      "--------------------------------------------------\n",
      "Total time: 6.78873 s\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "Summary of global timer:\n",
      "--------------------------------------------------\n",
      "                     Action  Time (s)   % CPU\n",
      "0  Applied HLT and lumimask   3.27308  15.102\n",
      "1      Applied preselection   6.80459  31.396\n",
      "2         Applied Rochester   4.21584  19.451\n",
      "3       Applied dimuon cuts   2.34168  10.804\n",
      "4          Applied jet cuts   3.15883  14.574\n",
      "5    Computed jet variables   0.39981   1.845\n",
      "6            Filled outputs   1.47992   6.828\n",
      "--------------------------------------------------\n",
      "Total time: 21.67374 s\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "Summary of preselection timer:\n",
      "--------------------------------------------------\n",
      "                  Action  Time (s)   % CPU\n",
      "0         Flags computed   1.26251  27.477\n",
      "1         Muons selected   1.24980  27.200\n",
      "2     Electrons selected   1.75274  38.146\n",
      "3            df filtered   0.32495   7.072\n",
      "4         muons filtered   0.00392   0.085\n",
      "5  event_weight filtered   0.00094   0.021\n",
      "--------------------------------------------------\n",
      "Total time: 4.59486 s\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "Summary of global timer:\n",
      "--------------------------------------------------\n",
      "                     Action  Time (s)   % CPU\n",
      "0  Applied HLT and lumimask   2.01955  12.329\n",
      "1      Applied preselection   4.61075  28.148\n",
      "2         Applied Rochester   3.25944  19.899\n",
      "3       Applied dimuon cuts   2.28533  13.952\n",
      "4          Applied jet cuts   2.74100  16.734\n",
      "5    Computed jet variables   0.39745   2.426\n",
      "6            Filled outputs   1.06671   6.512\n",
      "--------------------------------------------------\n",
      "Total time: 16.38023 s\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "Summary of preselection timer:\n",
      "--------------------------------------------------\n",
      "                  Action  Time (s)   % CPU\n",
      "0         Flags computed   1.32454  26.846\n",
      "1         Muons selected   1.33849  27.129\n",
      "2     Electrons selected   1.94416  39.405\n",
      "3            df filtered   0.32268   6.540\n",
      "4         muons filtered   0.00304   0.062\n",
      "5  event_weight filtered   0.00092   0.019\n",
      "--------------------------------------------------\n",
      "Total time: 4.93383 s\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "Summary of global timer:\n",
      "--------------------------------------------------\n",
      "                     Action  Time (s)   % CPU\n",
      "0  Applied HLT and lumimask   2.26933  13.494\n",
      "1      Applied preselection   4.95054  29.437\n",
      "2         Applied Rochester   3.16768  18.836\n",
      "3       Applied dimuon cuts   2.12471  12.634\n",
      "4          Applied jet cuts   2.80546  16.682\n",
      "5    Computed jet variables   0.36154   2.150\n",
      "6            Filled outputs   1.13836   6.769\n",
      "--------------------------------------------------\n",
      "Total time: 16.81761 s\n",
      "==================================================\n",
      "\n",
      "--------------------------------------------------\n",
      "Summary of preselection timer:\n",
      "--------------------------------------------------\n",
      "                  Action  Time (s)   % CPU\n",
      "0         Flags computed   1.20998  24.632\n",
      "1         Muons selected   1.37120  27.914\n",
      "2     Electrons selected   1.86654  37.998\n",
      "3            df filtered   0.46220   9.409\n",
      "4         muons filtered   0.00180   0.037\n",
      "5  event_weight filtered   0.00045   0.009\n",
      "--------------------------------------------------\n",
      "Total time: 4.91217 s\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from coffea.processor.executor import iterative_executor\n",
    "from python.timer import Timer\n",
    "from python.dimuon_processor import DimuonProcessor\n",
    "\n",
    "# include this to make it work at Purdue:\n",
    "from ipywidgets import IntProgress, HBox, HTML\n",
    "# however, still doesn't show a nice progress bar widget\n",
    "\n",
    "tstart = time.time() \n",
    "output = processor.run_uproot_job(samp_info.full_fileset, 'Events', DimuonProcessor(\n",
    "                                                                     samp_info=samp_info,\\\n",
    "                                                                     do_roccor=True,\\\n",
    "                                                                     evaluate_dnn=False,\\\n",
    "                                                                     do_timer=True),\\\n",
    "                                        iterative_executor, executor_args={'nano': True})\n",
    "elapsed = time.time() - tstart\n",
    "\n",
    "print(f\"Processed {output['cutflow']['all events']} events\")\n",
    "print(f\"Total time: {elapsed} s\")\n",
    "print(f\"Rate: {output['cutflow']['all events']/elapsed} events/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: Futures executor\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from coffea.processor.executor import futures_executor\n",
    "\n",
    "tstart = time.time() \n",
    "output = processor.run_uproot_job(fileset, 'Events',\\\n",
    "                                  DimuonProcessor(),\\\n",
    "                                  futures_executor,\\\n",
    "                                  executor_args={'nano': True, 'workers':8})\n",
    "elapsed = time.time() - tstart\n",
    "\n",
    "print(f\"Processed {output['cutflow']['all events']} events\")\n",
    "print(f\"Total time: {elapsed} s\")\n",
    "print(f\"Rate: {output['cutflow']['all events']/elapsed} events/s\")\n",
    "\n",
    "out_path = \"output/test_futures.coffea\"\n",
    "util.save(output, out_path)\n",
    "print(f\"Saved output to {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 3: Dask executor\n",
    "===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "from coffea.processor.executor import dask_executor\n",
    "import dask\n",
    "\n",
    "if at_purdue:\n",
    "    n_workers = 18\n",
    "else:\n",
    "    n_workers = 4\n",
    "\n",
    "distributed = pytest.importorskip(\"distributed\", minversion=\"1.28.1\")\n",
    "distributed.config['distributed']['worker']['memory']['terminate'] = False\n",
    "client = distributed.Client(processes=True, dashboard_address=None, n_workers=n_workers, threads_per_worker=1) \n",
    "\n",
    "tstart = time.time()\n",
    " \n",
    "for group, fileset_ in samp_info.filesets.items():\n",
    "    print(f\"Processing {group}...\")\n",
    "    output = processor.run_uproot_job(fileset_, 'Events',\\\n",
    "                                  DimuonProcessor(samp_info=samp_info,\\\n",
    "                                                  evaluate_dnn=False,),\\\n",
    "                                  dask_executor,\\\n",
    "                                  executor_args={'nano': True, 'client': client, 'retries':20})\n",
    "\n",
    "    out_path = f\"/depot/cms/hmm/coffea/ewk_test/test_dask_{group}.coffea\"\n",
    "    util.save(output, out_path)\n",
    "    print(f\"Saved output to {out_path}\")  \n",
    "\n",
    "elapsed = time.time() - tstart\n",
    "\n",
    "print(f\"Total time: {elapsed} s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 4: Parsl executor\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from coffea.processor.executor import parsl_executor\n",
    "import parsl\n",
    "from coffea.processor.parsl.detail import (_parsl_initialize, _parsl_stop, _default_cfg)\n",
    "_parsl_initialize(config=_default_cfg)\n",
    "\n",
    "# Doesn't work\n",
    "\n",
    "tstart = time.time() \n",
    "output = processor.run_uproot_job(fileset, 'Events', DimuonProcessor(), parsl_executor, executor_args={'nano': True})\n",
    "elapsed = time.time() - tstart\n",
    "\n",
    "print(f\"Processed {output['cutflow']['all events']} events\")\n",
    "print(f\"Total time: {elapsed} s\")\n",
    "print(f\"Rate: {output['cutflow']['all events']/elapsed} events/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 5: Apache Spark\n",
    "===\n",
    "\n",
    "\n",
    "NOW IT IS TIME TO START SPARK CLUSTER CONNECTION\n",
    "---\n",
    "\n",
    "When using SWAN, click on the 5-point start icon in Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql\n",
    "from pyarrow.compat import guid\n",
    "from coffea.processor.spark.detail import _spark_initialize, _spark_stop\n",
    "from coffea.processor.spark.spark_executor import spark_executor\n",
    "\"\"\"\n",
    "# NOT needed on SWAN, spark config is offloaded to spark connector\n",
    "\n",
    "spark_config = pyspark.sql.SparkSession.builder \\\n",
    "    .appName('spark-executor-test-%s' % guid()) \\\n",
    "    .master('local[*]') \\\n",
    "    .config('spark.driver.memory', '4g') \\\n",
    "    .config('spark.executor.memory', '4g') \\\n",
    "    .config('spark.sql.execution.arrow.enabled','true') \\\n",
    "    .config('spark.sql.execution.arrow.maxRecordsPerBatch', 200000)\n",
    "\n",
    "spark = _spark_initialize(config=spark, log_level='WARN', \n",
    "                          spark_progress=False, laurelin_version='0.5.1')\n",
    "\"\"\"\n",
    "partitionsize = 200000\n",
    "thread_workers = 2\n",
    "\n",
    "# Doesn't work (no full NanoEvents support)\n",
    "\n",
    "tstart = time.time() \n",
    "# if jobs fail, it might be because some columns are missing from processor._columns\n",
    "output = processor.run_spark_job(fileset, DimuonProcessor(), spark_executor, \n",
    "                                 spark=spark, partitionsize=partitionsize, thread_workers=thread_workers,\n",
    "                                 executor_args={'file_type': 'edu.vanderbilt.accre.laurelin.Root', 'cache': False, 'nano': True, 'retries': 5}\n",
    "                                )\n",
    "\n",
    "elapsed = time.time() - tstart\n",
    "\n",
    "print(f\"Processed {output['cutflow']['all events']} events\")\n",
    "print(f\"Total time: {elapsed} s\")\n",
    "print(f\"Rate: {output['cutflow']['all events']/elapsed} events/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting\n",
    "===\n",
    "\n",
    "Make plots and put them into a grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from python.plotting import Plotter\n",
    "    \n",
    "# vars_to_plot = []\n",
    "\n",
    "# vars_to_plot += ['dimuon_mass']\n",
    "# vars_to_plot += ['dimuon_pt', 'dimuon_eta', 'dimuon_phi']\n",
    "# vars_to_plot += ['dimuon_dEta', 'dimuon_dPhi']\n",
    "# vars_to_plot += ['mu1_pt', 'mu1_eta', 'mu1_phi',  'mu2_pt', 'mu2_eta', 'mu2_phi']\n",
    "# vars_to_plot += ['jet1_pt', 'jet1_eta', 'jet1_phi', 'jet1_qgl']\n",
    "# vars_to_plot += ['jet2_pt', 'jet2_eta', 'jet2_phi', 'jet2_qgl']\n",
    "# vars_to_plot += ['jj_mass', 'jj_pt', 'jj_eta', 'jj_phi']\n",
    "# vars_to_plot += ['jj_dEta', 'jj_dPhi']\n",
    "# vars_to_plot += ['njets', 'npv', 'met']\n",
    "# vars_to_plot += ['dnn_score']\n",
    "\n",
    "all_plots_2016_pars = {\n",
    "    'processor': DimuonProcessor(),\n",
    "    'path': '/depot/cms/hmm/coffea/',\n",
    "    'samples': [\n",
    "        'data_B','data_C','data_D','data_E','data_F','data_G','data_H',\n",
    "        'dy_0j', 'dy_1j', 'dy_2j',\n",
    "        'dy_m105_160_amc',\n",
    "        'dy_m105_160_vbf_amc',\n",
    "        'ewk_lljj_mll50_mjj120', 'ewk_lljj_mll105_160',\n",
    "        'ttjets_dl', \n",
    "        'ggh_amcPS', 'vbf_amcPS',\n",
    "        'ttjets_sl', 'ttz', 'ttw',\n",
    "        'st_tw_top','st_tw_antitop',\n",
    "        'ww_2l2nu','wz_2l2q','wz_3lnu','wz_1l1nu2q','zz',\n",
    "        'www','wwz','wzz','zzz',\n",
    "        ],\n",
    "    'vars': ['dimuon_mass'],\n",
    "    'year': '2016',\n",
    "    'regions' : [\"z-peak\", \"h-sidebands\", \"h-peak\"],\n",
    "    'channels': [\"ggh_01j\", \"ggh_2j\", \"vbf\"], \n",
    "}\n",
    "\n",
    "all_plots = Plotter(**all_plots_2016_pars)\n",
    "all_plots.make_datamc_comparison(do_inclusive=True, do_exclusive=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from python.plotting import Plotter\n",
    "\n",
    "ewz_study_pars = {\n",
    "    'processor': DimuonProcessor(),\n",
    "    'path': '/depot/cms/hmm/coffea/ewk_test/',\n",
    "    'samples': ['ewk_lljj_mll105_160', 'ewk_lljj_mll105_160_ptj0'],\n",
    "    'vars': ['jet1_pt', 'jet1_eta', 'jet2_pt', 'jet2_eta'],\n",
    "    'year': '2017',\n",
    "    'regions' : [\"h-sidebands\", \"h-peak\"],\n",
    "    'channels': [\"vbf\"], \n",
    "}\n",
    "\n",
    "ewz_study_plots = Plotter(**ewz_study_pars)\n",
    "ewz_study_plots.make_shape_comparison(do_inclusive=False, do_exclusive=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env: conda_tests]",
   "language": "python",
   "name": "conda-env-conda_tests-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "sparkconnect": {
   "bundled_options": [
    "MemoryIntensive",
    "ComputeIntensive",
    "LongRunningAnalysis"
   ],
   "list_of_options": [
    {
     "name": "spark.kubernetes.container.image",
     "value": "gitlab-registry.cern.ch/db/spark-service/docker-registry/swan:laurelin"
    },
    {
     "name": "spark.sql.execution.arrow.enabled",
     "value": "true"
    },
    {
     "name": "spark.sql.execution.arrow.maxRecordsPerBatch",
     "value": "200000"
    },
    {
     "name": "spark.kubernetes.container.image.pullPolicy",
     "value": "Always"
    },
    {
     "name": "spark.driver.extraClassPath",
     "value": "./laurelin-0.5.1.jar:./lz4-java-1.5.1.jar:./log4j-core-2.11.2.jar:./log4j-api-2.11.2.jar:./xz-1.2.jar"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
