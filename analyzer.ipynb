{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <img  src=\"https://swan.web.cern.ch/sites/swan.web.cern.ch/files/pictures/logo_swan_letters.png\" alt=\"SWAN\" style=\"float: left; width: 15%; margin-right: 5%; margin-left: 17%; margin-top: 1.0em; margin-bottom: 2.0em;\">\n",
    "<img src=\"https://spark.apache.org/images/spark-logo-trademark.png\" alt=\"EP-SFT\" style=\"float: left; width: 25%; margin-right: 0%; margin-left: 0%; margin-bottom: 2.0em;\">\n",
    "<img src=\"https://cms-docdb.cern.ch/cgi-bin/PublicDocDB/RetrieveFile?docid=3045&filename=CMSlogo_color_label_1024_May2014.png&version=3\" alt=\"CMS\" style=\"float: left; width: 12%; margin-left: 5%; margin-right: 5%; margin-bottom: 2.0em;\"> -->\n",
    "<p style=\"clear: both;\">\n",
    "<div style=\"text-align:center\"><h1>CMS H&#8594;µµ analysis with Apache Spark  \n",
    "     <br> using Coffea and Laurelin packages from Fermilab</h1></div>\n",
    "<div style=\"text-align:center\"><i>Author: Dmitry Kondratyev, based on example code by Lindsey Gray</i></div>\n",
    "<hr style=\"border-top-width: 4px; border-top-color: #34609b;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for Higgs boson decaying into two muons\n",
    "\n",
    "This code uses awkward array toolset, and utilizing Coffea [histograms](https://coffeateam.github.io/coffea/modules/coffea.hist.html).\n",
    "This also shows the analysis object syntax implemented by Coffea [JaggedCandidateArray](https://coffeateam.github.io/coffea/api/coffea.analysis_objects.JaggedCandidateMethods.html), and the usage of custom [accumulators](https://coffeateam.github.io/coffea/api/coffea.processor.AccumulatorABC.html) other than histograms.  Further, it introduces the [processor](https://coffeateam.github.io/coffea/api/coffea.processor.ProcessorABC.html) concept and the interface to apache spark.\n",
    "\n",
    "#### SWAN env: LCG96 Python3 stack and Cloud Containers cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you do not have coffea installed (e.g. on SWAN with LCG 96Python3 stack)\n",
    "!pip install --user --upgrade coffea\n",
    "\n",
    "# spark.jars.packages doesnt work with Spark 2.4 with kubernetes\n",
    "!wget -N https://repo1.maven.org/maven2/edu/vanderbilt/accre/laurelin/0.5.1/laurelin-0.5.1.jar && \\\n",
    "wget -N https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-api/2.11.2/log4j-api-2.11.2.jar && \\\n",
    "wget -N https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-core/2.11.2/log4j-core-2.11.2.jar && \\\n",
    "wget -N https://repo1.maven.org/maven2/org/lz4/lz4-java/1.5.1/lz4-java-1.5.1.jar && \\\n",
    "wget -N https://repo1.maven.org/maven2/org/tukaani/xz/1.2/xz-1.2.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell before establishing spark connection\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONPATH'] = os.environ['PYTHONPATH'] + ':' + '/usr/local/lib/python3.6/site-packages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import coffea\n",
    "print(\"Coffea version: \", coffea.__version__)\n",
    "\n",
    "from coffea import hist, util\n",
    "from coffea.analysis_objects import JaggedCandidateArray\n",
    "import coffea.processor as processor\n",
    "from coffea.lookup_tools import extractor, dense_lookup\n",
    "\n",
    "import uproot\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "paths = {\n",
    "'data': '/eos/cms/store/data/Run2016*/SingleMuon/NANOAOD/Nano25Oct2019-v1/*/',\n",
    "'dy': '/eos/cms/store/mc/RunIISummer16NanoAODv6/DYJetsToLL_M-50_TuneCUETP8M1_13TeV-amcatnloFXFX-pythia8/NANOAODSIM/PUMoriond17_Nano25Oct2019_102X_mcRun2_asymptotic_v7_ext2-v1/*/',\n",
    "'ttjets_dl': '/eos/cms/store/mc/RunIISummer16NanoAODv6/TTJets_DiLept_TuneCUETP8M1_13TeV-madgraphMLM-pythia8/NANOAODSIM/PUMoriond17_Nano25Oct2019_102X_mcRun2_asymptotic_v7-v1/*/'\n",
    "}\n",
    "\n",
    "# TODO: add possibility to load of files through XrootD\n",
    "# TODO: add Golden JSON lumi-section filter\n",
    "\n",
    "samples = paths.keys()\n",
    "\n",
    "debug = True\n",
    "\n",
    "fileset = {}\n",
    "\n",
    "lumi = 1\n",
    "\n",
    "# Find ROOT files in local directories\n",
    "for sample, path in paths.items():\n",
    "    all_files = []\n",
    "    all_files = glob.glob(path+'*root')\n",
    "    \n",
    "    if debug:\n",
    "        all_files = [all_files[0]]\n",
    "\n",
    "    if 'data' in sample:\n",
    "        entries = 0\n",
    "        for f in all_files:\n",
    "           fi = uproot.open('root://eoscms.cern.ch/'+f)['Events']\n",
    "           entries += fi.numentries\n",
    "        lumi = 35860.*entries/645880988.\n",
    "        print(f\"Loading {entries/645880988.}% of 2016 data.\")\n",
    "        print(f\"Integrated luminosity {lumi}/pb\")\n",
    "        \n",
    "    fileset[sample] = {\n",
    "        'files': ['root://eoscms.cern.ch/'+f for f in all_files],\n",
    "        'treename': 'Events'\n",
    "    }\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "puLookup = util.load('data/pileup/puLookup.coffea')\n",
    "muSFFileList = [{'id'   : (\"data/muon_sf/EfficienciesStudies_2016_legacy_rereco_rootfiles_RunBCDEF_SF_ID.root\", \"NUM_TightID_DEN_genTracks_eta_pt\"),\n",
    "                 'iso'   : (\"data/muon_sf/EfficienciesStudies_2016_legacy_rereco_rootfiles_RunBCDEF_SF_ISO.root\", \"NUM_TightRelIso_DEN_TightIDandIPCut_eta_pt\"),\n",
    "                 'trig'  : (\"data/muon_sf/EfficienciesStudies_2016_trigger_EfficienciesAndSF_RunBtoF.root\", \"IsoMu24_OR_IsoTkMu24_PtEtaBins/abseta_pt_ratio\"),\n",
    "                 'scale' : 19.656062760/35.882515396},\n",
    "                {'id'     : (\"data/muon_sf/EfficienciesStudies_2016_legacy_rereco_rootfiles_RunGH_SF_ID.root\", \"NUM_TightID_DEN_genTracks_eta_pt\"),\n",
    "                 'iso'   : (\"data/muon_sf/EfficienciesStudies_2016_legacy_rereco_rootfiles_RunGH_SF_ISO.root\", \"NUM_TightRelIso_DEN_TightIDandIPCut_eta_pt\"),\n",
    "                 'trig'  : (\"data/muon_sf/EfficienciesStudies_2016_trigger_EfficienciesAndSF_RunGtoH.root\", \"IsoMu24_OR_IsoTkMu24_PtEtaBins/abseta_pt_ratio\"),\n",
    "                 'scale' : 16.226452636/35.882515396}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at ProcessorABC documentation to see the expected methods and what they are supposed to do\n",
    "# https://coffeateam.github.io/coffea/api/coffea.processor.ProcessorABC.html\n",
    "class DimuonProcessor(processor.ProcessorABC):\n",
    "    def __init__(self, mass_window=[76,106]):\n",
    "        self.mass_window = mass_window\n",
    "        self._columns = ['nMuon', 'Muon_pt', 'Muon_eta', 'Muon_phi', 'Muon_mass', 'Muon_charge', 'Muon_pfRelIso04_all',\n",
    "                         'nJet', 'Jet_pt', 'Jet_eta', 'Jet_phi', 'Jet_mass',\n",
    "                        'PV_npvsGood', 'MET_pt', 'genWeight', 'HLT_IsoMu24', 'HLT_IsoTkMu24']\n",
    "        dataset_axis = hist.Cat(\"dataset\", \"\")\n",
    "\n",
    "        axes = {}\n",
    "        axes['dimuon_mass'] =  hist.Bin(\"dimuon_mass\", r\"$m_{\\mu\\mu}$ [GeV]\", 100, self.mass_window[0], self.mass_window[1])\n",
    "        axes['dimuon_pt'] = hist.Bin(\"dimuon_pt\", r\"$p_{T}(\\mu\\mu)$ [GeV]\", 100, 0, 400)\n",
    "        axes['dimuon_eta'] = hist.Bin(\"dimuon_eta\", r\"$\\eta (\\mu\\mu)$ [GeV]\", 100, -5, 5)\n",
    "        axes['dimuon_phi'] = hist.Bin(\"dimuon_phi\", r\"$\\phi (\\mu\\mu)$ [GeV]\", 100, -3.2, 3.2)\n",
    "        axes['dimuon_dEta'] = hist.Bin(\"dimuon_dEta\", r\"$\\Delta\\eta (\\mu\\mu)$ [GeV]\", 100, 0, 5)\n",
    "        axes['dimuon_dPhi'] = hist.Bin(\"dimuon_dPhi\", r\"$\\Delta\\phi (\\mu\\mu)$ [GeV]\", 100, 0, 6.4)\n",
    "\n",
    "        axes['mu1_pt'] = hist.Bin(\"mu1_pt\", r\"$p_{T}(\\mu_{1})$ [GeV]\", 100, 0, 400)\n",
    "        axes['mu1_pt_nosf'] = hist.Bin(\"mu1_pt_nosf\", r\"$p_{T}(\\mu_{1}) no SF$ [GeV]\", 100, 0, 400)\n",
    "        axes['mu1_eta'] = hist.Bin(\"mu1_eta\", r\"$\\eta (\\mu_{1})$ [GeV]\", 100, -2.4, 2.4)\n",
    "        axes['mu1_phi'] = hist.Bin(\"mu1_phi\", r\"$\\phi (\\mu_{1})$ [GeV]\", 100, -3.2, 3.2)\n",
    "        axes['mu2_pt'] = hist.Bin(\"mu2_pt\", r\"$p_{T}(\\mu_{2})$ [GeV]\", 100, 0, 300)\n",
    "        axes['mu2_eta'] = hist.Bin(\"mu2_eta\", r\"$\\eta (\\mu_{2})$ [GeV]\", 100, -2.4, 2.4)\n",
    "        axes['mu2_phi'] = hist.Bin(\"mu2_phi\", r\"$\\phi (\\mu_{2})$ [GeV]\", 100, -3.2, 3.2)\n",
    "\n",
    "        axes['jet1_pt'] = hist.Bin(\"jet1_pt\", r\"$p_{T}(jet1)$ [GeV]\", 100, 0, 400)\n",
    "        axes['jet1_eta'] = hist.Bin(\"jet1_eta\", r\"$\\eta (jet1)$ [GeV]\", 100, -4.7, 4.7)\n",
    "        axes['jet1_phi'] = hist.Bin(\"jet1_phi\", r\"$\\phi (jet1)$ [GeV]\", 100, -3.2, 3.2)\n",
    "        axes['jet2_pt'] = hist.Bin(\"jet2_pt\", r\"$p_{T}(jet2)$ [GeV]\", 100, 0, 300)\n",
    "        axes['jet2_eta'] = hist.Bin(\"jet2_eta\", r\"$\\eta (jet2)$ [GeV]\", 100, -4.7, 4.7)\n",
    "        axes['jet2_phi'] = hist.Bin(\"jet2_phi\", r\"$\\phi (jet2)$ [GeV]\", 100, -3.2, 3.2)\n",
    "        \n",
    "        axes['njets'] = hist.Bin(\"njets\", \"njets\", 10, 0, 10)\n",
    "        axes['npv'] = hist.Bin(\"npv\", \"npv\", 50, 0, 50)\n",
    "        axes['npv_prePU'] = hist.Bin(\"npv_prePU\", \"npv_prePU\", 50, 0, 50)\n",
    "        axes['met'] = hist.Bin(\"met\", r\"$E_{T}^{miss.}$ [GeV]\", 100, 0, 200)\n",
    "\n",
    "        axes['genweight'] = hist.Bin(\"genweight\", \"genweight\", 50, 0, 50)\n",
    "        \n",
    "        variables = axes.keys()\n",
    "        \n",
    "        accumulators = {}\n",
    "        for v in variables:\n",
    "            accumulators[v] = hist.Hist(\"Counts\", dataset_axis, axes[v])\n",
    "            # TODO: add category axis and systematics axis\n",
    "            \n",
    "        accumulators['sumGenWeights'] = hist.Hist(\"sumGenWeights\", dataset_axis)\n",
    "        accumulators['entries'] = hist.Hist(\"Entries\", dataset_axis)\n",
    "        accumulators['cutflow'] = processor.defaultdict_accumulator(int)\n",
    "        \n",
    "        accumulators['dimuon_mass_unbinned'] = processor.column_accumulator(np.ndarray([]))\n",
    "        # request: possibility to add another axis to column_accumulator (e.g. 'dataset')\n",
    "        \n",
    "        self._accumulator = processor.dict_accumulator(accumulators)\n",
    "    \n",
    "        mu_id_vals = 0\n",
    "        mu_id_err = 0\n",
    "        mu_iso_vals = 0\n",
    "        mu_iso_err = 0\n",
    "        mu_trig_vals = 0\n",
    "        mu_trig_err = 0\n",
    "\n",
    "        for scaleFactors in muSFFileList:\n",
    "            id_file = uproot.open(scaleFactors['id'][0])\n",
    "            iso_file = uproot.open(scaleFactors['iso'][0])\n",
    "            trig_file = uproot.open(scaleFactors['trig'][0])\n",
    "\n",
    "            mu_id_vals += id_file[scaleFactors['id'][1]].values * scaleFactors['scale']\n",
    "            mu_id_err += id_file[scaleFactors['id'][1]].variances**0.5 * scaleFactors['scale']\n",
    "            mu_id_edges = id_file[scaleFactors['id'][1]].edges\n",
    "\n",
    "            mu_iso_vals += iso_file[scaleFactors['iso'][1]].values * scaleFactors['scale']\n",
    "            mu_iso_err += iso_file[scaleFactors['iso'][1]].variances**0.5 * scaleFactors['scale']\n",
    "            mu_iso_edges = iso_file[scaleFactors['iso'][1]].edges\n",
    "\n",
    "            mu_trig_vals += trig_file[scaleFactors['trig'][1]].values * scaleFactors['scale']\n",
    "            mu_trig_err += trig_file[scaleFactors['trig'][1]].variances**0.5 * scaleFactors['scale']\n",
    "            mu_trig_edges = trig_file[scaleFactors['trig'][1]].edges\n",
    "\n",
    "        self.mu_id_sf = dense_lookup.dense_lookup(mu_id_vals, mu_id_edges)\n",
    "        self.mu_id_err = dense_lookup.dense_lookup(mu_id_err, mu_id_edges)\n",
    "        self.mu_iso_sf = dense_lookup.dense_lookup(mu_iso_vals, mu_iso_edges)\n",
    "        self.mu_iso_err = dense_lookup.dense_lookup(mu_iso_err, mu_iso_edges)\n",
    "        self.mu_trig_sf = dense_lookup.dense_lookup(mu_trig_vals, mu_trig_edges)\n",
    "        self.mu_trig_err = dense_lookup.dense_lookup(mu_trig_err, mu_trig_edges)    \n",
    "    \n",
    "#         self.extractor = extractor()\n",
    "        # Take PU weights and muon scale factors from https://github.com/UFLX2MuMu/Ntupliser\n",
    "#         self.extractor.add_weight_sets(['PU_wgt PU_wgt data/pileup/PU_wgt_2016_Summer16_v0.root'])\n",
    "#         self.extractor.finalize()\n",
    "#         self.evaluator = self.extractor.make_evaluator()\n",
    "    \n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "    \n",
    "    @property\n",
    "    def columns(self):\n",
    "        return self._columns\n",
    "    \n",
    "    def process(self, df):\n",
    "        output = self.accumulator.identity()\n",
    "        dataset = df.metadata['dataset']\n",
    "        isData = 'data' in dataset\n",
    "    \n",
    "        if not isData:            \n",
    "            muID = self.mu_id_sf(df.Muon.eta, df.Muon.pt)\n",
    "            muIso = self.mu_iso_sf(df.Muon.eta, df.Muon.pt)\n",
    "            muTrig = self.mu_iso_sf(abs(df.Muon.eta), df.Muon.pt)\n",
    "            muSF = (muID*muIso*muTrig).prod()\n",
    "            \n",
    "#             muIDerr = self.mu_id_err(muons.eta, muons.pt)\n",
    "#             muIsoerr = self.mu_iso_err(muons.eta, muons.pt)\n",
    "#             muTrigerr = self.mu_iso_err(abs(muons.eta), muons.pt)\n",
    "#             muSF_up = ((muID + muIDerr) * (muIso + muIsoerr) * (muTrig + muTrigerr)).prod()\n",
    "#             muSF_down = ((muID - muIDerr) * (muIso - muIsoerr) * (muTrig - muTrigerr)).prod()    \n",
    "    \n",
    "        nEvts = df.shape[0]\n",
    "        \n",
    "        if isData:\n",
    "            genweight = np.ones(nEvts)\n",
    "            event_weight = np.ones(nEvts)\n",
    "            event_weight_nosf = np.ones(nEvts)\n",
    "        else:\n",
    "            genweight = df.genWeight.array\n",
    "#             pu_weight = self.evaluator['PU_wgt'](df.Pileup.nTrueInt)\n",
    "            pu_weight = puLookup(dataset, df.Pileup.nTrueInt)\n",
    "            event_weight = genweight*muSF*pu_weight\n",
    "            event_weight_nosf = genweight*pu_weight\n",
    "\n",
    "        output['sumGenWeights'].fill(dataset=dataset, weight=genweight.sum())\n",
    "         \n",
    "\n",
    "        # Select and fill general event info\n",
    "\n",
    "        # TODO: electron veto\n",
    "        # TODO: Add FSR recovery\n",
    "        # TODO: Add muon scale factors\n",
    "        # TODO: Add Rochester correction\n",
    "\n",
    "        muons = df.Muon\n",
    "        muons = muons[(muons.pt > 20) & (abs(muons.eta) < 2.4) & (muons.pfRelIso04_all<0.25)]\n",
    "\n",
    "        # TODO: require at least one muon to be matched with an L3 object\n",
    "\n",
    "        hlt = df.HLT.IsoMu24 | df.HLT.IsoTkMu24          \n",
    "        event_filter = (hlt & (muons.counts == 2) & (muons['charge'].prod() == -1)).flatten()\n",
    "\n",
    "        df = df[event_filter]\n",
    "        muons = muons[event_filter] # 'muons' only stores two muons per event\n",
    "        event_weight = event_weight[event_filter]\n",
    "        event_weight_nosf = event_weight_nosf[event_filter]\n",
    "        # maybe will move event filter somewhere just before filling histograms\n",
    "        \n",
    "        # for fast calculation of dimuon kinematics\n",
    "        muons_jca = JaggedCandidateArray.candidatesfromcounts(\n",
    "            muons.counts,\n",
    "            pt=muons.pt.content,\n",
    "            eta=muons.eta.content,\n",
    "            phi=muons.phi.content,\n",
    "            mass=muons.mass.content,\n",
    "        )\n",
    "        dimuons = muons_jca.distincts()\n",
    "        dimuon_filter = ((muons[muons.pt.argmax()].pt > 26) & (dimuons.mass > self.mass_window[0]) & (dimuons.mass < self.mass_window[1])).flatten()\n",
    "        \n",
    "        df = df[dimuon_filter]    \n",
    "        muons = muons[dimuon_filter]\n",
    "        mu1 = muons[muons.pt.argmax()]\n",
    "        mu2 = muons[muons.pt.argmin()]\n",
    "        dimuons = dimuons[dimuon_filter]\n",
    "        event_weight = event_weight[dimuon_filter]\n",
    "        event_weight_nosf = event_weight_nosf[dimuon_filter]\n",
    "        \n",
    "        # TODO: dimuon_costhetaCS, dimuon_phiCS\n",
    "        \n",
    "        # Select and fill jets and jet pairs\n",
    "        # TODO: nsoftjets, jet pT, eta, phi, QGL; dijet mass, eta, phi, deta, dphi\n",
    "        \n",
    "        jets = df.Jet\n",
    "        jet_selection = ((jets.pt > 25) & (abs(jets.eta) < 4.7))\n",
    "        jets = jets[jet_selection]\n",
    "        \n",
    "        # TODO: DeltaR(j, mu) > 0.4\n",
    "        # TODO: jetID, PU jetID\n",
    "\n",
    "        jet1 = jets[jets.pt.argmax()]\n",
    "        # TODO: get jet2 as well\n",
    "        \n",
    "        # TODO: kinematic variables of multimuon-multijet system\n",
    "        # >> many of them...\n",
    "        \n",
    "        # Other variables\n",
    "        npv = df.PV.npvsGood      \n",
    "        met = df.MET.pt        \n",
    "\n",
    "        \n",
    "        # TODO: Add initial categorization (ggH vs. VBF)\n",
    "        \n",
    "        # Fill event weights\n",
    "        # TODO: PU weight, NNLOPS reweighting, Zpt reweighting\n",
    "        \n",
    "        # TODO: Add systematic uncertainties\n",
    "        \n",
    "        # TODO: Evaluate DNN\n",
    "        \n",
    "\n",
    "        output['cutflow']['all events'] += nEvts\n",
    "        output['cutflow']['event_filter'] += event_filter.sum()\n",
    "        output['cutflow']['dimuon_filter'] += dimuon_filter.sum()\n",
    "\n",
    "        ### Fill muons ###\n",
    "        output['dimuon_mass'].fill(dataset=dataset, dimuon_mass=dimuons.mass.flatten(), weight=event_weight) \n",
    "        if isData:\n",
    "            output['dimuon_mass_unbinned'] += processor.column_accumulator(dimuons.mass.flatten())\n",
    "        output['dimuon_pt'].fill(dataset=dataset, dimuon_pt=dimuons.pt.flatten(), weight=event_weight)\n",
    "        output['dimuon_eta'].fill(dataset=dataset, dimuon_eta=dimuons.eta.flatten(), weight=event_weight)\n",
    "        output['dimuon_phi'].fill(dataset=dataset, dimuon_phi=dimuons.phi.flatten(), weight=event_weight)\n",
    "        output['dimuon_dEta'].fill(dataset=dataset, dimuon_dEta=abs(mu1.eta.flatten() - mu2.eta.flatten()), weight=event_weight)\n",
    "\n",
    "        output['dimuon_dPhi'].fill(dataset=dataset, dimuon_dPhi=abs(mu1.delta_phi(mu2)).flatten(), weight=event_weight)        \n",
    "\n",
    "        output['mu1_pt'].fill(dataset=dataset, mu1_pt=mu1.pt.flatten(), weight=event_weight)\n",
    "        output['mu1_pt_nosf'].fill(dataset=dataset, mu1_pt_nosf=mu1.pt.flatten(), weight=event_weight_nosf)\n",
    "        output['mu1_eta'].fill(dataset=dataset, mu1_eta=mu1.eta.flatten(), weight=event_weight)\n",
    "        output['mu1_phi'].fill(dataset=dataset, mu1_phi=mu1.phi.flatten(), weight=event_weight)\n",
    "\n",
    "        output['mu2_pt'].fill(dataset=dataset, mu2_pt=mu2.pt.flatten(), weight=event_weight)\n",
    "        output['mu2_eta'].fill(dataset=dataset, mu2_eta=mu2.eta.flatten(), weight=event_weight)\n",
    "        output['mu2_phi'].fill(dataset=dataset, mu2_phi=mu2.phi.flatten(), weight=event_weight)    \n",
    "        \n",
    "        ### Fill jets ###\n",
    "#         output['jet1_pt'].fill(dataset=dataset, jet1_pt=jet1.pt.flatten(), weight=event_weight)\n",
    "#         output['jet1_eta'].fill(dataset=dataset, jet1_eta=jet1.eta.flatten(), weight=event_weight)\n",
    "#         output['jet1_phi'].fill(dataset=dataset, jet1_phi=jet1.phi.flatten(), weight=event_weight)\n",
    "#         output['njets'].fill(dataset=dataset, njets=jets.counts, weight=event_weight)\n",
    "        \n",
    "        ### Fill other variables ###\n",
    "        output['npv'].fill(dataset=dataset, npv=npv, weight=event_weight)\n",
    "        output['met'].fill(dataset=dataset, met=met, weight=event_weight)\n",
    "            \n",
    "        output['entries'].fill(dataset=dataset, weight=nEvts) # temporary\n",
    "        \n",
    "        \n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: Iterative executor\n",
    "===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: iterative processing \n",
    "\n",
    "from coffea.processor.executor import iterative_executor\n",
    "\n",
    "tstart = time.time() \n",
    "output = processor.run_uproot_job(fileset, 'Events', DimuonProcessor(), iterative_executor, executor_args={'nano': True})\n",
    "elapsed = time.time() - tstart\n",
    "\n",
    "print(f\"Processed {output['cutflow']['all events']} events\")\n",
    "print(f\"Total time: {elapsed} s\")\n",
    "print(f\"Rate: {output['cutflow']['all events']/elapsed} events/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 2: Dask executor\n",
    "===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from coffea.processor.executor import dask_executor\n",
    "\n",
    "distributed = pytest.importorskip(\"distributed\", minversion=\"1.28.1\")\n",
    "client = distributed.Client(processes=False, dashboard_address=None)\n",
    "\n",
    "tstart = time.time() \n",
    "output = processor.run_uproot_job(fileset, 'Events', DimuonProcessor(mass_window=[115,150]), dask_executor, executor_args={'nano': True, 'client': client})\n",
    "elapsed = time.time() - tstart\n",
    "\n",
    "print(f\"Processed {output['cutflow']['all events']} events\")\n",
    "print(f\"Total time: {elapsed} s\")\n",
    "print(f\"Rate: {output['cutflow']['all events']/elapsed} events/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 3: Apache Spark\n",
    "===\n",
    "\n",
    "\n",
    "NOW IT IS TIME TO START SPARK CLUSTER CONNECTION\n",
    "---\n",
    "\n",
    "When using SWAN, click on the 5-point start icon in Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql\n",
    "from pyarrow.compat import guid\n",
    "from coffea.processor.spark.detail import _spark_initialize, _spark_stop\n",
    "from coffea.processor.spark.spark_executor import spark_executor\n",
    "\"\"\"\n",
    "# NOT needed on SWAN, spark config is offloaded to spark connector\n",
    "\n",
    "spark_config = pyspark.sql.SparkSession.builder \\\n",
    "    .appName('spark-executor-test-%s' % guid()) \\\n",
    "    .master('local[*]') \\\n",
    "    .config('spark.driver.memory', '4g') \\\n",
    "    .config('spark.executor.memory', '4g') \\\n",
    "    .config('spark.sql.execution.arrow.enabled','true') \\\n",
    "    .config('spark.sql.execution.arrow.maxRecordsPerBatch', 200000)\n",
    "\n",
    "spark = _spark_initialize(config=spark, log_level='WARN', \n",
    "                          spark_progress=False, laurelin_version='0.5.1')\n",
    "\"\"\"\n",
    "partitionsize = 200000\n",
    "thread_workers = 2\n",
    "\n",
    "tstart = time.time() \n",
    "\n",
    "output = processor.run_spark_job(fileset, DimuonProcessor(), spark_executor, \n",
    "                                 spark=spark, partitionsize=partitionsize, thread_workers=thread_workers,\n",
    "                                 executor_args={'file_type': 'edu.vanderbilt.accre.laurelin.Root', 'cache': False}\n",
    "                                )\n",
    "\n",
    "elapsed = time.time() - tstart\n",
    "\n",
    "print(f\"Processed {output['cutflow']['all events']} events\")\n",
    "print(f\"Total time: {elapsed} s\")\n",
    "print(f\"Rate: {output['cutflow']['all events']/elapsed} events/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_variable(fig, var, gs, weights):\n",
    "    \n",
    "    output_copy = output[var].copy() # copy to prevent scaling multiple times\n",
    "    output_copy.scale(weights, axis='dataset')\n",
    "    \n",
    "    # Group the samples for plotting (will be important when we unite low-yield samples like VV)\n",
    "    \n",
    "    data = output_copy['data']\n",
    "    \n",
    "    bkg_sources = {\n",
    "        'dy': ['dy'],\n",
    "        'ttbar':['ttjets_dl'],  \n",
    "    }\n",
    "    bkg = output_copy.group('dataset', hist.Cat(\"dataset\", \"Dataset\"), bkg_sources)\n",
    "    bkg.axis('dataset').sorting = 'integral' # sort backgrounds by event yields\n",
    "    \n",
    "    data_opts = {'color': 'k', 'marker': '.', 'markersize':15}\n",
    "    stack_fill_opts = {'alpha': 0.8, 'edgecolor':(0,0,0)}\n",
    "    stack_error_opts = {'label':'Stat. unc.','facecolor':(0,0,0,.4), 'hatch':'', 'linewidth': 0}\n",
    "    \n",
    "    # Top panel: Data vs. MC plot\n",
    "    plt1 = fig.add_subplot(gs[0])\n",
    "    ax_bkg = hist.plot1d(bkg, ax=plt1, overlay='dataset', overflow='all', stack=True, fill_opts=stack_fill_opts, error_opts=stack_error_opts)\n",
    "    ax_data = hist.plot1d(data, overlay='dataset', overflow='all', line_opts=None, error_opts=data_opts)\n",
    "    plt1.set_yscale('log')\n",
    "    plt1.set_ylim(0.1, 1e7)\n",
    "    lbl = hep.cms.cmslabel(plt1, data=True, paper=False, year='2016')\n",
    "    plt1.set_xlabel('')\n",
    "    plt1.tick_params(axis='x', labelbottom=False)\n",
    "    \n",
    "\n",
    "    # Bottom panel: Data/MC ratio plot\n",
    "    plt2 = fig.add_subplot(gs[1], sharex=plt1)\n",
    "    num = data.sum('dataset')\n",
    "    denom = bkg.sum('dataset')\n",
    "    hist.plotratio(num=num, ax=plt2,\n",
    "                    denom=denom,\n",
    "                    error_opts=data_opts, denom_fill_opts={}, guide_opts={},\n",
    "                    unc='num')\n",
    "    \n",
    "    \n",
    "    plt2.axhline(1, ls='--')\n",
    "    plt2.set_ylim([0,2])    \n",
    "    plt2.set_ylabel('Data/MC')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare things to plot\n",
    "from parameters import cross_sections\n",
    "\n",
    "mc_datasets = [s for s in samples if 'data' not in s]\n",
    "\n",
    "lumi_weights = {'data':1}\n",
    "for mc in mc_datasets:\n",
    "    N = output['sumGenWeights'].values()[(mc,)]\n",
    "#     print(N)\n",
    "    lumi_weights[mc] = cross_sections[mc]*lumi / N\n",
    "\n",
    "# print(output['dimuon_mass_unbinned'].value)    \n",
    "    \n",
    "vars_to_plot = []\n",
    "vars_to_plot += ['dimuon_mass', 'dimuon_pt', 'dimuon_eta', 'dimuon_phi']\n",
    "#vars_to_plot += ['dimuon_dEta', 'dimuon_dPhi']\n",
    "#vars_to_plot += ['mu1_pt','mu2_pt', 'mu1_eta','mu2_eta', 'mu1_phi','mu2_phi']\n",
    "#vars_to_plot += ['jet1_pt', 'jet1_eta', 'jet1_phi', 'njets',]\n",
    "vars_to_plot += ['mu1_pt', 'mu1_pt_nosf']\n",
    "# vars_to_plot += ['npv', 'npv_prePU', 'met']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make plots iteratively and put into a grid\n",
    "---\n",
    "\n",
    "Slower, but output looks nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import mplhep as hep\n",
    "plt.style.use(hep.cms.style.ROOT)\n",
    "from matplotlib import gridspec\n",
    "import math\n",
    "    \n",
    "fig = plt.figure()\n",
    "    \n",
    "nplots_x = 4 # number of plots in one row\n",
    "nplots_y = math.ceil(len(vars_to_plot) / nplots_x) # number of rows\n",
    "\n",
    "plotsize=10\n",
    "ratio_plot_size = 0.25\n",
    "fig.set_size_inches(nplots_x*plotsize,nplots_y*plotsize*(1+ratio_plot_size))\n",
    "outer_grid = gridspec.GridSpec(nplots_y, nplots_x, hspace = .3) \n",
    "for i, var in enumerate(vars_to_plot):\n",
    "    gs = gridspec.GridSpecFromSubplotSpec(2, 1, subplot_spec = outer_grid[i], height_ratios=[(1-ratio_plot_size),ratio_plot_size], hspace = .05)\n",
    "    plot_variable(fig, var, gs, lumi_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make plots in parallel\n",
    "---\n",
    "\n",
    "Faster, but can't display the plots in a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import mplhep as hep\n",
    "plt.style.use(hep.cms.style.ROOT)\n",
    "import multiprocessing as mp\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 50\n",
    "    \n",
    "figs = {}\n",
    "def plot_var(i):\n",
    "    fig = plt.figure()\n",
    "    plotsize=8\n",
    "    ratio_plot_size = 0.24\n",
    "    fig.set_size_inches(plotsize,plotsize*(1+ratio_plot_size))\n",
    "    gs = gridspec.GridSpec(2, 1, height_ratios=[(1-ratio_plot_size),ratio_plot_size], hspace = .05)\n",
    "    plot_variable(fig, vars_to_plot[i], gs, lumi_weights)\n",
    "    return fig\n",
    "\n",
    "pool = mp.Pool(mp.cpu_count() - 1)\n",
    "results = [pool.apply(plot_var, args=(x,)) for x in range(len(vars_to_plot))]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "sparkconnect": {
   "bundled_options": [
    "MemoryIntensive",
    "ComputeIntensive"
   ],
   "list_of_options": [
    {
     "name": "spark.kubernetes.container.image",
     "value": "gitlab-registry.cern.ch/db/spark-service/docker-registry/swan:laurelin"
    },
    {
     "name": "spark.sql.execution.arrow.enabled",
     "value": "true"
    },
    {
     "name": "spark.sql.execution.arrow.maxRecordsPerBatch",
     "value": "200000"
    },
    {
     "name": "spark.kubernetes.container.image.pullPolicy",
     "value": "Always"
    },
    {
     "name": "spark.driver.extraClassPath",
     "value": "./laurelin-0.5.1.jar:./lz4-java-1.5.1.jar:./log4j-core-2.11.2.jar:./log4j-api-2.11.2.jar:./xz-1.2.jar"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
