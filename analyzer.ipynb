{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <img  src=\"https://swan.web.cern.ch/sites/swan.web.cern.ch/files/pictures/logo_swan_letters.png\" alt=\"SWAN\" style=\"float: left; width: 15%; margin-right: 5%; margin-left: 17%; margin-top: 1.0em; margin-bottom: 2.0em;\">\n",
    "<img src=\"https://spark.apache.org/images/spark-logo-trademark.png\" alt=\"EP-SFT\" style=\"float: left; width: 25%; margin-right: 0%; margin-left: 0%; margin-bottom: 2.0em;\">\n",
    "<img src=\"https://cms-docdb.cern.ch/cgi-bin/PublicDocDB/RetrieveFile?docid=3045&filename=CMSlogo_color_label_1024_May2014.png&version=3\" alt=\"CMS\" style=\"float: left; width: 12%; margin-left: 5%; margin-right: 5%; margin-bottom: 2.0em;\"> -->\n",
    "<p style=\"clear: both;\">\n",
    "<div style=\"text-align:center\"><h1>CMS H&#8594;µµ analysis  \n",
    "     <br> with Coffea package from Fermilab</h1></div>\n",
    "<div style=\"text-align:center\"><i>Author: Dmitry Kondratyev, based on example code by Lindsey Gray</i></div>\n",
    "<hr style=\"border-top-width: 4px; border-top-color: #34609b;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search for Higgs boson decaying into two muons\n",
    "\n",
    "This code uses awkward array toolset, and utilizing Coffea [histograms](https://coffeateam.github.io/coffea/modules/coffea.hist.html).\n",
    "This also shows the analysis object syntax implemented by Coffea [JaggedCandidateArray](https://coffeateam.github.io/coffea/api/coffea.analysis_objects.JaggedCandidateMethods.html), and the usage of custom [accumulators](https://coffeateam.github.io/coffea/api/coffea.processor.AccumulatorABC.html) other than histograms.  Further, it introduces the [processor](https://coffeateam.github.io/coffea/api/coffea.processor.ProcessorABC.html) concept and the interface to apache spark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions to run at SWAN:\n",
    "===\n",
    "\n",
    "#### Load SWAN environment: LCG96 Python3 stack and Cloud Containers cluster\n",
    "\n",
    "Then run next two cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this cell if you do not have coffea installed (e.g. on SWAN with LCG 96Python3 stack)\n",
    "!pip install --user --upgrade coffea\n",
    "\n",
    "# spark.jars.packages doesnt work with Spark 2.4 with kubernetes\n",
    "!wget -N https://repo1.maven.org/maven2/edu/vanderbilt/accre/laurelin/0.5.1/laurelin-0.5.1.jar && \\\n",
    "wget -N https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-api/2.11.2/log4j-api-2.11.2.jar && \\\n",
    "wget -N https://repo1.maven.org/maven2/org/apache/logging/log4j/log4j-core/2.11.2/log4j-core-2.11.2.jar && \\\n",
    "wget -N https://repo1.maven.org/maven2/org/lz4/lz4-java/1.5.1/lz4-java-1.5.1.jar && \\\n",
    "wget -N https://repo1.maven.org/maven2/org/tukaani/xz/1.2/xz-1.2.jar\n",
    "                    \n",
    "!mkdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this cell before establishing spark connection\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONPATH'] = os.environ['PYTHONPATH'] + ':' + '/usr/local/lib/python3.6/site-packages'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions to run at Purdue Jupyter hub:\n",
    "===\n",
    "\n",
    "- Login to interactive node on a cluster (e.g. `hammer.rcac.purdue.edu`)\n",
    "- Activate local conda environment (create a new directory for the environment, if needed):\n",
    "```\n",
    "   module load anaconda/5.3.1-py37 \n",
    "   source activate /home/dkondra/conda_tests/\n",
    "```\n",
    "- Once the environment is activated, `conda install` will automatically install packages to that environment\n",
    "- Install missing packages like this: `conda install [-c conda-forge] <package name>`\n",
    "- `coffea` can be installed like this (it will use conda's `pip`): \n",
    "```\n",
    "    pip install --upgrade coffea\n",
    "```\n",
    "- In order for conda to work with notebooks, install `nb_conda`: \n",
    "```\n",
    "    conda install nb_conda\n",
    "```\n",
    "- After that, in Jupyter notebook there will be an option in **Kernel -> Change Kernel** to run the notebook using desired conda environment\n",
    "- Set up VOMS proxy:\n",
    "```\n",
    "    . setup_proxy.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env X509_USER_PROXY=/home/dkondra/x509up_u616617"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import coffea\n",
    "print(\"Coffea version: \", coffea.__version__)\n",
    "import socket\n",
    "\n",
    "print(socket.gethostname())\n",
    "from coffea import util\n",
    "import coffea.processor as processor\n",
    "import multiprocessing as mp\n",
    "print(f\"{mp.cpu_count()} CPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python.samples_info import SamplesInfo\n",
    "samples = [\n",
    "### Data ###\n",
    "#           'data_A',\n",
    "#      'data_B',\n",
    "#        'data_C',\n",
    "#     'data_D','data_E',\n",
    "#     'data_F',\n",
    "#    'data_G','data_H',\n",
    "\n",
    "### Essential MC ###    \n",
    "    'dy_m105_160_amc', \n",
    "#    'dy_m105_160_vbf_amc',\n",
    "#      'ggh_amcPS', \n",
    "#     'vbf_powhegPS', \n",
    "#     'ttjets_dl',\n",
    "#    \"ewk_lljj_mll105_160_ptj0\",\n",
    "\n",
    "### Non-essential MC ### \n",
    "#     'ttjets_sl',\n",
    "#     'ttz',\n",
    "#     'ttw',\n",
    "#     'st_tw_top','st_tw_antitop',\n",
    "#     'ww_2l2nu',\n",
    "#     'wz_2l2q',\n",
    "#     'wz_3lnu',\n",
    "#     'wz_1l1nu2q',\n",
    "#      'zz',\n",
    "# # ##\n",
    "    \n",
    "]\n",
    "\n",
    "purdue = 'root://xrootd.rcac.purdue.edu/'\n",
    "\n",
    "year = '2016'\n",
    "label = 'test' # change this to save to other directory\n",
    "\n",
    "samp_info = SamplesInfo(year=year, out_path=f'test_{year}_{label}', server=purdue, debug=True)\n",
    "\n",
    "# 'outer' refers to parallelization by sample, 'inner' - by ROOT file in each sample\n",
    "samp_info.load(samples, nchunks=1, parallelize_outer=1, parallelize_inner=10)\n",
    "samp_info.compute_lumi_weights()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterative executor\n",
    "===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from coffea.processor.executor import iterative_executor\n",
    "from python.dimuon_processor import DimuonProcessor\n",
    "from ipywidgets import IntProgress, HBox, HTML\n",
    "\n",
    "tstart = time.time() \n",
    "output = processor.run_uproot_job(samp_info.full_fileset, 'Events', DimuonProcessor(\n",
    "                                                                     samp_info=samp_info, do_jecunc=False),\n",
    "                                        iterative_executor, executor_args={'nano': True})\n",
    "elapsed = time.time() - tstart\n",
    "print(f\"Total time: {elapsed} s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask executor\n",
    "===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "from coffea.processor.executor import dask_executor\n",
    "import dask\n",
    "from python.dimuon_processor import DimuonProcessor\n",
    "\n",
    "n_workers = 16\n",
    "\n",
    "distributed = pytest.importorskip(\"distributed\", minversion=\"1.28.1\")\n",
    "distributed.config['distributed']['worker']['memory']['terminate'] = False\n",
    "client = distributed.Client(processes=True, dashboard_address=None, n_workers=n_workers, threads_per_worker=1) \n",
    "\n",
    "tstart = time.time()\n",
    "\n",
    "for ds_name, fileset_ in samp_info.filesets_chunked.items():\n",
    "    for ichunk, ifileset in enumerate(fileset_):\n",
    "        print(f\"Processing {ds_name}, chunk {ichunk+1}/{samp_info.nchunks} ...\")\n",
    "        output = processor.run_uproot_job(ifileset, 'Events',\\\n",
    "                                      DimuonProcessor(samp_info=samp_info, do_jecunc=False),\\\n",
    "                                      dask_executor,\\\n",
    "                                      executor_args={'nano': True, 'client': client})\n",
    "\n",
    "        out_dir = f\"/depot/cms/hmm/coffea/{samp_info.out_path}/\"\n",
    "\n",
    "        try:\n",
    "            os.mkdir(out_dir)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        for mode in output.keys():\n",
    "            out_dir_ = f\"{out_dir}/{mode}/\"\n",
    "            out_path_ = f\"{out_dir_}/{ds_name}_{ichunk}.coffea\"\n",
    "            try:\n",
    "                os.mkdir(out_dir_)\n",
    "            except:\n",
    "                pass\n",
    "            util.save(output[mode], out_path_)\n",
    "\n",
    "        output.clear()\n",
    "        print(f\"Saved output to {out_dir}\")\n",
    "    \n",
    "elapsed = time.time() - tstart\n",
    "\n",
    "print(f\"Total time: {elapsed} s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Data/MC comparison\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob\n",
    "import argparse\n",
    "from python.postprocessing import postprocess, plot, save_shapes\n",
    "from config.variables import variables\n",
    "from config.datasets import datasets\n",
    "import pandas as pd\n",
    "\n",
    "year = '2016'\n",
    "label = 'test'\n",
    "\n",
    "to_plot = ['dimuon_mass']\n",
    "vars_to_plot = {v.name:v for v in variables if v.name in to_plot}\n",
    "samples = [\n",
    "    'data_A',\n",
    "    'data_B',\n",
    "    'data_C',\n",
    "    'data_D',\n",
    "    'data_E',\n",
    "    'data_F',\n",
    "    'data_G',\n",
    "    'data_H',\n",
    "    'dy_m105_160_amc',\n",
    "    'dy_m105_160_vbf_amc',\n",
    "    'ewk_lljj_mll105_160_ptj0',\n",
    "    'ttjets_dl',\n",
    "    'ttjets_sl',\n",
    "    'ttz',\n",
    "    'ttw',\n",
    "    'st_tw_top','st_tw_antitop',\n",
    "    'ww_2l2nu',\n",
    "    'wz_2l2q',\n",
    "    'wz_3lnu',\n",
    "    'zz',\n",
    "    'ggh_amcPS',\n",
    "    'vbf_powhegPS',\n",
    "]\n",
    "\n",
    "\n",
    "postproc_args = {\n",
    "    'modules': ['to_pandas',  'get_hists'],\n",
    "    'year': year,\n",
    "    'label': label,\n",
    "    'in_path': f'/depot/cms/hmm/coffea/all_{year}_{label}/',\n",
    "    'syst_variations': ['nominal'],\n",
    "    'samples':samples,\n",
    "    'channels': ['vbf'],\n",
    "    'regions': ['h-peak', 'h-sidebands'],\n",
    "    'vars_to_plot': list(vars_to_plot.values()),\n",
    "    'wgt_variations': False,\n",
    "}\n",
    "\n",
    "\n",
    "dfs, hist_dfs, edges = postprocess(postproc_args)\n",
    "hist = {}\n",
    "for var, hists in hist_dfs.items():\n",
    "    hist[var] = pd.concat(hists, ignore_index=True)\n",
    "    \n",
    "for vname, var in vars_to_plot.items():\n",
    "     for r in postproc_args['regions']:\n",
    "        plot(var, hist, 'wgt_nominal', edges[vname], postproc_args, r, save=False, show=True, plotsize=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env: hmumu_coffea]",
   "language": "python",
   "name": "conda-env-hmumu_coffea-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "sparkconnect": {
   "bundled_options": [
    "MemoryIntensive",
    "ComputeIntensive",
    "LongRunningAnalysis"
   ],
   "list_of_options": [
    {
     "name": "spark.kubernetes.container.image",
     "value": "gitlab-registry.cern.ch/db/spark-service/docker-registry/swan:laurelin"
    },
    {
     "name": "spark.sql.execution.arrow.enabled",
     "value": "true"
    },
    {
     "name": "spark.sql.execution.arrow.maxRecordsPerBatch",
     "value": "200000"
    },
    {
     "name": "spark.kubernetes.container.image.pullPolicy",
     "value": "Always"
    },
    {
     "name": "spark.driver.extraClassPath",
     "value": "./laurelin-0.5.1.jar:./lz4-java-1.5.1.jar:./log4j-core-2.11.2.jar:./log4j-api-2.11.2.jar:./xz-1.2.jar"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
