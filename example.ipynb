{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running:\n",
    "---\n",
    "\n",
    "- Activate a Conda environment with the following packages installed: `xrootd`, `dask`, `pytest`, `nb_conda`\n",
    "- Make sure that current Jupyter kernel uses that environment\n",
    "- Initialize VOMS proxy (might need to explicitly set the path in a Jupyter cell: `%env X509_USER_PROXY=<path>`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import coffea\n",
    "from coffea import util\n",
    "import coffea.processor as processor\n",
    "\n",
    "from python.samples_info import SamplesInfo\n",
    "\n",
    "# Replace the server and dataset path with whatever works on your machine \n",
    "server = 'root://xrootd.rcac.purdue.edu/'\n",
    "example_datasets = {\n",
    "    \"2016\": {\n",
    "        \"dy_m105_160_amc\": \"/store/mc/RunIISummer16NanoAODv6/DYJetsToLL_M-105To160_TuneCP5_PSweights_13TeV-amcatnloFXFX-pythia8/\",\n",
    "    }   \n",
    "}\n",
    "\n",
    "samples = [\n",
    "    'dy_m105_160_amc', \n",
    "]\n",
    "\n",
    "# path to save output files (they may take up to several GB)\n",
    "out_path = '/depot/cms/hmm/coffea/'\n",
    "\n",
    "samp_info = SamplesInfo(year=\"2016\", out_path=out_path, server=server, example=True, example_datasets=example_datasets)\n",
    "\n",
    "# this will get the list of all root files in <server>/<dataset_path> via xrootd and split the list of files \n",
    "# into <nchunks> chunks (or less, if there are less files in the dataset). \n",
    "samp_info.load(samples, nchunks=5)\n",
    "samp_info.compute_lumi_weights()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask executor\n",
    "===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pytest\n",
    "from coffea.processor.executor import dask_executor\n",
    "import dask\n",
    "from python.dimuon_processor import DimuonProcessor\n",
    "\n",
    "n_workers = 12\n",
    "\n",
    "distributed = pytest.importorskip(\"distributed\", minversion=\"1.28.1\")\n",
    "distributed.config['distributed']['worker']['memory']['terminate'] = False\n",
    "client = distributed.Client(processes=True, dashboard_address=None, n_workers=n_workers, threads_per_worker=1) \n",
    "\n",
    "tstart = time.time()\n",
    "\n",
    "# The chunked lists of files are stored in a dictionary samp_info.filesets_chunked, that has a format \n",
    "# {dataset_name: [list of filesets]},\n",
    "# and each of the 'filesets' is in the format that is good to be used as an input to Coffea processor.\n",
    "\n",
    "for sample, filesets in samp_info.filesets_chunked.items():\n",
    "    for ichunk, fileset in enumerate(filesets):\n",
    "        print(f\"Processing {sample}, chunk {ichunk+1}/{samp_info.nchunks} ...\")\n",
    "        output = processor.run_uproot_job(fileset, 'Events',\\\n",
    "                                      DimuonProcessor(samp_info=samp_info),\\\n",
    "                                      dask_executor,\\\n",
    "                                      executor_args={'nano': True, 'client': client})\n",
    "\n",
    "        prefix = \"\"\n",
    "        out_path = f\"{samp_info.out_path}/{prefix}{sample}_{ichunk}.coffea\"\n",
    "        util.save(output, out_path)\n",
    "        print(f\"Saved output to {out_path}\")   \n",
    "    \n",
    "elapsed = time.time() - tstart\n",
    "\n",
    "print(f\"Total time: {elapsed} s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env: conda_tests]",
   "language": "python",
   "name": "conda-env-conda_tests-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "sparkconnect": {
   "bundled_options": [
    "MemoryIntensive",
    "ComputeIntensive",
    "LongRunningAnalysis"
   ],
   "list_of_options": [
    {
     "name": "spark.kubernetes.container.image",
     "value": "gitlab-registry.cern.ch/db/spark-service/docker-registry/swan:laurelin"
    },
    {
     "name": "spark.sql.execution.arrow.enabled",
     "value": "true"
    },
    {
     "name": "spark.sql.execution.arrow.maxRecordsPerBatch",
     "value": "200000"
    },
    {
     "name": "spark.kubernetes.container.image.pullPolicy",
     "value": "Always"
    },
    {
     "name": "spark.driver.extraClassPath",
     "value": "./laurelin-0.5.1.jar:./lz4-java-1.5.1.jar:./log4j-core-2.11.2.jar:./log4j-api-2.11.2.jar:./xz-1.2.jar"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
